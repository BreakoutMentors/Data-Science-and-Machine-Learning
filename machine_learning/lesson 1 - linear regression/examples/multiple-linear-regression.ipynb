{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression What Makes Us Happy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP41RoUd3n+Y70WkYRuVYlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krmiddlebrook/intro_to_deep_learning/blob/master/machine_learning/lesson%201%20-%20linear%20regression/examples/multiple-linear-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5H75AdnZ8KJ",
        "colab_type": "text"
      },
      "source": [
        "# Multiple Linear Regression: What Makes Us Happy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvzapAQFaK3t",
        "colab_type": "text"
      },
      "source": [
        "In our previous lesson, we explored the topic of single-variable linear regression and demonstrated how to build a linear model as a single-layer neural network. In single-variable regression analysis, there is only one independent variable (i.e., x variable) and one dependent variable (i.e., y variable). However, what if we want to build a model to predict a label given multiple x variables (i.e., features)? To achieve this, we use multiple linear regression--a method to model the relationship between a dependent variable (y) and multiple independent variables (x). Intuitively, adding more x (in this case x columns) data to our features tends to help the model improve its overall performance.\n",
        "\n",
        "While adding more variables allows us to model more complex \"real-world\" relationships there are also additional steps we must take to make sure our model is sound and robust.\n",
        "\n",
        "In this lesson, we introduce the multiple linear regression method and demonstrate how to build a multiple linear regression model to predict happiness Scores using the same [World Happiness Report dataset](https://www.kaggle.com/unsdsn/world-happiness) from the previous lesson. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1cMSwsmuz1",
        "colab_type": "text"
      },
      "source": [
        "## Multiple Linear Regression\n",
        "Before we discuss multiple linear regression, let's review single-variable linear regression. Recall that single-variable linear regression aims to fit a line to the data using the following formula: \n",
        "$$\n",
        "\\hat{y} = wx + b\n",
        "$$\n",
        "where $w$ is known as the *weight*, and $b$ is   the *bias* term.\n",
        "\n",
        "When the inputs ($x$) consists of $d$ features, we express the above linear function as: \n",
        "$$\n",
        "\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.\n",
        "$$\n",
        "\n",
        "In machine learning, we typically work with *high-dimensional* datasets, meaning $d$ is large so there are many features. When $d$ is large, it's not convenient to write the above linear equation, instead we can express it using vector notation as follows:\n",
        "$$\n",
        "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
        "$$\n",
        "\n",
        "where the vector $\\mathbf{x} \\in \\mathbb{R}^d$ and the vector $\\mathbf{w} \\in \\mathbb{R}^d$  contain the *features* and *weights* respectively.\n",
        "\n",
        "In the above equation, $\\mathbf{x}$ corresponds to a single input sample. It is often more convenient to refer to features of our entire dataset of $n$ samples via the *matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, where each sample is represented by a row and each feature by a column. For a collection of features $\\mathbf{X}$ and labels $y \\in \\mathbb{R^n}$, the multiple linear regression function can be expressed as the matrix-vector product:\n",
        "\n",
        "$$\n",
        "{\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b.\n",
        "$$\n",
        "\n",
        "The goal of multiple linear regression is to find the weight vector  $\\mathbf{w}$  and the bias term  $b$  that results in the lowest prediction error. Notice that the goal is basically the same as single-variable linear regression except multiple weights are learned instead of one.\n",
        "\n",
        "The figure below illustrates a single-layer multiple linear regression neural network. The input layer consists of $d$ *neurons* each corresponding to a feature (from $\\mathbf{x}$) and $d$ *connections* each corresponding to the *weight* between the output neuron an input neuron. \n",
        "\n",
        "<figure>\n",
        "<img src='https://d2l.ai/_images/singleneuron.svg' width='60%'></img><figcaption>Linear Regression: a multiple linear regression neural network</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8hGH7ke4rFR",
        "colab_type": "text"
      },
      "source": [
        "# Multiple Linear Regression: What makes us happy?\n",
        "Now that we know a bit about the multiple linear regression method, it's time to apply it to a real-world problem--predicting happiness given country statistics. Our goal is to build a *single-layer fully-connected neural network* (i.e., a mutliple linear regression model) to predict a country's happiness Score using the World Happiness (https://www.kaggle.com/unsdsn/world-happiness) dataset.\n",
        "\n",
        "Like single-variable linear regression, we will perform the following steps:\n",
        "\n",
        "1. Explore and prepare the dataset.\n",
        "2. Build the model.\n",
        "3. Train the model.\n",
        "4. Evaluate the model.\n",
        "5. Draw conclusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lehhNxE1fLxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the libraries we be need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DReavXyx7CoZ",
        "colab_type": "text"
      },
      "source": [
        "## 1. Explore and prepare the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHqEvmn1ZYa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6c0b5eb9-6d7a-4c7d-962e-ca6193a6eff8"
      },
      "source": [
        "# load the dataset into a dataframe\n",
        "data_url = 'https://raw.githubusercontent.com/krmiddlebrook/intro_to_deep_learning/master/datasets/world-happiness/2019.csv'\n",
        "happy2019 = pd.read_csv(data_url)\n",
        "happy2019.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Overall rank</th>\n",
              "      <th>Country or region</th>\n",
              "      <th>Score</th>\n",
              "      <th>GDP per capita</th>\n",
              "      <th>Social support</th>\n",
              "      <th>Healthy life expectancy</th>\n",
              "      <th>Freedom to make life choices</th>\n",
              "      <th>Generosity</th>\n",
              "      <th>Perceptions of corruption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Finland</td>\n",
              "      <td>7.769</td>\n",
              "      <td>1.340</td>\n",
              "      <td>1.587</td>\n",
              "      <td>0.986</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>7.600</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.573</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Norway</td>\n",
              "      <td>7.554</td>\n",
              "      <td>1.488</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.028</td>\n",
              "      <td>0.603</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Iceland</td>\n",
              "      <td>7.494</td>\n",
              "      <td>1.380</td>\n",
              "      <td>1.624</td>\n",
              "      <td>1.026</td>\n",
              "      <td>0.591</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Netherlands</td>\n",
              "      <td>7.488</td>\n",
              "      <td>1.396</td>\n",
              "      <td>1.522</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.557</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Overall rank Country or region  ...  Generosity  Perceptions of corruption\n",
              "0             1           Finland  ...       0.153                      0.393\n",
              "1             2           Denmark  ...       0.252                      0.410\n",
              "2             3            Norway  ...       0.271                      0.341\n",
              "3             4           Iceland  ...       0.354                      0.118\n",
              "4             5       Netherlands  ...       0.322                      0.298\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8P_pZ917oTt",
        "colab_type": "text"
      },
      "source": [
        "There are two variables we don't want to use in our model: Overall rank and Country or region. In the below cell, we will remove those variables and prepare the dataset for the model. Remember this process involves defining the features ($\\mathbf{x}$) and the labels ($y$), splitting the dataset into a training and test set, and separating the features and the labels in both sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgFdnBbl6zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7664f9c2-e80c-4a3e-8df8-32816e7b66b6"
      },
      "source": [
        "# define the x (features) and y (labels) variables\n",
        "x_cols = happy2019.columns[3:].tolist() \n",
        "y_col = 'Score'\n",
        "print('x features: ', x_cols)\n",
        "print('y labels: ', y_col)\n",
        "\n",
        "# split the dataset into train/test datasets \n",
        "train = happy2019.sample(frac=0.8, random_state=0)\n",
        "test = happy2019.drop(train.index)\n",
        "\n",
        "# separate the x (features) and y (labels) in the train/test datasets\n",
        "train_features = train[x_cols].values\n",
        "test_features = test[x_cols].values\n",
        "\n",
        "train_labels = train[y_col].values.reshape(-1, 1)\n",
        "test_labels = test[y_col].values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "print('train features shape:', train_features.shape)\n",
        "print('train labels shape:', train_labels.shape)\n",
        "\n",
        "print('test features shape:', test_features.shape)\n",
        "print('test labels shape:', test_labels.shape)\n",
        "\n",
        "print('first 5 test labels:\\n', test_labels[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x features:  ['GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\n",
            "y labels:  Score\n",
            "train features shape: (125, 6)\n",
            "train labels shape: (125, 1)\n",
            "test features shape: (31, 6)\n",
            "test labels shape: (31, 1)\n",
            "first 5 test labels:\n",
            " [[7.246]\n",
            " [6.726]\n",
            " [6.444]\n",
            " [6.354]\n",
            " [6.3  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmGj7bGL-UqH",
        "colab_type": "text"
      },
      "source": [
        "The above code returns a training and test dataset. The *features* correspond to GDP per capita, Social support, Healthy life expectancy, Freedom to make life choices, Generosity, and  Perceptions of corruption columns and the happiness Score corresponds to the *labels*. There are two datasets--a *training dataset* and a *test dataset*. The `train_features` and `train_labels` arrays represent the features and labels of the training dataset. The `test_features` and `test_labels` arrays represent the features and labels of the test dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTOstVUn_JR3",
        "colab_type": "text"
      },
      "source": [
        "## 2. Build the model\n",
        "Now that the data is ready, we can build the model! \n",
        "\n",
        "Before we define the model in Python code, let's write out its function given one input sample:\n",
        "$$\n",
        "\\hat{\\text{score}} = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6 + b,\n",
        "$$\n",
        "where 1-6 corresponds to one of the feature column we defined earlier (i.e., GDP per capita, Social support, etc.). The function can be compressed using vector notation as:\n",
        "$$ \n",
        "\\hat{\\text{score}} = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
        "$$\n",
        "\n",
        "Given the entire training set, we write the matrix-vector equation for our linear model as:\n",
        "$$\n",
        "{\\hat{\\textbf{score}}} = \\mathbf{X} \\mathbf{w} + b.\n",
        "$$\n",
        "\n",
        "The model we build and then train will try to find the optimal *weights* ($\\mathbf{w}$) to minimize the difference between the real labels ($\\textbf{score}$) and the predictions ($\\hat{\\textbf{score}})$.\n",
        "\n",
        "Now that we know the function we want to estimate, let's use [Tensorflow](https://www.tensorflow.org/) to build a linear regression model, just like we did in the last lesson.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc_gJ4RllO5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "4dc3de86-23e9-45fb-fa17-5d2baede22f0"
      },
      "source": [
        "# build the linear model \n",
        "model = keras.Sequential([\n",
        "            layers.Input((6,)), # the input layer (corresponds to the features)\n",
        "            layers.Dense(1), # out layer (contains the 6 weights and a bias term)\n",
        "        ], name='multiple_linear_regression_model')\n",
        "\n",
        "print('model summary')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model summary\n",
            "Model: \"multiple_linear_regression_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 7\n",
            "Trainable params: 7\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EafDyr3FSoK",
        "colab_type": "text"
      },
      "source": [
        "The model we defined above is a multiple linear model that could also be called a *single-layer fully-connected neural network*. We defined it using the `Dense` class. Note that we passed two arguments into the `Sequential` class. The first one specifies the input feature dimension, which is 6 (corresponding to the number of x features), and the second one is the output feature dimension, which is a single scalar and therefore 1. Each input *feature* has a corresponding *weight* and there is one bias term. The *weights* and *bias*, or *parameters*, are connected to the single output *neuron*.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9f-65plLUGO",
        "colab_type": "text"
      },
      "source": [
        "### Define the loss function, optimization algorithm, and metrics \n",
        "\n",
        "After defining the model, we need to configure the *loss function*, *optimization algorithm*, and *metrics* for the model. We will use mean squared error for the loss function, stochastic gradient descent for the optimization algorithm, and mean absolute error and mean squared error for the metrics. We glue these configurations to our model using the `compile` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRlVR35NLWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.compile(loss=keras.losses.MSE,\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "              metrics=[keras.metrics.MAE, keras.metrics.MSE])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD4l2J8zLGWN",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train the model\n",
        "Now that we have a model, it's time to train it. We will train the model for 100 *epochs* (i.e., iterations), and record the training and validation metrics in the `history` object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtKfpJA4OpPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec006837-2cc7-4b9d-b578-f38011ffa88a"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 33.3436 - mean_absolute_error: 5.6296 - mean_squared_error: 33.3436 - val_loss: 22.8826 - val_mean_absolute_error: 4.6577 - val_mean_squared_error: 22.8826\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 16.7526 - mean_absolute_error: 3.9947 - mean_squared_error: 16.7526 - val_loss: 11.4207 - val_mean_absolute_error: 3.2561 - val_mean_squared_error: 11.4207\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 8.4914 - mean_absolute_error: 2.8171 - mean_squared_error: 8.4914 - val_loss: 5.9405 - val_mean_absolute_error: 2.2980 - val_mean_squared_error: 5.9405\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 4.4807 - mean_absolute_error: 2.0120 - mean_squared_error: 4.4807 - val_loss: 3.1645 - val_mean_absolute_error: 1.6041 - val_mean_squared_error: 3.1645\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 2.4357 - mean_absolute_error: 1.4428 - mean_squared_error: 2.4357 - val_loss: 1.8051 - val_mean_absolute_error: 1.1306 - val_mean_squared_error: 1.8051\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.4226 - mean_absolute_error: 1.0488 - mean_squared_error: 1.4226 - val_loss: 1.1566 - val_mean_absolute_error: 0.8942 - val_mean_squared_error: 1.1566\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9280 - mean_absolute_error: 0.7984 - mean_squared_error: 0.9280 - val_loss: 0.8443 - val_mean_absolute_error: 0.7438 - val_mean_squared_error: 0.8443\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6832 - mean_absolute_error: 0.6542 - mean_squared_error: 0.6832 - val_loss: 0.7039 - val_mean_absolute_error: 0.6964 - val_mean_squared_error: 0.7039\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5668 - mean_absolute_error: 0.5918 - mean_squared_error: 0.5668 - val_loss: 0.6352 - val_mean_absolute_error: 0.6761 - val_mean_squared_error: 0.6352\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5056 - mean_absolute_error: 0.5575 - mean_squared_error: 0.5056 - val_loss: 0.5959 - val_mean_absolute_error: 0.6583 - val_mean_squared_error: 0.5959\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4690 - mean_absolute_error: 0.5362 - mean_squared_error: 0.4690 - val_loss: 0.5810 - val_mean_absolute_error: 0.6475 - val_mean_squared_error: 0.5810\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4526 - mean_absolute_error: 0.5275 - mean_squared_error: 0.4526 - val_loss: 0.5732 - val_mean_absolute_error: 0.6383 - val_mean_squared_error: 0.5732\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4435 - mean_absolute_error: 0.5246 - mean_squared_error: 0.4435 - val_loss: 0.5699 - val_mean_absolute_error: 0.6312 - val_mean_squared_error: 0.5699\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4367 - mean_absolute_error: 0.5212 - mean_squared_error: 0.4367 - val_loss: 0.5682 - val_mean_absolute_error: 0.6275 - val_mean_squared_error: 0.5682\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4344 - mean_absolute_error: 0.5210 - mean_squared_error: 0.4344 - val_loss: 0.5671 - val_mean_absolute_error: 0.6235 - val_mean_squared_error: 0.5671\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4316 - mean_absolute_error: 0.5204 - mean_squared_error: 0.4316 - val_loss: 0.5665 - val_mean_absolute_error: 0.6219 - val_mean_squared_error: 0.5665\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4302 - mean_absolute_error: 0.5200 - mean_squared_error: 0.4302 - val_loss: 0.5661 - val_mean_absolute_error: 0.6211 - val_mean_squared_error: 0.5661\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4282 - mean_absolute_error: 0.5212 - mean_squared_error: 0.4282 - val_loss: 0.5660 - val_mean_absolute_error: 0.6216 - val_mean_squared_error: 0.5660\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4264 - mean_absolute_error: 0.5215 - mean_squared_error: 0.4264 - val_loss: 0.5639 - val_mean_absolute_error: 0.6197 - val_mean_squared_error: 0.5639\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4249 - mean_absolute_error: 0.5201 - mean_squared_error: 0.4249 - val_loss: 0.5624 - val_mean_absolute_error: 0.6186 - val_mean_squared_error: 0.5624\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4231 - mean_absolute_error: 0.5189 - mean_squared_error: 0.4231 - val_loss: 0.5606 - val_mean_absolute_error: 0.6171 - val_mean_squared_error: 0.5606\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4219 - mean_absolute_error: 0.5182 - mean_squared_error: 0.4219 - val_loss: 0.5589 - val_mean_absolute_error: 0.6156 - val_mean_squared_error: 0.5589\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4213 - mean_absolute_error: 0.5180 - mean_squared_error: 0.4213 - val_loss: 0.5589 - val_mean_absolute_error: 0.6160 - val_mean_squared_error: 0.5589\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4206 - mean_absolute_error: 0.5186 - mean_squared_error: 0.4206 - val_loss: 0.5577 - val_mean_absolute_error: 0.6151 - val_mean_squared_error: 0.5577\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4183 - mean_absolute_error: 0.5174 - mean_squared_error: 0.4183 - val_loss: 0.5552 - val_mean_absolute_error: 0.6128 - val_mean_squared_error: 0.5552\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4176 - mean_absolute_error: 0.5164 - mean_squared_error: 0.4176 - val_loss: 0.5546 - val_mean_absolute_error: 0.6125 - val_mean_squared_error: 0.5546\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4161 - mean_absolute_error: 0.5164 - mean_squared_error: 0.4161 - val_loss: 0.5522 - val_mean_absolute_error: 0.6102 - val_mean_squared_error: 0.5522\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4151 - mean_absolute_error: 0.5152 - mean_squared_error: 0.4151 - val_loss: 0.5506 - val_mean_absolute_error: 0.6086 - val_mean_squared_error: 0.5506\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4140 - mean_absolute_error: 0.5147 - mean_squared_error: 0.4140 - val_loss: 0.5487 - val_mean_absolute_error: 0.6068 - val_mean_squared_error: 0.5487\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4142 - mean_absolute_error: 0.5146 - mean_squared_error: 0.4142 - val_loss: 0.5491 - val_mean_absolute_error: 0.6079 - val_mean_squared_error: 0.5491\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4121 - mean_absolute_error: 0.5138 - mean_squared_error: 0.4121 - val_loss: 0.5477 - val_mean_absolute_error: 0.6066 - val_mean_squared_error: 0.5477\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4104 - mean_absolute_error: 0.5127 - mean_squared_error: 0.4104 - val_loss: 0.5469 - val_mean_absolute_error: 0.6062 - val_mean_squared_error: 0.5469\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4090 - mean_absolute_error: 0.5123 - mean_squared_error: 0.4090 - val_loss: 0.5452 - val_mean_absolute_error: 0.6046 - val_mean_squared_error: 0.5452\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4079 - mean_absolute_error: 0.5119 - mean_squared_error: 0.4079 - val_loss: 0.5443 - val_mean_absolute_error: 0.6038 - val_mean_squared_error: 0.5443\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4072 - mean_absolute_error: 0.5122 - mean_squared_error: 0.4072 - val_loss: 0.5431 - val_mean_absolute_error: 0.6028 - val_mean_squared_error: 0.5431\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4063 - mean_absolute_error: 0.5113 - mean_squared_error: 0.4063 - val_loss: 0.5414 - val_mean_absolute_error: 0.6012 - val_mean_squared_error: 0.5414\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4047 - mean_absolute_error: 0.5103 - mean_squared_error: 0.4047 - val_loss: 0.5406 - val_mean_absolute_error: 0.6006 - val_mean_squared_error: 0.5406\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4037 - mean_absolute_error: 0.5103 - mean_squared_error: 0.4037 - val_loss: 0.5384 - val_mean_absolute_error: 0.5983 - val_mean_squared_error: 0.5384\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4025 - mean_absolute_error: 0.5092 - mean_squared_error: 0.4025 - val_loss: 0.5377 - val_mean_absolute_error: 0.5980 - val_mean_squared_error: 0.5377\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4010 - mean_absolute_error: 0.5087 - mean_squared_error: 0.4010 - val_loss: 0.5354 - val_mean_absolute_error: 0.5957 - val_mean_squared_error: 0.5354\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3997 - mean_absolute_error: 0.5077 - mean_squared_error: 0.3997 - val_loss: 0.5340 - val_mean_absolute_error: 0.5952 - val_mean_squared_error: 0.5340\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3993 - mean_absolute_error: 0.5075 - mean_squared_error: 0.3993 - val_loss: 0.5330 - val_mean_absolute_error: 0.5943 - val_mean_squared_error: 0.5330\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3979 - mean_absolute_error: 0.5067 - mean_squared_error: 0.3979 - val_loss: 0.5319 - val_mean_absolute_error: 0.5936 - val_mean_squared_error: 0.5319\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3976 - mean_absolute_error: 0.5066 - mean_squared_error: 0.3976 - val_loss: 0.5313 - val_mean_absolute_error: 0.5924 - val_mean_squared_error: 0.5313\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3961 - mean_absolute_error: 0.5063 - mean_squared_error: 0.3961 - val_loss: 0.5302 - val_mean_absolute_error: 0.5919 - val_mean_squared_error: 0.5302\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3953 - mean_absolute_error: 0.5058 - mean_squared_error: 0.3953 - val_loss: 0.5283 - val_mean_absolute_error: 0.5916 - val_mean_squared_error: 0.5283\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3957 - mean_absolute_error: 0.5061 - mean_squared_error: 0.3957 - val_loss: 0.5267 - val_mean_absolute_error: 0.5916 - val_mean_squared_error: 0.5267\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3945 - mean_absolute_error: 0.5046 - mean_squared_error: 0.3945 - val_loss: 0.5260 - val_mean_absolute_error: 0.5906 - val_mean_squared_error: 0.5260\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3932 - mean_absolute_error: 0.5040 - mean_squared_error: 0.3932 - val_loss: 0.5254 - val_mean_absolute_error: 0.5895 - val_mean_squared_error: 0.5254\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3917 - mean_absolute_error: 0.5034 - mean_squared_error: 0.3917 - val_loss: 0.5248 - val_mean_absolute_error: 0.5883 - val_mean_squared_error: 0.5248\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3905 - mean_absolute_error: 0.5033 - mean_squared_error: 0.3905 - val_loss: 0.5244 - val_mean_absolute_error: 0.5873 - val_mean_squared_error: 0.5244\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3895 - mean_absolute_error: 0.5031 - mean_squared_error: 0.3895 - val_loss: 0.5230 - val_mean_absolute_error: 0.5867 - val_mean_squared_error: 0.5230\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3910 - mean_absolute_error: 0.5036 - mean_squared_error: 0.3910 - val_loss: 0.5215 - val_mean_absolute_error: 0.5865 - val_mean_squared_error: 0.5215\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3880 - mean_absolute_error: 0.5018 - mean_squared_error: 0.3880 - val_loss: 0.5209 - val_mean_absolute_error: 0.5855 - val_mean_squared_error: 0.5209\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3872 - mean_absolute_error: 0.5017 - mean_squared_error: 0.3872 - val_loss: 0.5203 - val_mean_absolute_error: 0.5856 - val_mean_squared_error: 0.5203\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3860 - mean_absolute_error: 0.5014 - mean_squared_error: 0.3860 - val_loss: 0.5186 - val_mean_absolute_error: 0.5850 - val_mean_squared_error: 0.5186\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3864 - mean_absolute_error: 0.5024 - mean_squared_error: 0.3864 - val_loss: 0.5172 - val_mean_absolute_error: 0.5844 - val_mean_squared_error: 0.5172\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3841 - mean_absolute_error: 0.4999 - mean_squared_error: 0.3841 - val_loss: 0.5162 - val_mean_absolute_error: 0.5841 - val_mean_squared_error: 0.5162\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3845 - mean_absolute_error: 0.4997 - mean_squared_error: 0.3845 - val_loss: 0.5157 - val_mean_absolute_error: 0.5843 - val_mean_squared_error: 0.5157\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3827 - mean_absolute_error: 0.4995 - mean_squared_error: 0.3827 - val_loss: 0.5147 - val_mean_absolute_error: 0.5839 - val_mean_squared_error: 0.5147\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3830 - mean_absolute_error: 0.4999 - mean_squared_error: 0.3830 - val_loss: 0.5135 - val_mean_absolute_error: 0.5830 - val_mean_squared_error: 0.5135\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3821 - mean_absolute_error: 0.4977 - mean_squared_error: 0.3821 - val_loss: 0.5128 - val_mean_absolute_error: 0.5833 - val_mean_squared_error: 0.5128\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3806 - mean_absolute_error: 0.4978 - mean_squared_error: 0.3806 - val_loss: 0.5120 - val_mean_absolute_error: 0.5831 - val_mean_squared_error: 0.5120\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3804 - mean_absolute_error: 0.4978 - mean_squared_error: 0.3804 - val_loss: 0.5110 - val_mean_absolute_error: 0.5828 - val_mean_squared_error: 0.5110\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3787 - mean_absolute_error: 0.4969 - mean_squared_error: 0.3787 - val_loss: 0.5103 - val_mean_absolute_error: 0.5828 - val_mean_squared_error: 0.5103\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3782 - mean_absolute_error: 0.4970 - mean_squared_error: 0.3782 - val_loss: 0.5100 - val_mean_absolute_error: 0.5830 - val_mean_squared_error: 0.5100\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3779 - mean_absolute_error: 0.4973 - mean_squared_error: 0.3779 - val_loss: 0.5092 - val_mean_absolute_error: 0.5829 - val_mean_squared_error: 0.5092\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3766 - mean_absolute_error: 0.4967 - mean_squared_error: 0.3766 - val_loss: 0.5083 - val_mean_absolute_error: 0.5827 - val_mean_squared_error: 0.5083\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3767 - mean_absolute_error: 0.4961 - mean_squared_error: 0.3767 - val_loss: 0.5076 - val_mean_absolute_error: 0.5825 - val_mean_squared_error: 0.5076\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3751 - mean_absolute_error: 0.4957 - mean_squared_error: 0.3751 - val_loss: 0.5072 - val_mean_absolute_error: 0.5826 - val_mean_squared_error: 0.5072\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3746 - mean_absolute_error: 0.4959 - mean_squared_error: 0.3746 - val_loss: 0.5064 - val_mean_absolute_error: 0.5824 - val_mean_squared_error: 0.5064\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3744 - mean_absolute_error: 0.4954 - mean_squared_error: 0.3744 - val_loss: 0.5054 - val_mean_absolute_error: 0.5821 - val_mean_squared_error: 0.5054\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3731 - mean_absolute_error: 0.4946 - mean_squared_error: 0.3731 - val_loss: 0.5045 - val_mean_absolute_error: 0.5817 - val_mean_squared_error: 0.5045\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3721 - mean_absolute_error: 0.4943 - mean_squared_error: 0.3721 - val_loss: 0.5037 - val_mean_absolute_error: 0.5814 - val_mean_squared_error: 0.5037\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3720 - mean_absolute_error: 0.4936 - mean_squared_error: 0.3720 - val_loss: 0.5031 - val_mean_absolute_error: 0.5814 - val_mean_squared_error: 0.5031\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3712 - mean_absolute_error: 0.4939 - mean_squared_error: 0.3712 - val_loss: 0.5028 - val_mean_absolute_error: 0.5814 - val_mean_squared_error: 0.5028\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3704 - mean_absolute_error: 0.4936 - mean_squared_error: 0.3704 - val_loss: 0.5019 - val_mean_absolute_error: 0.5812 - val_mean_squared_error: 0.5019\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3700 - mean_absolute_error: 0.4935 - mean_squared_error: 0.3700 - val_loss: 0.5006 - val_mean_absolute_error: 0.5805 - val_mean_squared_error: 0.5006\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3693 - mean_absolute_error: 0.4925 - mean_squared_error: 0.3693 - val_loss: 0.4997 - val_mean_absolute_error: 0.5798 - val_mean_squared_error: 0.4997\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3691 - mean_absolute_error: 0.4927 - mean_squared_error: 0.3691 - val_loss: 0.4991 - val_mean_absolute_error: 0.5800 - val_mean_squared_error: 0.4991\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3676 - mean_absolute_error: 0.4917 - mean_squared_error: 0.3676 - val_loss: 0.4984 - val_mean_absolute_error: 0.5797 - val_mean_squared_error: 0.4984\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3672 - mean_absolute_error: 0.4916 - mean_squared_error: 0.3672 - val_loss: 0.4979 - val_mean_absolute_error: 0.5798 - val_mean_squared_error: 0.4979\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3665 - mean_absolute_error: 0.4912 - mean_squared_error: 0.3665 - val_loss: 0.4972 - val_mean_absolute_error: 0.5793 - val_mean_squared_error: 0.4972\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3662 - mean_absolute_error: 0.4909 - mean_squared_error: 0.3662 - val_loss: 0.4967 - val_mean_absolute_error: 0.5792 - val_mean_squared_error: 0.4967\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3651 - mean_absolute_error: 0.4901 - mean_squared_error: 0.3651 - val_loss: 0.4962 - val_mean_absolute_error: 0.5792 - val_mean_squared_error: 0.4962\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3649 - mean_absolute_error: 0.4899 - mean_squared_error: 0.3649 - val_loss: 0.4955 - val_mean_absolute_error: 0.5789 - val_mean_squared_error: 0.4955\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3639 - mean_absolute_error: 0.4896 - mean_squared_error: 0.3639 - val_loss: 0.4949 - val_mean_absolute_error: 0.5790 - val_mean_squared_error: 0.4949\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3634 - mean_absolute_error: 0.4896 - mean_squared_error: 0.3634 - val_loss: 0.4944 - val_mean_absolute_error: 0.5788 - val_mean_squared_error: 0.4944\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3631 - mean_absolute_error: 0.4893 - mean_squared_error: 0.3631 - val_loss: 0.4940 - val_mean_absolute_error: 0.5791 - val_mean_squared_error: 0.4940\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3624 - mean_absolute_error: 0.4895 - mean_squared_error: 0.3624 - val_loss: 0.4932 - val_mean_absolute_error: 0.5785 - val_mean_squared_error: 0.4932\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3629 - mean_absolute_error: 0.4895 - mean_squared_error: 0.3629 - val_loss: 0.4926 - val_mean_absolute_error: 0.5784 - val_mean_squared_error: 0.4926\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3615 - mean_absolute_error: 0.4891 - mean_squared_error: 0.3615 - val_loss: 0.4920 - val_mean_absolute_error: 0.5781 - val_mean_squared_error: 0.4920\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3607 - mean_absolute_error: 0.4882 - mean_squared_error: 0.3607 - val_loss: 0.4916 - val_mean_absolute_error: 0.5783 - val_mean_squared_error: 0.4916\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3622 - mean_absolute_error: 0.4900 - mean_squared_error: 0.3622 - val_loss: 0.4910 - val_mean_absolute_error: 0.5776 - val_mean_squared_error: 0.4910\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3596 - mean_absolute_error: 0.4875 - mean_squared_error: 0.3596 - val_loss: 0.4905 - val_mean_absolute_error: 0.5777 - val_mean_squared_error: 0.4905\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3595 - mean_absolute_error: 0.4881 - mean_squared_error: 0.3595 - val_loss: 0.4900 - val_mean_absolute_error: 0.5773 - val_mean_squared_error: 0.4900\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3588 - mean_absolute_error: 0.4878 - mean_squared_error: 0.3588 - val_loss: 0.4896 - val_mean_absolute_error: 0.5770 - val_mean_squared_error: 0.4896\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3587 - mean_absolute_error: 0.4873 - mean_squared_error: 0.3587 - val_loss: 0.4890 - val_mean_absolute_error: 0.5770 - val_mean_squared_error: 0.4890\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3582 - mean_absolute_error: 0.4874 - mean_squared_error: 0.3582 - val_loss: 0.4887 - val_mean_absolute_error: 0.5776 - val_mean_squared_error: 0.4887\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3582 - mean_absolute_error: 0.4879 - mean_squared_error: 0.3582 - val_loss: 0.4882 - val_mean_absolute_error: 0.5776 - val_mean_squared_error: 0.4882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa_D6YswOvo1",
        "colab_type": "text"
      },
      "source": [
        "We get to ~0.3542 validation mean squared error after training for 100 epochs on the training dataset. For reference, the same metric was ~0.7807 in the single-variable linear regression model (from the previous lesson).  Let's visualize the model's training progress using the stats stored in the history object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QGfBDMVPPYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "0af9e2f0-0dd3-4ce6-a352-f80dd216a324"
      },
      "source": [
        "# create a dataframe to store the history \n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "print(hist.tail())\n",
        "\n",
        "# visualize the mean squared error over the training process\n",
        "hist.plot.line(x='epoch', y='val_mse');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        loss  mean_absolute_error  ...  val_mean_squared_error  epoch\n",
            "95  0.359492             0.488087  ...                0.489979     95\n",
            "96  0.358822             0.487758  ...                0.489611     96\n",
            "97  0.358735             0.487253  ...                0.489042     97\n",
            "98  0.358198             0.487372  ...                0.488747     98\n",
            "99  0.358247             0.487920  ...                0.488234     99\n",
            "\n",
            "[5 rows x 7 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgV4Jk8TV_zJ",
        "colab_type": "text"
      },
      "source": [
        "From the plot we can see that our model *converges* around the 10th epoch. In other words, the most optimal parameters (weights and bias) are found after about the 10 training iteration. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3-3e5HYT0N9",
        "colab_type": "text"
      },
      "source": [
        "## 4. Evaluate the model\n",
        "Now that we trained our model, it's time to evaluate it using the *test* dataset, which we did not use when training the model. This gives us a sense of how well our model predicts unseen data, which is the case when we use it in the real world. We will use the `evaluate` method to test the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwP7HvCkSLAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "600fe704-8303-4524-cfab-9b7574873d93"
      },
      "source": [
        "loss, mae, mse = model.evaluate(test_features, test_labels)\n",
        "print('Test set Mean Absolute Error: ', round(mae, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2282 - mean_absolute_error: 0.4185 - mean_squared_error: 0.2282\n",
            "Test set Mean Absolute Error:  0.4185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk8eZggWk_F",
        "colab_type": "text"
      },
      "source": [
        "The average (absolute) error is around +/- 0.4185 units for happiness Score, which is better than the single-variable linear model (+/- 0.516). Is this good? We'll leave that decision up to you. Let's also visualize the prediction and real happiness Score values using data in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6-UEhdrW9mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "062129bf-8048-4f94-9c73-130e71f8b21d"
      },
      "source": [
        "test_predictions = model.predict(test_features).flatten()\n",
        "\n",
        "ax = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [Happiness Score]')\n",
        "plt.ylabel('Predictions [Happiness Score]')\n",
        "lims = [0, max(test_labels) + 1] # [0, 31]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kebG1ga8XDJj",
        "colab_type": "text"
      },
      "source": [
        "It looks like our model predicts reasonably well. Let's take a look at the error distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5h3qJqXLnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "00cb2712-ebc7-441b-9361-8c994a290838"
      },
      "source": [
        "error = test_predictions.reshape(-1, 1) - test_labels\n",
        "plt.hist(error, bins = 10)\n",
        "plt.xlabel(\"Prediction Error [Happiness Score]\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAVN0lEQVR4nO3deZRcZZ3G8echYQdZpGVwiR1GXNAZgWkZRFAWFwRHVBgWZdHRCYLDER0cw+DxKOM4oKNwXDEiggMDCpLjEgVZEsAzQAgYQgCRLSjImKAwGkSE8Js/3reS25VeKt39VjVvfz/n1Om71X1/dav6qVv31n3LESEAQH3W63UBAIAyCHgAqBQBDwCVIuABoFIEPABUanqvC2jaZpttor+/v9dlAMAzxk033fRwRPQNNW9SBXx/f78WLVrU6zIA4BnD9v3DzeMQDQBUioAHgEoR8ABQKQIeACpFwANApQh4AKhU0a9J2l4m6Q+SVkl6KiIGSrYHAFijG9+D3zsiHu5COwCABg7RAEClSu/Bh6Sf2A5JX4uIOe0L2J4laZYkzZgxY8wN9c+eN+b7jseyUw/oSbvorl69vnqJ1/YzX+k9+D0iYhdJb5b0AduvbV8gIuZExEBEDPT1DdmdAgBgDIoGfEQ8mP8ulzRX0q4l2wMArFEs4G1vanvz1rCkN0paWqo9AMBgJY/Bbytpru1WO/8dEZcWbA8A0FAs4CPiXkmvLLV+AMDI+JokAFSKgAeAShHwAFApAh4AKkXAA0ClCHgAqBQBDwCVIuABoFIEPABUioAHgEoR8ABQKQIeACpFwANApQh4AKgUAQ8AlSLgAaBSBDwAVIqAB4BKEfAAUCkCHgAqRcADQKUIeACoFAEPAJUi4AGgUgQ8AFSKgAeAShHwAFApAh4AKkXAA0ClCHgAqBQBDwCVIuABoFLFA972NNs/s/3D0m0BANboxh78ByXd0YV2AAANRQPe9vMlHSDprJLtAADWVnoP/gxJ/yLp6eEWsD3L9iLbi1asWFG4HACYOooFvO23SFoeETeNtFxEzImIgYgY6OvrK1UOAEw5JffgXyPprbaXSbpQ0j62zyvYHgCgoVjAR8RJEfH8iOiXdJikqyLiiFLtAQAG43vwAFCp6d1oJCIWSFrQjbYAAAl78ABQKQIeACpFwANApQh4AKgUAQ8AlSLgAaBSBDwAVIqAB4BKEfAAUCkCHgAqRcADQKUIeACoFAEPAJUi4AGgUgQ8AFSKgAeAShHwAFApAh4AKkXAA0ClCHgAqBQBDwCVIuABoFIEPABUioAHgEoR8ABQKQIeACpFwANApQh4AKgUAQ8AlSLgAaBSBDwAVIqAB4BKFQt42xvZXmj7Ftu32f5kqbYAAGubXnDdT0jaJyJW2l5f0k9t/zgiri/YJgAgKxbwERGSVubR9fMtSrUHABis6DF429NsL5a0XNLlEXFDyfYAAGuUPESjiFglaSfbW0qaa/sVEbG0uYztWZJmSdKMGTNKllNE/+x5vS6h65adekDP2p6K2xsYq658iyYiHpU0X9J+Q8ybExEDETHQ19fXjXIAYEoo+S2avrznLtsbS3qDpJ+Xag8AMFhHAW/7NZ1Ma7OdpPm2l0i6UekY/A/XvUQAwFh0egz+i5J26WDaahGxRNLOY6wLADBOIwa87VdL2l1Sn+0PN2Y9S9K0koUBAMZntD34DSRtlpfbvDH995IOLlUUAGD8Rgz4iLha0tW2z4mI+7tUEwBgAnR6DH5D23Mk9TfvExH7lCgKADB+nQb8RZLOlHSWpFXlygEATJROA/6piPhq0UoAABOq0wudfmD7ONvb2d66dStaGQBgXDrdgz86//1IY1pI2n5iywEATJSOAj4iZpYuBAAwsToKeNtHDTU9Ir41seUAACZKp4doXtUY3kjSvpJulkTAA8Ak1ekhmuOb47mXyAuLVAQAmBBj7S74MUkclweASazTY/A/0JrfU50m6WWSvlOqKADA+HV6DP4/G8NPSbo/Ih4oUA8AYIJ0dIgmdzr2c6UeJbeS9OeSRQEAxq/TX3Q6RNJCSX8v6RBJN9imu2AAmMQ6PURzsqRXRcRyKf3eqqQrJF1cqjAAwPh0+i2a9Vrhnv12He4LAOiBTvfgL7V9maQL8vihkn5UpiQAwEQY7TdZXyRp24j4iO13SNojz7pO0vmliwMAjN1oe/BnSDpJkiLiEkmXSJLtv8rz/q5odQCAMRvtOPq2EXFr+8Q8rb9IRQCACTFawG85wryNJ7IQAMDEGi3gF9n+x/aJtt8n6aYyJQEAJsJox+BPkDTX9ru0JtAHJG0g6e0lCwMAjM+IAR8Rv5G0u+29Jb0iT54XEVcVrwwAMC6d9gc/X9L8wrUAACYQV6MCQKUIeACoFAEPAJUi4AGgUgQ8AFSqWMDbfoHt+bZvt32b7Q+WagsAsLZOuwsei6ck/XNE3Gx7c0k32b48Im4v2CYAICu2Bx8RD0XEzXn4D5LukPS8Uu0BAAYruQe/mu1+STtLumGIebMkzZKkGTNmdKMcjFP/7Hm9LgFdMBWf52WnHtDrEiZU8ZOstjeT9F1JJ0TE79vnR8SciBiIiIG+vr7S5QDAlFE04G2vrxTu5+cfDAEAdEnJb9FY0jck3RERny/VDgBgaCX34F8j6UhJ+9henG/7F2wPANBQ7CRrRPxUkkutHwAwMq5kBYBKEfAAUCkCHgAqRcADQKUIeACoFAEPAJUi4AGgUgQ8AFSKgAeAShHwAFApAh4AKkXAA0ClCHgAqBQBDwCVIuABoFIEPABUioAHgEoR8ABQKQIeACpFwANApQh4AKgUAQ8AlSLgAaBSBDwAVIqAB4BKEfAAUCkCHgAqRcADQKUIeACoFAEPAJUi4AGgUgQ8AFSqWMDbPtv2cttLS7UBABheyT34cyTtV3D9AIARFAv4iLhG0u9KrR8AMLLpvS7A9ixJsyRpxowZPa4GwFTWP3teT9pdduoBRdbb85OsETEnIgYiYqCvr6/X5QBANXoe8ACAMgh4AKhUya9JXiDpOkkvsf2A7feWagsAsLZiJ1kj4vBS6wYAjI5DNABQKQIeACpFwANApQh4AKgUAQ8AlSLgAaBSBDwAVIqAB4BKEfAAUCkCHgAqRcADQKUIeACoFAEPAJUi4AGgUgQ8AFSKgAeAShHwAFApAh4AKkXAA0ClCHgAqBQBDwCVIuABoFIEPABUioAHgEoR8ABQKQIeACpFwANApQh4AKgUAQ8AlSLgAaBSBDwAVIqAB4BKEfAAUKmiAW97P9t32r7b9uySbQEABisW8LanSfqypDdL2lHS4bZ3LNUeAGCwknvwu0q6OyLujYg/S7pQ0oEF2wMANEwvuO7nSfpVY/wBSX/bvpDtWZJm5dGVtu8sWNO62EbSw70uYh080+qVqLlbqLk7xlyzTxtXuy8cbkbJgO9IRMyRNKfXdbSzvSgiBnpdR6eeafVK1Nwt1Nwdk7HmkodoHpT0gsb48/M0AEAXlAz4GyXtYHum7Q0kHSbp+wXbAwA0FDtEExFP2f4nSZdJmibp7Ii4rVR7BUy6w0ajeKbVK1Fzt1Bzd0y6mh0Rva4BAFAAV7ICQKUIeACo1JQNeNtb277c9l3571bDLLfK9uJ8+35j+kzbN+RuGL6dTyT3vGbbO9m+zvZttpfYPrQx7xzb9zUez04Fax2xmwrbG+btdnfejv2NeSfl6XfaflOpGsdQ84dt356365W2X9iYN+TrZBLU/G7bKxq1va8x7+j8WrrL9tGTqObTG/X+wvajjXld3862z7a93PbSYebb9hfy41lie5fGvJ5s49UiYkreJH1G0uw8PFvSacMst3KY6d+RdFgePlPSsZOhZkkvlrRDHn6upIckbZnHz5F0cBfqnCbpHknbS9pA0i2Sdmxb5jhJZ+bhwyR9Ow/vmJffUNLMvJ5pk6TmvSVtkoePbdU80utkEtT8bklfGuK+W0u6N//dKg9vNRlqblv+eKUvaPRyO79W0i6Slg4zf39JP5ZkSbtJuqGX27h5m7J78ErdJpybh8+V9LZO72jbkvaRdPFY7j8Oo9YcEb+IiLvy8K8lLZfU14XamjrppqL5WC6WtG/ergdKujAinoiI+yTdndfX85ojYn5E/DGPXq90bUcvjac7kDdJujwifhcRj0i6XNJ+hepsWteaD5d0QRfqGlZEXCPpdyMscqCkb0VyvaQtbW+n3m3j1aZywG8bEQ/l4f+VtO0wy21ke5Ht6223AvXZkh6NiKfy+ANKXTOU1mnNkiTbuyrtJd3TmPzv+WPk6bY3LFTnUN1UtG+f1cvk7fh/Stu1k/uWsK7tvldpr61lqNdJaZ3WfFB+zi+23br4cNJv53wIbKakqxqTe7GdRzPcY+rVNl6t510VlGT7Ckl/McSsk5sjERG2h/u+6Asj4kHb20u6yvatSmFUxATVrLwH8V+Sjo6Ip/Pkk5TeGDZQ+s7uRyWdMhF1TyW2j5A0IOl1jclrvU4i4p6h19BVP5B0QUQ8YfsYpU9N+/S4pk4dJuniiFjVmDZZt/OkVHXAR8Trh5tn+ze2t4uIh3IYLh9mHQ/mv/faXiBpZ0nfVfoYNj3vfU5YNwwTUbPtZ0maJ+nk/JGxte7W3v8Ttr8p6cSJqHkInXRT0VrmAdvTJW0h6bcd3reEjtq1/XqlN9vXRcQTrenDvE5KB8+oNUfEbxujZymdx2ndd6+2+y6Y8ArXti7P72GSPtCc0KPtPJrhHlOvtvEa3T5hMVlukj6rwScsPzPEMltJ2jAPbyPpLuUTQpIu0uCTrMdNkpo3kHSlpBOGmLdd/mtJZ0g6tVCd05VOKM3UmhNpL29b5gMafJL1O3n45Rp8kvVedeckayc1t8Jkh05fJ5Og5u0aw2+XdH0e3lrSfbn2rfLw1pOh5rzcSyUtU74Ys5fbObfXr+FPsh6gwSdZF/ZyGw+qrZuNTaab0vHeK/OL5IrWhlf66H1WHt5d0q35RXirpPc27r+9pIVKJwEvar3wJkHNR0h6UtLixm2nPO+q/DiWSjpP0mYFa91f0i9yIJ6cp50i6a15eKO83e7O23H7xn1Pzve7U9Kbu/iaGK3mKyT9prFdvz/a62QS1Pwfkm7Ltc2X9NLGff8hb/+7Jb1nstScxz+hth2QXm1npZO8D+X/qweUzr+8X9L783wr/bjRPbmugV5v49aNrgoAoFJT+Vs0AFA1Ah4AKkXAA0ClCHgAqBQBDwCVIuAr1Ohxb6nti2xvMo51nWP74Dx8lu0dR1h2L9u7N8bfb/uosbbdWE+/7ccbvQgunoj1jtDeMtu32h7I4wtaw416huxZcJztnpIvpOoa27s59ea52PYdtj9RuL35tlc2tyfKqfpK1ins8YjYSZJsn6/0nd3Pt2Y2rsBdJxHxvlEW2UvSSkn/k5c/c13bGME9rcc0HNvTonFZe/v4MPex0sU0T7fN2jsiHh57uesuIj7ezfaycyUdEhG32J4m6SXjXeFI2z0i9s5XoKIL2IOv37WSXpT3rq/NfWjfbnua7c/avjF3RHWMtLpv6y859dd9haTntFbU3JN16tP7Ztu3OPWN3q/0RvKhvDe4p+1P2D4xL79T7iBqie25zn3Z53WeZnuhU9/fe67Lg8t7g5+zfYukVw8x/uH8SWap7RPyffrz4/uW0kVfLxipjVHa78/b9eZ82z1P38v2Nbbn5bbOtL1eo+bTnfrsv9J2X57e/LS0zPYn8zpvtf3SPH1Tp/7JF9r+me0D8/SX52mL8zbeIS87Lz9HS934bYCG5yhdxKOIWBURt+f1bWb7m7ntJbYPytMPz9OW2j5thOfhiEY9X8tvHui2bl9Zxa38TbnPbKVPaN9T6rt8L0mPSZqZ582S9LE8vKGkRUqXj79DqVvTaUr9yT+q3Ie8Uj8aA0rdD/+qsa7WFbWfkHRio47V45KWKPXfIqWrFs9orPNzeXh/SVcM8Xj6JT2uwVfn7pnnhdIeqNrHJf2N0pWFm0raTOmKzp3z+p6WtNsw22+ZpG0a4wuUrqpttX278mXrkjaRtFEe3kHSojy8l6Q/KV3xPC1v04MbNb4rD39cub92NfrrzzUcn4eP05orlT8t6Yg8vKXSFaGbSvpiY50bSNpY0kGSvt54HFsM8Vg/LukRSXMlHdN4LKe1nqM8vlV+PfwyP//Tla6MftsQ2/1lSp2crZ/HvyLpqLbtOTDUtuc2sTf24Ou0se3FSqH9S0nfyNMXRupjXZLeKOmovNwNSt0g7KD04wYXRNqb+7UGd9Xaspuka1rrioiR+sqW7S2UfnTk6jzp3NxOyyX5701K4TuUeyJip8bt2jx9lVLnbxpifA9JcyPisYhYmdtpfUK4PxodsXXgXa22ld6IWtaX9HWnXkYvUvrBkpaFkfo9X6V0ufseefrTkr6dh89rTG831HZ5o6TZ+XlboNTlwwxJ10n6V9sfVepx8XGlN7c35E9Ie0bEWr2gRsQpSm/aP5H0TkmX5lmvV7r8vrXcI5JeJWlBRKyIdIjvfK15HpvbfV+lN9cbc537Kr3Rocs4Bl+n1cfgW9KhZj3WnKS0h3hZ23LN8OqWVq+Mq7Tur8k/xeDjve3jw3ls9EU68iGl/mleqXTI80+Nee39gAzXL8hw04faLpZ0UETc2bbsHbZvUOr46ke2j4mIq5x+Pm5/SZ+yfWUO9MGNp+52v2r765JW2H72MPWMpLndLenciDhpDOvBBGIPfuq6TNKxtteXJNsvtr2ppGskHZqP0W+n9DN17a6X9FrbM/N9t87T/yBp8/aF857jI43j60dKurp9uQKulfQ225vkx/b2PG0ibSHpoUgnaY9UOhzTsqvTb/euJ+lQST/N09eTdHAefmdjeicuk3S88zu27Z3z3+0l3RsRX1A6LPfXtp8r6Y8RcZ5ST6S7tK/M9gGtdSl9gluldFjucjW66s3nTBZKep3tbfIx9cM19PN4paSDbT8n33drN36/Ft3DHvzUdZbSx/6b8z/4CqWfAJyr9IMQtysd3rmu/Y4RscL2LEmX5PBaLukNSsddL84n/o5vu9vRks50+srmvZLes471/mX+uN9ydg6zYUXEzbbPUQomKR3H/pkbP/A9Ab4i6btOX9u8VIM/Gdwo6UuSXqTUk+PcPP0xpfD/mNK2G+rk53D+Tamr5yV5298n6S2SDpF0pO0nlX7U5dNKh1Q+a/tppZ4Qjx1ifUdKOt32HyU9pXQoapXtT0n6stPXQVdJ+mREXOL0I9nzlfbS50XE99pXGBG358f2k1zjk0pvFvevw+PEBKA3SaCN7WVKJwHH/DVJ23spnWB+yxDzVkbEZmOv8JnN6WuSJ0bEol7XUjsO0QBrWyHpSnMxzoSzPV/phOuTva5lKmAPHgAqxR48AFSKgAeAShHwAFApAh4AKkXAA0Cl/h9SMcAyn8QwHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlo6-qXpXKvc",
        "colab_type": "text"
      },
      "source": [
        "The histogram shows that the errors aren't quite *Normally distributed* (also called *gaussian*), but we might expect that because the number of samples is very small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnE63-EUXTTq",
        "colab_type": "text"
      },
      "source": [
        "## 5. Draw Conclusions\n",
        "We built a single-layer fully-connected neural network model (multiple linear regression model) to predict happiness Score given a country's features. The model converged after about 10 epochs of training, and it achieved an average (absolute) error of +/- 0.4185. Including more features in the model outperformed the single-variable linear regression model, confirming the hypothesis we made last lesson that more features could improve performance. That said, we expect that a *deeper* model (more layers and neurons) and more data samples will improve performance.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL_wTrZZYK65",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "In this lesson we took a deeper dive into multiple regression, from the perspective of neural networks. We built a single-layer fully-connected neural network and demonstrated how to train and evaluate it. We covered several important techniques, most importantly: using multiple features to train a linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kzCoSgtYtnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}