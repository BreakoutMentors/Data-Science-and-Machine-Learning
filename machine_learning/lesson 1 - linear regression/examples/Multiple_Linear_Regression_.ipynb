{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple Linear Regression .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BreakoutMentors/Data-Science-and-Machine-Learning/blob/adam-migration-to-pytorch/machine_learning/lesson%201%20-%20linear%20regression/examples/Multiple_Linear_Regression_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5H75AdnZ8KJ"
      },
      "source": [
        "# Multiple Linear Regression: What Makes Us Happy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvzapAQFaK3t"
      },
      "source": [
        "In our previous lesson, we explored the topic of single-variable linear regression and demonstrated how to build a linear model as a single-layer neural network. In single-variable regression analysis, there is only one independent variable (i.e., x variable) and one dependent variable (i.e., y variable). However, what if we want to build a model to predict a label given multiple x variables (i.e., features)? To achieve this, we use multiple linear regression--a method to model the relationship between a dependent variable (y) and multiple independent variables (x). Intuitively, adding more x (in this case x columns) data to our features tends to help the model improve its overall performance.\n",
        "\n",
        "While adding more variables allows us to model more complex \"real-world\" relationships there are also additional steps we must take to make sure our model is sound and robust.\n",
        "\n",
        "In this lesson, we introduce the multiple linear regression method and demonstrate how to build a multiple linear regression model to predict life expectancy using the same [Life Expectancy(WHO)](https://www.kaggle.com/kumarajarshi/life-expectancy-who) dataset from the previous lesson. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1cMSwsmuz1"
      },
      "source": [
        "## Multiple Linear Regression\n",
        "Before we discuss multiple linear regression, let's review single-variable linear regression. Recall that single-variable linear regression aims to fit a line to the data using the following formula: \n",
        "$$\n",
        "\\hat{y} = wx + b\n",
        "$$\n",
        "where $w$ is known as the *weight*, and $b$ is   the *bias* term.\n",
        "\n",
        "When the inputs ($x$) consists of $d$ features, we express the above linear function as: \n",
        "$$\n",
        "\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.\n",
        "$$\n",
        "\n",
        "In machine learning, we typically work with *high-dimensional* datasets, meaning $d$ is large so there are many features. When $d$ is large, it's not convenient to write the above linear equation, instead we can express it using vector notation as follows:\n",
        "$$\n",
        "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
        "$$\n",
        "\n",
        "where the vector $\\mathbf{x} \\in \\mathbb{R}^d$ and the vector $\\mathbf{w} \\in \\mathbb{R}^d$  contain the *features* and *weights* respectively.\n",
        "\n",
        "In the above equation, $\\mathbf{x}$ corresponds to a single input sample. It is often more convenient to refer to features of our entire dataset of $n$ samples via the *matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, where each sample is represented by a row and each feature by a column. For a collection of features $\\mathbf{X}$ and labels $y \\in \\mathbb{R^n}$, the multiple linear regression function can be expressed as the matrix-vector product:\n",
        "\n",
        "$$\n",
        "{\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b.\n",
        "$$\n",
        "\n",
        "The goal of multiple linear regression is to find the weight vector  $\\mathbf{w}$  and the bias term  $b$  that results in the lowest prediction error. Notice that the goal is basically the same as single-variable linear regression except multiple weights are learned instead of one.\n",
        "\n",
        "The figure below illustrates a single-layer multiple linear regression neural network. The input layer consists of $d$ *neurons* each corresponding to a feature (from $\\mathbf{x}$) and $d$ *connections* each corresponding to the *weight* between the output neuron an input neuron. \n",
        "\n",
        "<figure>\n",
        "<img src='https://d2l.ai/_images/singleneuron.svg' width='60%'></img><figcaption>Linear Regression: a multiple linear regression neural network</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8hGH7ke4rFR"
      },
      "source": [
        "# Multiple Linear Regression: What makes us happy?\n",
        "Now that we know a bit about the multiple linear regression method, it's time to apply it to a real-world problem--predicting life expectancy given other health statistics. Our goal is to build a *single-layer fully-connected neural network* (i.e., a mutliple linear regression model) to predict life expectancy using the WHO's [Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who) dataset.\n",
        "\n",
        "Like single-variable linear regression, we will perform the following steps:\n",
        "\n",
        "1. Explore and prepare the dataset.\n",
        "2. Build the model.\n",
        "3. Train the model.\n",
        "4. Evaluate the model.\n",
        "5. Draw conclusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lehhNxE1fLxn"
      },
      "source": [
        "# import the libraries we be need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DReavXyx7CoZ"
      },
      "source": [
        "## 1. Explore and prepare the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHqEvmn1ZYa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ca257756-a9eb-470f-c123-7be508f7b41a"
      },
      "source": [
        "# load the dataset into a dataframe\n",
        "data_url = 'https://raw.githubusercontent.com/BreakoutMentors/Data-Science-and-Machine-Learning/adam-migration-to-pytorch/datasets/Life%20Expectancy%20Data.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "df.columns = [col_name.strip() for col_name in df.columns] # used to fix weird spaces in the column names\n",
        "df.head() # view the first 5 rows of the data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Status</th>\n",
              "      <th>Life expectancy</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Measles</th>\n",
              "      <th>BMI</th>\n",
              "      <th>under-five deaths</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Population</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Income composition of resources</th>\n",
              "      <th>Schooling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>2015</td>\n",
              "      <td>Developing</td>\n",
              "      <td>65.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>62</td>\n",
              "      <td>0.01</td>\n",
              "      <td>71.279624</td>\n",
              "      <td>65.0</td>\n",
              "      <td>1154</td>\n",
              "      <td>19.1</td>\n",
              "      <td>83</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.16</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>584.259210</td>\n",
              "      <td>33736494.0</td>\n",
              "      <td>17.2</td>\n",
              "      <td>17.3</td>\n",
              "      <td>0.479</td>\n",
              "      <td>10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>2014</td>\n",
              "      <td>Developing</td>\n",
              "      <td>59.9</td>\n",
              "      <td>271.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0.01</td>\n",
              "      <td>73.523582</td>\n",
              "      <td>62.0</td>\n",
              "      <td>492</td>\n",
              "      <td>18.6</td>\n",
              "      <td>86</td>\n",
              "      <td>58.0</td>\n",
              "      <td>8.18</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>612.696514</td>\n",
              "      <td>327582.0</td>\n",
              "      <td>17.5</td>\n",
              "      <td>17.5</td>\n",
              "      <td>0.476</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>59.9</td>\n",
              "      <td>268.0</td>\n",
              "      <td>66</td>\n",
              "      <td>0.01</td>\n",
              "      <td>73.219243</td>\n",
              "      <td>64.0</td>\n",
              "      <td>430</td>\n",
              "      <td>18.1</td>\n",
              "      <td>89</td>\n",
              "      <td>62.0</td>\n",
              "      <td>8.13</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>631.744976</td>\n",
              "      <td>31731688.0</td>\n",
              "      <td>17.7</td>\n",
              "      <td>17.7</td>\n",
              "      <td>0.470</td>\n",
              "      <td>9.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>2012</td>\n",
              "      <td>Developing</td>\n",
              "      <td>59.5</td>\n",
              "      <td>272.0</td>\n",
              "      <td>69</td>\n",
              "      <td>0.01</td>\n",
              "      <td>78.184215</td>\n",
              "      <td>67.0</td>\n",
              "      <td>2787</td>\n",
              "      <td>17.6</td>\n",
              "      <td>93</td>\n",
              "      <td>67.0</td>\n",
              "      <td>8.52</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>669.959000</td>\n",
              "      <td>3696958.0</td>\n",
              "      <td>17.9</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.463</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>2011</td>\n",
              "      <td>Developing</td>\n",
              "      <td>59.2</td>\n",
              "      <td>275.0</td>\n",
              "      <td>71</td>\n",
              "      <td>0.01</td>\n",
              "      <td>7.097109</td>\n",
              "      <td>68.0</td>\n",
              "      <td>3013</td>\n",
              "      <td>17.2</td>\n",
              "      <td>97</td>\n",
              "      <td>68.0</td>\n",
              "      <td>7.87</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>63.537231</td>\n",
              "      <td>2978599.0</td>\n",
              "      <td>18.2</td>\n",
              "      <td>18.2</td>\n",
              "      <td>0.454</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Country  Year  ... Income composition of resources  Schooling\n",
              "0  Afghanistan  2015  ...                           0.479       10.1\n",
              "1  Afghanistan  2014  ...                           0.476       10.0\n",
              "2  Afghanistan  2013  ...                           0.470        9.9\n",
              "3  Afghanistan  2012  ...                           0.463        9.8\n",
              "4  Afghanistan  2011  ...                           0.454        9.5\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxVn4gJgsaMu",
        "outputId": "6d9f0f88-5d83-486d-f6ab-229dfa92ff97"
      },
      "source": [
        "# Looking at null values\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2938 entries, 0 to 2937\n",
            "Data columns (total 22 columns):\n",
            " #   Column                           Non-Null Count  Dtype  \n",
            "---  ------                           --------------  -----  \n",
            " 0   Country                          2938 non-null   object \n",
            " 1   Year                             2938 non-null   int64  \n",
            " 2   Status                           2938 non-null   object \n",
            " 3   Life expectancy                  2928 non-null   float64\n",
            " 4   Adult Mortality                  2928 non-null   float64\n",
            " 5   infant deaths                    2938 non-null   int64  \n",
            " 6   Alcohol                          2744 non-null   float64\n",
            " 7   percentage expenditure           2938 non-null   float64\n",
            " 8   Hepatitis B                      2385 non-null   float64\n",
            " 9   Measles                          2938 non-null   int64  \n",
            " 10  BMI                              2904 non-null   float64\n",
            " 11  under-five deaths                2938 non-null   int64  \n",
            " 12  Polio                            2919 non-null   float64\n",
            " 13  Total expenditure                2712 non-null   float64\n",
            " 14  Diphtheria                       2919 non-null   float64\n",
            " 15  HIV/AIDS                         2938 non-null   float64\n",
            " 16  GDP                              2490 non-null   float64\n",
            " 17  Population                       2286 non-null   float64\n",
            " 18  thinness  1-19 years             2904 non-null   float64\n",
            " 19  thinness 5-9 years               2904 non-null   float64\n",
            " 20  Income composition of resources  2771 non-null   float64\n",
            " 21  Schooling                        2775 non-null   float64\n",
            "dtypes: float64(16), int64(4), object(2)\n",
            "memory usage: 505.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iJC13_htjRG",
        "outputId": "b441158b-24a0-4b6f-adbc-b34ec80b2b22"
      },
      "source": [
        "# First action of cleaning is to remove the rows with life expectancy nans\n",
        "df = df.loc[~df['Life expectancy'].isna(), :]\n",
        "\n",
        "print('The amount of columns that will be deleted if we deleted all rows with Nans:', df.shape[0] - df.dropna().shape[0])\n",
        "print('That is a really high number! We should replace those values')\n",
        "# Second action is to fill all other Nans \n",
        "fillers = {'Alcohol':df['Alcohol'].mean(), 'Hepatitis B':df['Hepatitis B'].mean(),\n",
        "           'BMI':df['BMI'].mean(), 'Polio':df['Polio'].mean(),\n",
        "           'Total expenditure':df['Total expenditure'].mean(),\n",
        "           'Diphtheria':df['Diphtheria'].mean(), 'GDP':df['GDP'].mean(),\n",
        "           'Population':df['Population'].mean(),\n",
        "           'thinness  1-19 years':df['thinness  1-19 years'].mean(),\n",
        "           'thinness 5-9 years':df['thinness 5-9 years'].mean(),\n",
        "           'Income composition of resources':df['Income composition of resources'].mean(),\n",
        "           'Schooling':df['Schooling'].mean()}\n",
        "\n",
        "df = df.fillna(value=fillers)\n",
        "print('The amount of Nans still left:', df.isna().sum().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The amount of columns that will be deleted if we deleted all rows with Nans: 1279\n",
            "That is a really high number! We should replace those values\n",
            "The amount of Nans still left: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8P_pZ917oTt"
      },
      "source": [
        "There are three variables we don't want to use in our model: Country, Year, Status. The reason why we do not want these variables is because they do not describe any information about health or wealth, and we do not want the location and year to have influence in the model. In the below cell, we will remove those variables and prepare the dataset for the model. Remember this process involves defining the features ($\\mathbf{x}$) and the labels ($y$), splitting the dataset into a training and test set, and separating the features and the labels in both sets. In this case, the features will be all the other variables except life expectancy, which is our labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgFdnBbl6zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b81418d-cb22-461c-c8ef-c6766f57beba"
      },
      "source": [
        "# Dropping Country, Year and Status from dataframe\n",
        "df = df.drop(columns=['Country', 'Year', 'Status'])\n",
        "\n",
        "# define the x (features) and y (labels) variables\n",
        "x_cols = df.drop(columns=['Life expectancy']).columns.tolist()\n",
        "y_col = 'Life expectancy'\n",
        "print('x features: ', x_cols)\n",
        "print('y labels: ', y_col)\n",
        "\n",
        "\n",
        "# Calculating means and stds to normalize data\n",
        "means = df.loc[:, x_cols].mean().values\n",
        "stds = df.loc[:, x_cols].std().values\n",
        "\n",
        "# split the dataset into train/test datasets \n",
        "train = df.sample(frac=0.8, random_state=0)\n",
        "test = df.drop(train.index)\n",
        "\n",
        "# Splitting training data into validation data\n",
        "valid = train.sample(frac=0.1, random_state=0)\n",
        "train = train.drop(valid.index) # Deleting rows sampled for validation data\n",
        "\n",
        "# separate the x (features) and y (labels) in the train/valid/test datasets\n",
        "#train_features = torch.tensor((train[x_cols].values-means)/stds, dtype=torch.float)\n",
        "#test_features = torch.tensor((test[x_cols].values-means)/stds, dtype=torch.float)\n",
        "#valid_features = torch.tensor((valid[x_cols].values-means)/stds, dtype=torch.float)\n",
        "\n",
        "train_features = torch.tensor(train[x_cols].values, dtype=torch.float)\n",
        "test_features = torch.tensor(test[x_cols].values, dtype=torch.float)\n",
        "valid_features = torch.tensor(valid[x_cols].values, dtype=torch.float)\n",
        "\n",
        "train_labels = torch.tensor(train[y_col].values.reshape(-1, 1), dtype=torch.float)\n",
        "test_labels = torch.tensor(test[y_col].values.reshape(-1, 1), dtype=torch.float)\n",
        "valid_labels = torch.tensor(valid[y_col].values.reshape(-1, 1), dtype=torch.float)\n",
        "\n",
        "print('train features shape:', train_features.shape)\n",
        "print('train labels shape:', train_labels.shape)\n",
        "\n",
        "print('validation features shape:', valid_features.shape)\n",
        "print('validation labels shape:', valid_labels.shape)\n",
        "\n",
        "print('test features shape:', test_features.shape)\n",
        "print('test labels shape:', test_labels.shape)\n",
        "\n",
        "print('first 5 test labels:\\n', test_labels[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x features:  ['Adult Mortality', 'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles', 'BMI', 'under-five deaths', 'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 'GDP', 'Population', 'thinness  1-19 years', 'thinness 5-9 years', 'Income composition of resources', 'Schooling']\n",
            "y labels:  Life expectancy\n",
            "train features shape: torch.Size([2108, 18])\n",
            "train labels shape: torch.Size([2108, 1])\n",
            "validation features shape: torch.Size([234, 18])\n",
            "validation labels shape: torch.Size([234, 1])\n",
            "test features shape: torch.Size([586, 18])\n",
            "test labels shape: torch.Size([586, 1])\n",
            "first 5 test labels:\n",
            " tensor([[65.0000],\n",
            "        [59.5000],\n",
            "        [58.1000],\n",
            "        [76.2000],\n",
            "        [75.9000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmGj7bGL-UqH"
      },
      "source": [
        "The above code returns a training, validation, and test datasets. The `x features:` printed line above shows the 18 variables chosen as our *features*, such as Alcohol usage, BMI, and GDP. And the Life expectancy variable represents the *labels*. There are three datasets--a *training dataset*, a *validation dataset* and a *test dataset*. The `train_features` and `train_labels` arrays represent the features and labels of the training dataset, each containing 2108 samples. The `valid_features` and `valid_labels` arrays represent the features and labels of the validation dataset, each containing 234 samples.  The `test_features` and `test_labels` arrays represent the features and labels of the test dataset, each containing 586 samples.  \n",
        "\n",
        "Now that we have the *features* and *labels* separated, we are ready to build our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsINWD1a1kx0"
      },
      "source": [
        "### Introducing Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o5uJUQR1htO"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Defining datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "valid_dataset = torch.utils.data.TensorDataset(valid_features, valid_labels)\n",
        "\n",
        "# Loading datasets into dataloaders\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTOstVUn_JR3"
      },
      "source": [
        "## 2. Build the model\n",
        "Now that the data is ready, we can build the model! \n",
        "\n",
        "Before we define the model in Python code, let's write out its function given one input sample:\n",
        "$$\n",
        "\\hat{\\text{Life Expectancy}} = w_1 x_1 + w_2 x_2 + ... + w_{16} x_{16} + w_{17} x_{17} + w_{18} x_{18} + b,\n",
        "$$\n",
        "where 1-18 corresponds to one of the feature column we defined earlier (i.e., GDP, BMI, etc.). The function can be compressed using vector notation as:\n",
        "$$ \n",
        "\\hat{\\text{Life Expectancy}} = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
        "$$\n",
        "\n",
        "Given the entire training set, we write the matrix-vector equation for our linear model as:\n",
        "$$\n",
        "{\\hat{\\textbf{Life Expectancy}}} = \\mathbf{X} \\mathbf{w} + b.\n",
        "$$\n",
        "\n",
        "The model we build and then train will try to find the optimal *weights* ($\\mathbf{w}$) to minimize the difference between the real labels ($\\textbf{Life Expectancy}$) and the predictions ($\\hat{\\textbf{Life Expectancy}})$.\n",
        "\n",
        "Now that we know the function we want to estimate, let's use [PyTorch](https://pytorch.org/) to build a linear regression model, just like we did in the last lesson. However, we are adding one parameter to the constructor of the class called `num_features`, which is the number of feature variables to add some customizability in the construction of our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc_gJ4RllO5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87137b36-2b59-4d4f-b706-61502ce91740"
      },
      "source": [
        "# build the linear model \n",
        "class LR_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    @params\n",
        "        num_features(int): The number of features to construct the input layer of the NN\n",
        "    \"\"\"\n",
        "    # Defining Constructor\n",
        "    def __init__(self, num_features):\n",
        "        super(LR_Model, self).__init__()\n",
        "\n",
        "        # Defining Layers\n",
        "        self.fc1 = nn.Linear(num_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc1(x)\n",
        "\n",
        "# Initializing model\n",
        "num_features = train_features.shape[1] # Getting number of features\n",
        "model = LR_Model(num_features)\n",
        "\n",
        "print('Model Summary')\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Summary\n",
            "LR_Model(\n",
            "  (fc1): Linear(in_features=18, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EafDyr3FSoK"
      },
      "source": [
        "The model we defined above is a multiple linear model that could also be called a *single-layer fully-connected neural network*. We defined it using the [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) class. Note that we passed two arguments into the `torch.nn.Linear` class. The first one specifies the input feature dimension, which is 18 (corresponding to the number of x features in `train_features`), and the second one is the output feature dimension, which is a single scalar and therefore 1. Each input *feature* has a corresponding *weight* and there is one bias term. The *weights* and *bias*, or *parameters*, are connected to the single output *neuron*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9f-65plLUGO"
      },
      "source": [
        "### Define the loss function and optimization algorithm\n",
        "\n",
        "After defining the model, we need to configure the *loss function*, *optimization algorithm* for the model. We will use mean squared error for the loss function and stochastic gradient descent (SGD) for the optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRlVR35NLWWn"
      },
      "source": [
        "# Defining Loss Function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD4l2J8zLGWN"
      },
      "source": [
        "## 3. Train the model\n",
        "Now that we have a model, it's time to train it. We will train the model for 100 *epochs* (i.e., iterations), and record the training losses and validation losses for every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn64l5vAf8NM",
        "outputId": "ee0868f1-83d2-4941-c0fa-158b97a946d4"
      },
      "source": [
        "# Try this\n",
        "\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "\n",
        "    # Setting model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Setting all gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate Output\n",
        "    train_predictions = model(train_features)\n",
        "        \n",
        "    # Calculate Loss\n",
        "    train_loss = loss_fn(train_predictions, train_labels)\n",
        "\n",
        "    # Calculate Gradients\n",
        "    train_loss.backward()\n",
        "\n",
        "    # Perform Gradient Descent Step\n",
        "    optimizer.step()\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    # Setting model to evaluation mode, no parameters will change\n",
        "    model.eval()\n",
        "\n",
        "    # Calculate Output\n",
        "    valid_predictions = model(valid_features)\n",
        "\n",
        "    # Calculate Loss\n",
        "    valid_loss = loss_fn(valid_predictions, valid_labels)\n",
        "\n",
        "    # Saving Losses\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 3316877885440.000000 \tValidation Loss: inf\n",
            "Epoch: 2 \tTraining Loss: inf \tValidation Loss: inf\n",
            "Epoch: 3 \tTraining Loss: inf \tValidation Loss: inf\n",
            "Epoch: 4 \tTraining Loss: inf \tValidation Loss: nan\n",
            "Epoch: 5 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 6 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 7 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 8 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 9 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 10 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 11 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 12 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 13 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 14 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 15 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 16 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 17 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 18 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 19 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 20 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 21 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 22 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 23 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 24 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 25 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 26 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 27 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 28 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 29 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 30 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 31 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 32 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 33 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 34 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 35 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 36 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 37 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 38 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 39 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 40 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 41 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 42 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 43 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 44 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 45 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 46 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 47 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 48 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 49 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 50 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 51 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 52 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 53 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 54 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 55 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 56 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 57 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 58 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 59 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 60 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 61 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 62 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 63 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 64 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 65 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 66 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 67 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 68 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 69 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 70 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 71 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 72 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 73 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 74 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 75 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 76 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 77 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 78 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 79 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 80 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 81 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 82 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 83 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 84 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 85 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 86 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 87 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 88 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 89 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 90 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 91 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 92 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 93 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 94 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 95 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 96 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 97 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 98 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 99 \tTraining Loss: nan \tValidation Loss: nan\n",
            "Epoch: 100 \tTraining Loss: nan \tValidation Loss: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtKfpJA4OpPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ca1283-b123-40a6-ea0d-407d96231395"
      },
      "source": [
        "epochs = 100\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "\n",
        "    # Setting model to train mode\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        train_features, train_labels = batch\n",
        "        \n",
        "        # Setting all gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate Output\n",
        "        train_predictions = model(train_features)\n",
        "            \n",
        "        # Calculate Loss\n",
        "        loss = loss_fn(train_predictions, train_labels)\n",
        "\n",
        "        # Calculate Gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform Gradient Descent Step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Saving loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    # Setting model to evaluation mode, no parameters will change\n",
        "    model.eval()\n",
        "    for batch in valid_dataloader:\n",
        "        valid_features, valid_labels = batch\n",
        "        # Calculate Output\n",
        "        valid_predictions = model(valid_features)\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = loss_fn(valid_predictions, valid_labels)\n",
        "\n",
        "        # Saving loss\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "    # Saving Losses\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 498469.121338 \tValidation Loss: 42440.936279\n",
            "Epoch: 2 \tTraining Loss: 292893.781738 \tValidation Loss: 25543.155884\n",
            "Epoch: 3 \tTraining Loss: 173263.081970 \tValidation Loss: 15543.884644\n",
            "Epoch: 4 \tTraining Loss: 103069.909515 \tValidation Loss: 9510.426575\n",
            "Epoch: 5 \tTraining Loss: 61666.949677 \tValidation Loss: 5958.310486\n",
            "Epoch: 6 \tTraining Loss: 37345.081467 \tValidation Loss: 3809.943359\n",
            "Epoch: 7 \tTraining Loss: 22972.166153 \tValidation Loss: 2521.689590\n",
            "Epoch: 8 \tTraining Loss: 14512.844067 \tValidation Loss: 1762.958656\n",
            "Epoch: 9 \tTraining Loss: 9534.359032 \tValidation Loss: 1291.654335\n",
            "Epoch: 10 \tTraining Loss: 6597.653521 \tValidation Loss: 998.873707\n",
            "Epoch: 11 \tTraining Loss: 4852.297615 \tValidation Loss: 817.030854\n",
            "Epoch: 12 \tTraining Loss: 3821.584089 \tValidation Loss: 697.799014\n",
            "Epoch: 13 \tTraining Loss: 3216.879380 \tValidation Loss: 620.651438\n",
            "Epoch: 14 \tTraining Loss: 2853.726155 \tValidation Loss: 565.371024\n",
            "Epoch: 15 \tTraining Loss: 2636.952042 \tValidation Loss: 527.234568\n",
            "Epoch: 16 \tTraining Loss: 2506.818931 \tValidation Loss: 499.914740\n",
            "Epoch: 17 \tTraining Loss: 2430.872771 \tValidation Loss: 472.052381\n",
            "Epoch: 18 \tTraining Loss: 2379.490631 \tValidation Loss: 456.758935\n",
            "Epoch: 19 \tTraining Loss: 2355.709400 \tValidation Loss: 438.544094\n",
            "Epoch: 20 \tTraining Loss: 2335.844971 \tValidation Loss: 422.801758\n",
            "Epoch: 21 \tTraining Loss: 2324.087876 \tValidation Loss: 413.138853\n",
            "Epoch: 22 \tTraining Loss: 2317.527394 \tValidation Loss: 405.384372\n",
            "Epoch: 23 \tTraining Loss: 2311.820741 \tValidation Loss: 394.314023\n",
            "Epoch: 24 \tTraining Loss: 2305.959586 \tValidation Loss: 385.211974\n",
            "Epoch: 25 \tTraining Loss: 2304.031281 \tValidation Loss: 375.265132\n",
            "Epoch: 26 \tTraining Loss: 2302.092748 \tValidation Loss: 368.575615\n",
            "Epoch: 27 \tTraining Loss: 2298.950170 \tValidation Loss: 365.322473\n",
            "Epoch: 28 \tTraining Loss: 2299.945939 \tValidation Loss: 356.713181\n",
            "Epoch: 29 \tTraining Loss: 2305.443996 \tValidation Loss: 350.263837\n",
            "Epoch: 30 \tTraining Loss: 2295.169178 \tValidation Loss: 345.743139\n",
            "Epoch: 31 \tTraining Loss: 2295.157096 \tValidation Loss: 342.973480\n",
            "Epoch: 32 \tTraining Loss: 2292.433175 \tValidation Loss: 340.376430\n",
            "Epoch: 33 \tTraining Loss: 2294.466375 \tValidation Loss: 335.205385\n",
            "Epoch: 34 \tTraining Loss: 2292.296667 \tValidation Loss: 332.923806\n",
            "Epoch: 35 \tTraining Loss: 2293.857298 \tValidation Loss: 329.928679\n",
            "Epoch: 36 \tTraining Loss: 2291.289633 \tValidation Loss: 326.872249\n",
            "Epoch: 37 \tTraining Loss: 2292.395768 \tValidation Loss: 324.573241\n",
            "Epoch: 38 \tTraining Loss: 2288.938368 \tValidation Loss: 322.088748\n",
            "Epoch: 39 \tTraining Loss: 2290.577803 \tValidation Loss: 319.248296\n",
            "Epoch: 40 \tTraining Loss: 2291.403157 \tValidation Loss: 316.342602\n",
            "Epoch: 41 \tTraining Loss: 2290.230901 \tValidation Loss: 314.274667\n",
            "Epoch: 42 \tTraining Loss: 2288.513482 \tValidation Loss: 311.548374\n",
            "Epoch: 43 \tTraining Loss: 2287.582755 \tValidation Loss: 309.205092\n",
            "Epoch: 44 \tTraining Loss: 2287.634251 \tValidation Loss: 310.416452\n",
            "Epoch: 45 \tTraining Loss: 2285.052852 \tValidation Loss: 308.072460\n",
            "Epoch: 46 \tTraining Loss: 2288.393634 \tValidation Loss: 306.579213\n",
            "Epoch: 47 \tTraining Loss: 2286.591969 \tValidation Loss: 304.657803\n",
            "Epoch: 48 \tTraining Loss: 2284.954291 \tValidation Loss: 304.196735\n",
            "Epoch: 49 \tTraining Loss: 2283.827924 \tValidation Loss: 302.115739\n",
            "Epoch: 50 \tTraining Loss: 2288.408616 \tValidation Loss: 301.887239\n",
            "Epoch: 51 \tTraining Loss: 2282.887372 \tValidation Loss: 301.490053\n",
            "Epoch: 52 \tTraining Loss: 2281.682782 \tValidation Loss: 299.638458\n",
            "Epoch: 53 \tTraining Loss: 2289.648118 \tValidation Loss: 299.040881\n",
            "Epoch: 54 \tTraining Loss: 2284.924338 \tValidation Loss: 297.914239\n",
            "Epoch: 55 \tTraining Loss: 2283.783565 \tValidation Loss: 296.625610\n",
            "Epoch: 56 \tTraining Loss: 2285.485620 \tValidation Loss: 295.842003\n",
            "Epoch: 57 \tTraining Loss: 2284.592245 \tValidation Loss: 295.914054\n",
            "Epoch: 58 \tTraining Loss: 2283.314464 \tValidation Loss: 295.534703\n",
            "Epoch: 59 \tTraining Loss: 2283.382688 \tValidation Loss: 294.357526\n",
            "Epoch: 60 \tTraining Loss: 2280.690774 \tValidation Loss: 293.244412\n",
            "Epoch: 61 \tTraining Loss: 2282.250758 \tValidation Loss: 292.214191\n",
            "Epoch: 62 \tTraining Loss: 2284.636945 \tValidation Loss: 291.815913\n",
            "Epoch: 63 \tTraining Loss: 2280.524438 \tValidation Loss: 291.724521\n",
            "Epoch: 64 \tTraining Loss: 2284.361350 \tValidation Loss: 291.822683\n",
            "Epoch: 65 \tTraining Loss: 2283.702064 \tValidation Loss: 291.120277\n",
            "Epoch: 66 \tTraining Loss: 2278.250478 \tValidation Loss: 290.673908\n",
            "Epoch: 67 \tTraining Loss: 2279.869326 \tValidation Loss: 289.855683\n",
            "Epoch: 68 \tTraining Loss: 2279.237379 \tValidation Loss: 289.668628\n",
            "Epoch: 69 \tTraining Loss: 2281.912950 \tValidation Loss: 289.573322\n",
            "Epoch: 70 \tTraining Loss: 2280.547410 \tValidation Loss: 289.909801\n",
            "Epoch: 71 \tTraining Loss: 2278.977494 \tValidation Loss: 288.936790\n",
            "Epoch: 72 \tTraining Loss: 2280.205598 \tValidation Loss: 289.310094\n",
            "Epoch: 73 \tTraining Loss: 2279.858890 \tValidation Loss: 288.503783\n",
            "Epoch: 74 \tTraining Loss: 2281.549759 \tValidation Loss: 288.392626\n",
            "Epoch: 75 \tTraining Loss: 2279.850613 \tValidation Loss: 288.698230\n",
            "Epoch: 76 \tTraining Loss: 2279.670422 \tValidation Loss: 288.632275\n",
            "Epoch: 77 \tTraining Loss: 2280.781624 \tValidation Loss: 287.874112\n",
            "Epoch: 78 \tTraining Loss: 2279.690612 \tValidation Loss: 288.322285\n",
            "Epoch: 79 \tTraining Loss: 2277.046234 \tValidation Loss: 288.128395\n",
            "Epoch: 80 \tTraining Loss: 2279.490910 \tValidation Loss: 287.453012\n",
            "Epoch: 81 \tTraining Loss: 2278.831505 \tValidation Loss: 287.719145\n",
            "Epoch: 82 \tTraining Loss: 2277.566421 \tValidation Loss: 287.402130\n",
            "Epoch: 83 \tTraining Loss: 2283.500824 \tValidation Loss: 287.996810\n",
            "Epoch: 84 \tTraining Loss: 2277.473854 \tValidation Loss: 287.324812\n",
            "Epoch: 85 \tTraining Loss: 2278.036506 \tValidation Loss: 287.130619\n",
            "Epoch: 86 \tTraining Loss: 2277.645327 \tValidation Loss: 286.793310\n",
            "Epoch: 87 \tTraining Loss: 2276.971969 \tValidation Loss: 286.520791\n",
            "Epoch: 88 \tTraining Loss: 2275.473355 \tValidation Loss: 286.538136\n",
            "Epoch: 89 \tTraining Loss: 2277.137913 \tValidation Loss: 286.305347\n",
            "Epoch: 90 \tTraining Loss: 2277.678785 \tValidation Loss: 286.108822\n",
            "Epoch: 91 \tTraining Loss: 2277.346599 \tValidation Loss: 285.532957\n",
            "Epoch: 92 \tTraining Loss: 2276.243685 \tValidation Loss: 285.489115\n",
            "Epoch: 93 \tTraining Loss: 2276.143862 \tValidation Loss: 285.474841\n",
            "Epoch: 94 \tTraining Loss: 2281.679590 \tValidation Loss: 285.603836\n",
            "Epoch: 95 \tTraining Loss: 2277.657697 \tValidation Loss: 285.562890\n",
            "Epoch: 96 \tTraining Loss: 2276.303431 \tValidation Loss: 285.684173\n",
            "Epoch: 97 \tTraining Loss: 2276.970669 \tValidation Loss: 285.378127\n",
            "Epoch: 98 \tTraining Loss: 2277.549314 \tValidation Loss: 284.898428\n",
            "Epoch: 99 \tTraining Loss: 2275.186048 \tValidation Loss: 285.140685\n",
            "Epoch: 100 \tTraining Loss: 2275.988403 \tValidation Loss: 285.163543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa_D6YswOvo1"
      },
      "source": [
        "We get to ~2800 validation mean squared error after training for 100 epochs on the training dataset. For reference, the same metric was ~0.7807 in the single-variable linear regression model (from the previous lesson).  Let's visualize the model's training progress using the stats stored in the history object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QGfBDMVPPYv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9510c70c-e60a-4e24-884c-5726c232574c"
      },
      "source": [
        "plt.plot(valid_losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgV4Jk8TV_zJ"
      },
      "source": [
        "From the plot we can see that our model *converges* around the 10th epoch. In other words, the most optimal parameters (weights and bias) are found after about the 10 training iteration. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3-3e5HYT0N9"
      },
      "source": [
        "## 4. Evaluate the model\n",
        "Now that we trained our model, it's time to evaluate it using the *test* dataset, which we did not use when training the model. This gives us a sense of how well our model predicts unseen data, which is the case when we use it in the real world. We will use the `evaluate` method to test the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwP7HvCkSLAw"
      },
      "source": [
        "loss, mae, mse = model.evaluate(test_features, test_labels)\n",
        "print('Test set Mean Absolute Error: ', round(mae, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk8eZggWk_F"
      },
      "source": [
        "The average (absolute) error is around +/- 0.4185 units for happiness Score, which is better than the single-variable linear model (+/- 0.516). Is this good? We'll leave that decision up to you. Let's also visualize the prediction and real happiness Score values using data in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6-UEhdrW9mo"
      },
      "source": [
        "test_predictions = model.predict(test_features).flatten()\n",
        "\n",
        "ax = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [Happiness Score]')\n",
        "plt.ylabel('Predictions [Happiness Score]')\n",
        "lims = [0, max(test_labels) + 1] # [0, 31]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kebG1ga8XDJj"
      },
      "source": [
        "It looks like our model predicts reasonably well. Let's take a look at the error distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt5h3qJqXLnX"
      },
      "source": [
        "error = test_predictions.reshape(-1, 1) - test_labels\n",
        "plt.hist(error, bins = 10)\n",
        "plt.xlabel(\"Prediction Error [Happiness Score]\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlo6-qXpXKvc"
      },
      "source": [
        "The histogram shows that the errors aren't quite *Normally distributed* (also called *gaussian*), but we might expect that because the number of samples is very small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnE63-EUXTTq"
      },
      "source": [
        "## 5. Draw Conclusions\n",
        "We built a single-layer fully-connected neural network model (multiple linear regression model) to predict happiness Score given a country's features. The model converged after about 10 epochs of training, and it achieved an average (absolute) error of +/- 0.4185. Including more features in the model outperformed the single-variable linear regression model, confirming the hypothesis we made last lesson that more features could improve performance. That said, we expect that a *deeper* model (more layers and neurons) and more data samples will improve performance.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL_wTrZZYK65"
      },
      "source": [
        "# Summary\n",
        "In this lesson we took a deeper dive into multiple regression, from the perspective of neural networks. We built a single-layer fully-connected neural network and demonstrated how to train and evaluate it. We covered several important techniques, most importantly: using multiple features to train a linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kzCoSgtYtnK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}