{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Classification-Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BreakoutMentors/Data-Science-and-Machine-Learning/blob/main/machine_learning/lesson%202%20-%20logistic%20regression/Classification_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUaQgPnXJxhV"
      },
      "source": [
        "> Note: Always open in Colab for the best learning experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIsgQYlKKVna"
      },
      "source": [
        "# Classification: Logistic Regression\n",
        "In the previous lessons, we learned about linear regression and how we can use it to construct a single layer linear neural network to predict a numeric value (i.e., how powerful a Pokemon is given their x features). Regression is great when we want to answer *how much?* or *how many?* questions. In practice, we are often more interested in *classification*: asking *which one?* not *how much?*\n",
        "- Is this customer more likely to *sign up* or *not* for a subscription service?\n",
        "- Does this image contain one of the following, a cat or a dog?\n",
        "- Is this song in the genre of hip hop, pop, or funk?\n",
        "\n",
        "When we want to distinguish two classes (called *binary classification*), we can use a classification technique called logistic regression.\n",
        "\n",
        "In this notebook, we will learn the foundations of logistic regression and demonstrate how to solve binary classification problems using an example--building a logistic regression model to predict whether it will rain the next-day or not. The ideas we introduce here will build on previous material and continue to lay out the fundamental concepts used in deep learning and neural networks, which we will cover in future lessons. Here is the lesson roadmap:\n",
        "1. Introduction to logistic regression\n",
        "2. From linear to logistic regression\n",
        "3. Building a logistic regression classifier: predicting if it rains tomorrow\n",
        "7. Summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OYk2h2X2gqE"
      },
      "source": [
        "# Representing categorical data\n",
        "<figure>\n",
        "  <img src='https://envato-shoebox-0.imgix.net/2718/a008-795b-4376-972d-ed9cbad8ac4f/2015_227_003_0063_A_2018_07_19.jpg?auto=compress%2Cformat&fit=max&mark=https%3A%2F%2Felements-assets.envato.com%2Fstatic%2Fwatermark2.png&markalign=center%2Cmiddle&markalpha=18&w=700&s=e3fbeb220008b297bee64675504ae70c' width='50%'>\n",
        "  <figcaption>Representing data: a Shina Inu, Retriever, and Lab</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "Before we dive into logistic regression, let's consider how machine learning problems generally represents categorical data. \n",
        "\n",
        "Categorical features represent types of data which may be divided into groups. Examples of categorical features are dog breed, game genre, and educational level. While the latter feature may also be considered in a numerical manner by using exact values for  highest grade completed, it is often more informative to categorize such variables into a relatively small number of groups.\n",
        "\n",
        "Consider an example where we want to distinguish 3 different dog breeds--(golden) retrievers, labs, and shiba inus, given 3 features about each dog: height, weight, and fur color. The numeric features are height ($x_1$) and weight ($x_2$), while the categorical feature is fur color ($x_3$), which we determined has 3 colors: black, red, yellow (golden/light gold). To make this categorical feature useful, we need to convert it into a numerical representation. \n",
        "\n",
        "There are two general ways to represent categorical data in numeric terms. Perhaps the most natural choice to is to choose $x_3 \\in \\{1, 2, 3\\}$, where the integers represent the fur colors {black, red, yellow} repectively. This is a great way to compress and store info on a computer, but it's not great for machine learning. Fortunately, great minds got together long ago and invented a simple method to represent categorical data called *one-hot encoding*. A one-hot encoding is a vector with as many components as we have categories. The component corresponding to particular sample's category is set to 1 and all other components are set to 0. So in our case, this translates to:\n",
        "\n",
        "$$\n",
        "x_3 \\in \\{ (1, 0, 0), (0, 1, 0), (0, 0, 1) \\},\n",
        "$$\n",
        "\n",
        "where $x_3$ would be a three-dimensional vector representing the fur color feature with $(1, 0, 0)$ corresponding to \"black\", (0, 1, 0) to \"red\", and (0, 0, 1) to \"yellow\" fur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKHFYZBKd64"
      },
      "source": [
        "## Challenge: Representing categorical data\n",
        "Now that you know how to represent categorical data, consider the dog breed example above. We one-hot encoded the fur color feature $x_3$ so that all the features $x_1, x_2, x_3$ were represented by numeric values. Thus, the features ($\\mathbf{x}$) were ready to be passed as input to a machine learning model. On the other hand, are the labels $y$ (the dog bread) ready? Are they in the proper format? How should $y$ be *encoded*? Write your answer in the text cell below. \n",
        "\n",
        "Hint: currently, $y \\in \\{\\ \\text{retrievers}, \\text{labs}, \\text{shiba inus} \\}$ is a one-dimensional vector with categorical values.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trG2Tl_SP-Wl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGvK59PbdliB"
      },
      "source": [
        "# Intro to logistic regression\n",
        "<figure>\n",
        "  <img src='https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/dog-vs-cat.jpg' width='70%'>\n",
        "  <figcaption>Classification: Cat vs Dog</figcaption>\n",
        "</figure>\n",
        "\n",
        "With a healthy understanding of categorical encoding, let's dive into the logistic regression method.\n",
        "\n",
        "Logistic regression is perhaps the simplest and most common machine learning algorithm for binary classification tasks. It is a special case of linear regression where the labels variable ($y$) is categorical in nature. It is called \"logistic\" regression because it uses a *logit* function, called the *sigmoid* function, to estimate the probability of a given class.\n",
        "\n",
        "To motivate logistic regression, let's consider a simple image classification problem--distinguish between cat and dog photos. Here, each image consists of a $2 \\times 2$ grayscale image. We can represent each pixel value with a single scalar (number), giving us four features $x_1,x_2,x_3,x_4$. Further, let's assume that each image belongs to one among the categories “cat” and “dog”. However, as we demonstrated in the previous section, we can't use the labels $y$ in its current format (\"cat\" and \"dog\"). We need to convert the labels to discrete numerical values (i.e., 0 and 1). To this end, we map each category to an integer, making $y \\in \\{0,1\\}$, where the integers represent $\\{\\text{cat}, \\text{dog}\\}$ repsectively. Notice that this is not exactly like *one-hot encoding*, where the one-dimensional vector is converted into a multi-dimensional vector with dimensions equivalent to the number of classes in the labels $y$. Instead, we used the simpler (first) method we discussed in the previous section: encoding each category as a numerical value, in this case $\\{0, 1\\}$ corresponding to $\\{\\text{cat}, \\text{dog}\\}$. When we only need to encode two categories (called binary categorization), we don't have to use one-hot encoding. However, we do need to encode the data numerically. Specifically, among the category labels, we need to assign 0 to one category and 1 to the other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fwLwZ31JrZa"
      },
      "source": [
        "# From linear to logistic regression\n",
        "<figure>\n",
        "  <img src='https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg' width='70%'>\n",
        "  <figcaption>Linear vs Logistic Regression | Source: Datacamp</figcaption>\n",
        "</figure>\n",
        "\n",
        "Now that we know how labels are properly *encoded*, let's demonstrate the connection between linear and logistic regression.\n",
        "\n",
        "When we are doing linear regression the equation is as follows:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\mathbf{w} \\mathbf{X} + b,\\tag{1}\n",
        "$$\n",
        "\n",
        "where the linear model learns the most *optimal* parameter values for the *weights* ($\\mathbf{w}$) and *bias* term ($b$). The linear regression method is great when we want to predict continuous numerical data, but not so good when we need to distinguish between classes. \n",
        "\n",
        "To make a binary logistic classifier to distinguish between cat and dog photos, we need to convert the predictions ($\\hat{\\mathbf{y}}$) into probabilities ($\\hat{\\mathbf{p}}$). Here, each sample is assigned a corresponding probability $\\hat{p}$ that indicates the model's degree of *certainty* that it belongs to a particular class (in our case, cat or dog). Further, we set a threshold, usually 0.5, that the model will use to determine the final class prediction. For our cat ($y=0$) and dog ($y=1$) problem, a sample with a $\\hat{p}$ value greater than 0.5 would receive the \"dog\" label for example. \n",
        "\n",
        "In order to predict classes, logistic regression maps predictions ($\\hat{\\mathbf{y}}$) to probabilities ($\\hat{\\mathbf{p}}$) via the *sigmoid* logit function:\n",
        "$$\n",
        "\\tag{2}\n",
        "p = \\sigma(y) = \\frac{1}{1 + e^{-y}},\n",
        "$$\n",
        "\n",
        "which leads us to the equation for logistic regression: \n",
        "$$\n",
        "\\tag{3}\n",
        "\\hat{\\mathbf{p}} = \\sigma(\\hat{\\mathbf{y}}) = \\frac{1}{1 + e^{-(\\hat{\\mathbf{w} \\mathbf{X} + b})}}, \n",
        "$$ \n",
        "\n",
        "where the logistic model (binary classifier) learns the most *optimal* parameter values ($\\mathbf{w}$ and $b$) by producing probabilities ($\\hat{\\mathbf{p}}$) that *maximize the likelihood* of predicting the observed data. \n",
        "\n",
        "Generally, the logistic regression equation from $(3)$ is compressed:\n",
        "\n",
        "$$\n",
        "\\tag{4}\n",
        "\\hat{\\mathbf{p}} = \\sigma(\\hat{\\mathbf{y}}) = \\sigma(\\hat{\\mathbf{w} \\mathbf{X} + b}),\n",
        "$$\n",
        "\n",
        "where $\\sigma$ represents the sigmoid function (eq. $2$) in this case. Does this equation look similar to linear regression yet?\n",
        "\n",
        "To summarize logistic regression:\n",
        "- Category labels are converted to discrete integer values (e.g., 0 and 1).\n",
        "- The *sigmoid* logit function maps input features ($\\mathbf{x}$) to probabilities (i.e., a number between 0 and 1).\n",
        "- A category prediction is determined by the threshold value (usually 0.5) and the probability (i.e., in our cat/dog example, a sample with a probability greater than 0.5 is classified as a dog image).   \n",
        "- Logistic regression classifiers try to maximize *certainty*: predict a particular class with high confidence ($\\hat{p}$ closer to 1) and be correct (after thresholding, $\\hat{p} = y$), most of the time.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1yKY66dhryV"
      },
      "source": [
        "# Logistic Regression: predicting if it rains tomorrow\n",
        "Now that we know about the fundamentals of logistic regression, let's apply this method to a real-world problem--predicting if it rains the next day in Austrailia based on daily data. In this section, we will demonstrate in an end-to-end fashion the process of creating a logistic regression classifier: from building, to training, and finally evaluating the model. This process involves several steps:\n",
        "\n",
        "1. Find a dataset related to our question.\n",
        "2. Explore the dataset and prepare it for the model.\n",
        "3. Build the model.\n",
        "4. Train the model using an algorithm such as stochastic gradient descent.\n",
        "5. Evaluate the quality of our model.\n",
        "Draw conclusions.\n",
        "\n",
        "For step 1, we found the [Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package). The dataset contains approximately 145,000 samples when uncleaned, each representing daily weather data and whether it rained the next day. It provides data like location, temperature, amount of rainfall, windspeed and so much more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZcA1hmu_nSx"
      },
      "source": [
        "# import the libraries we be need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing PyTorch\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-p0Ri_eijre"
      },
      "source": [
        "## 2. Explore the dataset and prepare it for our model\n",
        "In this section we will focus on defining the   *features* ($\\mathbf{x}$) and *labels* ($\\mathbf{y}$) that we will use in our logistic regression classifier to predict next-day rain. As you will see, this require us to do some data cleaning and preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "82Z3ksE8AjsI",
        "outputId": "efc86055-3ccc-402a-b293-e4dc458a9676"
      },
      "source": [
        "data_url = 'https://raw.githubusercontent.com/BreakoutMentors/Data-Science-and-Machine-Learning/main/datasets/weatherAUS.csv'\n",
        "df = pd.read_csv(data_url)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Location</th>\n",
              "      <th>MinTemp</th>\n",
              "      <th>MaxTemp</th>\n",
              "      <th>Rainfall</th>\n",
              "      <th>Evaporation</th>\n",
              "      <th>Sunshine</th>\n",
              "      <th>WindGustDir</th>\n",
              "      <th>WindGustSpeed</th>\n",
              "      <th>WindDir9am</th>\n",
              "      <th>WindDir3pm</th>\n",
              "      <th>WindSpeed9am</th>\n",
              "      <th>WindSpeed3pm</th>\n",
              "      <th>Humidity9am</th>\n",
              "      <th>Humidity3pm</th>\n",
              "      <th>Pressure9am</th>\n",
              "      <th>Pressure3pm</th>\n",
              "      <th>Cloud9am</th>\n",
              "      <th>Cloud3pm</th>\n",
              "      <th>Temp9am</th>\n",
              "      <th>Temp3pm</th>\n",
              "      <th>RainToday</th>\n",
              "      <th>RainTomorrow</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-12-01</td>\n",
              "      <td>Albury</td>\n",
              "      <td>13.4</td>\n",
              "      <td>22.9</td>\n",
              "      <td>0.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>W</td>\n",
              "      <td>44.0</td>\n",
              "      <td>W</td>\n",
              "      <td>WNW</td>\n",
              "      <td>20.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1007.7</td>\n",
              "      <td>1007.1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16.9</td>\n",
              "      <td>21.8</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-12-02</td>\n",
              "      <td>Albury</td>\n",
              "      <td>7.4</td>\n",
              "      <td>25.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WNW</td>\n",
              "      <td>44.0</td>\n",
              "      <td>NNW</td>\n",
              "      <td>WSW</td>\n",
              "      <td>4.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1010.6</td>\n",
              "      <td>1007.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17.2</td>\n",
              "      <td>24.3</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-12-03</td>\n",
              "      <td>Albury</td>\n",
              "      <td>12.9</td>\n",
              "      <td>25.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WSW</td>\n",
              "      <td>46.0</td>\n",
              "      <td>W</td>\n",
              "      <td>WSW</td>\n",
              "      <td>19.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1007.6</td>\n",
              "      <td>1008.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>23.2</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-12-04</td>\n",
              "      <td>Albury</td>\n",
              "      <td>9.2</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NE</td>\n",
              "      <td>24.0</td>\n",
              "      <td>SE</td>\n",
              "      <td>E</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1017.6</td>\n",
              "      <td>1012.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.1</td>\n",
              "      <td>26.5</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-12-05</td>\n",
              "      <td>Albury</td>\n",
              "      <td>17.5</td>\n",
              "      <td>32.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>W</td>\n",
              "      <td>41.0</td>\n",
              "      <td>ENE</td>\n",
              "      <td>NW</td>\n",
              "      <td>7.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1010.8</td>\n",
              "      <td>1006.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>29.7</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date Location  MinTemp  ...  Temp3pm  RainToday  RainTomorrow\n",
              "0  2008-12-01   Albury     13.4  ...     21.8         No            No\n",
              "1  2008-12-02   Albury      7.4  ...     24.3         No            No\n",
              "2  2008-12-03   Albury     12.9  ...     23.2         No            No\n",
              "3  2008-12-04   Albury      9.2  ...     26.5         No            No\n",
              "4  2008-12-05   Albury     17.5  ...     29.7         No            No\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWhDZORkja6p",
        "outputId": "01fd00f6-92c7-4761-81d9-b0ea7678873d"
      },
      "source": [
        "# check the column types and get basic info\n",
        "df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 145460 entries, 0 to 145459\n",
            "Data columns (total 23 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   Date           145460 non-null  object \n",
            " 1   Location       145460 non-null  object \n",
            " 2   MinTemp        143975 non-null  float64\n",
            " 3   MaxTemp        144199 non-null  float64\n",
            " 4   Rainfall       142199 non-null  float64\n",
            " 5   Evaporation    82670 non-null   float64\n",
            " 6   Sunshine       75625 non-null   float64\n",
            " 7   WindGustDir    135134 non-null  object \n",
            " 8   WindGustSpeed  135197 non-null  float64\n",
            " 9   WindDir9am     134894 non-null  object \n",
            " 10  WindDir3pm     141232 non-null  object \n",
            " 11  WindSpeed9am   143693 non-null  float64\n",
            " 12  WindSpeed3pm   142398 non-null  float64\n",
            " 13  Humidity9am    142806 non-null  float64\n",
            " 14  Humidity3pm    140953 non-null  float64\n",
            " 15  Pressure9am    130395 non-null  float64\n",
            " 16  Pressure3pm    130432 non-null  float64\n",
            " 17  Cloud9am       89572 non-null   float64\n",
            " 18  Cloud3pm       86102 non-null   float64\n",
            " 19  Temp9am        143693 non-null  float64\n",
            " 20  Temp3pm        141851 non-null  float64\n",
            " 21  RainToday      142199 non-null  object \n",
            " 22  RainTomorrow   142193 non-null  object \n",
            "dtypes: float64(16), object(7)\n",
            "memory usage: 25.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGJhIyb3dHo6"
      },
      "source": [
        "### Removing Null Values\n",
        "\n",
        "I will be doing two steps to remove null values\n",
        "1. Remove columns that have more than $\\frac{1}{4}$ of missing data\n",
        "2. Remove rows that have any mission data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6z4FBijU-CT",
        "outputId": "2dbf5383-8ef2-4181-c6e7-d1d0f841aa8b"
      },
      "source": [
        "# Printing the columns with more than 1/4 of column has nulls to remove those\n",
        "nan_cols = df.isna().sum() > (0.25 * df.shape[0])\n",
        "nan_cols = nan_cols[nan_cols].index.tolist()\n",
        "print(\"These are columns that have more than 1/4 of the column missing\")\n",
        "print(nan_cols)\n",
        "\n",
        "# Removing those columns\n",
        "df = df.drop(columns=nan_cols)\n",
        "\n",
        "# For easy cleaning, will be removing all rows with missing data\n",
        "df = df.dropna()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "These are columns that have more than 1/4 of the column missing\n",
            "['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY3ohRySWTih",
        "outputId": "3d12a01a-ebdb-4f51-f56d-107ec4e4f28c"
      },
      "source": [
        "print('Number of missing values: ', df.isna().sum().sum())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing values:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfhQQ_hefw5"
      },
      "source": [
        "### Feature Extraction from Dates\n",
        "\n",
        "The 'Date' column contains the date of when the data was collected. If we are planning to predict if it will rain tomorrow, then you can believe months can have an effect of when it rains. Therefore, a new column named 'month' is created which will be used for the model.\n",
        "\n",
        "The 'Date' column is originally an *object* datatype column, but we will use `pd.to_datatime()` to convert the column data type into *datetime64[ns]* data type so pandas can understand the column as actual dates, not strings. Then using the `dt.month` attribute of `df['Date']`, you can get the month of every entry and save the returned pandas Series and save that Series to a new column in the dataframe.\n",
        "\n",
        "After saving the month, we can remove the 'Date' column since it we cannot input that into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpq3dIrQcFDL"
      },
      "source": [
        "# Changing column Date from object data type to datetime64[ns]\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Creating a new column called 'month'\n",
        "df['month'] = df.Date.dt.month\n",
        "\n",
        "# look at the head of 'month'\n",
        "df.loc[:, ['Date', 'month']].head()\n",
        "\n",
        "df = df.drop(columns=['Date'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc6S2PtTht1P"
      },
      "source": [
        "### Translating string binary columns to [0, 1]\n",
        "\n",
        "Before translating these columns, we should look at the two distributions. Just to prevent confusion, the 'RainToday' column will be used as a feature variable, and 'RainTomorrow' will be used as the dependent variable which we will be predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "-roQeg3jhH0F",
        "outputId": "76bc53c3-e1b8-4faa-ed97-db8e2926ddc4"
      },
      "source": [
        "df.RainToday.hist()\n",
        "plt.title('Distribution of RainToday')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "3J81sytnixS_",
        "outputId": "b82f6f05-e209-4378-b8cf-d7c72430c8c8"
      },
      "source": [
        "df.RainTomorrow.hist()\n",
        "plt.title('Distribution of RainTomorrow')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcTd48JdBU13"
      },
      "source": [
        "You can see that these two distributions are very similar and that they are very unbalanced. When we separate the data into their training and test sets, we will even the class distribution in column 'Rain Tomorrow'. Now lets convert the 'No' and 'Yes' values to 0's and 1's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8mfRJBqjpcC"
      },
      "source": [
        "def binary_conversion(rain):\n",
        "    # returns 1 if it rains\n",
        "    if rain == 'Yes':\n",
        "        return 1\n",
        "    # returns 0 if it did not rain\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Converting those two columns\n",
        "df['RainToday'] = df['RainToday'].map(binary_conversion)\n",
        "df['RainTomorrow'] = df['RainTomorrow'].map(binary_conversion)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOsuPs2cmIkt"
      },
      "source": [
        "### One-Hot encoding all categorical data\n",
        "\n",
        "The way this is done is by one-hot encoding all columns with 'object' data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akwojnlekawn"
      },
      "source": [
        "# Getting columns with the object data type\n",
        "object_cols = df.select_dtypes(include=object).columns.tolist()\n",
        "\n",
        "# Saving one-hot encoded dataframes to concatenate later\n",
        "one_hot_dfs = [df.copy()]\n",
        "for col in object_cols:\n",
        "    one_hot_df = pd.get_dummies(df[col], prefix=col)\n",
        "    one_hot_dfs.append(one_hot_df)\n",
        "\n",
        "# Concatening all dataframes into df again\n",
        "df = pd.concat(one_hot_dfs, axis=1)\n",
        "\n",
        "# Dropping the columns used to one-hot encode\n",
        "df = df.drop(columns=object_cols)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xCHU-zyqsGZ"
      },
      "source": [
        "### Balancing the Dataset\n",
        "\n",
        "The reason why we balance the dataset by variable we try to predict is to prevent overfitting of the class of the highest frequency, for this situation it is 'No' for 'RainTomorrow' column. And in a statistical sense, the model learns the distribution of the training data to make predictions. So if the distribution is highly skewed to one class, it will just predict the class of highest frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "7oNqexiSpNei",
        "outputId": "bfccbcd3-3f03-4840-d4e9-d9600265e416"
      },
      "source": [
        "def balance_dataset(df):\n",
        "    df_copy = df.copy()\n",
        "    df_rain = df_copy.loc[df['RainTomorrow']==1.0, :]\n",
        "    df_dry = df_copy.loc[df['RainTomorrow']==0.0, :].sample(df_rain.RainTomorrow.value_counts()[1])\n",
        "    df_copy = pd.concat([df_rain, df_dry]).sample(frac=1, random_state=0)\n",
        "    return df_copy\n",
        "\n",
        "df = balance_dataset(df)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MinTemp</th>\n",
              "      <th>MaxTemp</th>\n",
              "      <th>Rainfall</th>\n",
              "      <th>WindGustSpeed</th>\n",
              "      <th>WindSpeed9am</th>\n",
              "      <th>WindSpeed3pm</th>\n",
              "      <th>Humidity9am</th>\n",
              "      <th>Humidity3pm</th>\n",
              "      <th>Pressure9am</th>\n",
              "      <th>Pressure3pm</th>\n",
              "      <th>Temp9am</th>\n",
              "      <th>Temp3pm</th>\n",
              "      <th>RainToday</th>\n",
              "      <th>RainTomorrow</th>\n",
              "      <th>month</th>\n",
              "      <th>Location_Adelaide</th>\n",
              "      <th>Location_Albury</th>\n",
              "      <th>Location_AliceSprings</th>\n",
              "      <th>Location_BadgerysCreek</th>\n",
              "      <th>Location_Ballarat</th>\n",
              "      <th>Location_Bendigo</th>\n",
              "      <th>Location_Brisbane</th>\n",
              "      <th>Location_Cairns</th>\n",
              "      <th>Location_Canberra</th>\n",
              "      <th>Location_Cobar</th>\n",
              "      <th>Location_CoffsHarbour</th>\n",
              "      <th>Location_Dartmoor</th>\n",
              "      <th>Location_Darwin</th>\n",
              "      <th>Location_GoldCoast</th>\n",
              "      <th>Location_Hobart</th>\n",
              "      <th>Location_Katherine</th>\n",
              "      <th>Location_Launceston</th>\n",
              "      <th>Location_Melbourne</th>\n",
              "      <th>Location_MelbourneAirport</th>\n",
              "      <th>Location_Mildura</th>\n",
              "      <th>Location_Moree</th>\n",
              "      <th>Location_MountGambier</th>\n",
              "      <th>Location_Nhil</th>\n",
              "      <th>Location_NorahHead</th>\n",
              "      <th>Location_NorfolkIsland</th>\n",
              "      <th>...</th>\n",
              "      <th>WindGustDir_S</th>\n",
              "      <th>WindGustDir_SE</th>\n",
              "      <th>WindGustDir_SSE</th>\n",
              "      <th>WindGustDir_SSW</th>\n",
              "      <th>WindGustDir_SW</th>\n",
              "      <th>WindGustDir_W</th>\n",
              "      <th>WindGustDir_WNW</th>\n",
              "      <th>WindGustDir_WSW</th>\n",
              "      <th>WindDir9am_E</th>\n",
              "      <th>WindDir9am_ENE</th>\n",
              "      <th>WindDir9am_ESE</th>\n",
              "      <th>WindDir9am_N</th>\n",
              "      <th>WindDir9am_NE</th>\n",
              "      <th>WindDir9am_NNE</th>\n",
              "      <th>WindDir9am_NNW</th>\n",
              "      <th>WindDir9am_NW</th>\n",
              "      <th>WindDir9am_S</th>\n",
              "      <th>WindDir9am_SE</th>\n",
              "      <th>WindDir9am_SSE</th>\n",
              "      <th>WindDir9am_SSW</th>\n",
              "      <th>WindDir9am_SW</th>\n",
              "      <th>WindDir9am_W</th>\n",
              "      <th>WindDir9am_WNW</th>\n",
              "      <th>WindDir9am_WSW</th>\n",
              "      <th>WindDir3pm_E</th>\n",
              "      <th>WindDir3pm_ENE</th>\n",
              "      <th>WindDir3pm_ESE</th>\n",
              "      <th>WindDir3pm_N</th>\n",
              "      <th>WindDir3pm_NE</th>\n",
              "      <th>WindDir3pm_NNE</th>\n",
              "      <th>WindDir3pm_NNW</th>\n",
              "      <th>WindDir3pm_NW</th>\n",
              "      <th>WindDir3pm_S</th>\n",
              "      <th>WindDir3pm_SE</th>\n",
              "      <th>WindDir3pm_SSE</th>\n",
              "      <th>WindDir3pm_SSW</th>\n",
              "      <th>WindDir3pm_SW</th>\n",
              "      <th>WindDir3pm_W</th>\n",
              "      <th>WindDir3pm_WNW</th>\n",
              "      <th>WindDir3pm_WSW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>128458</th>\n",
              "      <td>9.9</td>\n",
              "      <td>15.3</td>\n",
              "      <td>30.6</td>\n",
              "      <td>46.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>1015.9</td>\n",
              "      <td>1015.8</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102180</th>\n",
              "      <td>3.7</td>\n",
              "      <td>12.1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>35.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>1021.2</td>\n",
              "      <td>1020.6</td>\n",
              "      <td>7.9</td>\n",
              "      <td>11.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48059</th>\n",
              "      <td>9.3</td>\n",
              "      <td>27.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1015.8</td>\n",
              "      <td>1010.7</td>\n",
              "      <td>14.2</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39854</th>\n",
              "      <td>17.3</td>\n",
              "      <td>22.6</td>\n",
              "      <td>1.2</td>\n",
              "      <td>43.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1018.5</td>\n",
              "      <td>1018.1</td>\n",
              "      <td>21.7</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5294</th>\n",
              "      <td>4.7</td>\n",
              "      <td>15.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1024.4</td>\n",
              "      <td>1022.6</td>\n",
              "      <td>9.3</td>\n",
              "      <td>14.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        MinTemp  MaxTemp  ...  WindDir3pm_WNW  WindDir3pm_WSW\n",
              "128458      9.9     15.3  ...               0               0\n",
              "102180      3.7     12.1  ...               0               0\n",
              "48059       9.3     27.2  ...               0               0\n",
              "39854      17.3     22.6  ...               0               0\n",
              "5294        4.7     15.2  ...               0               1\n",
              "\n",
              "[5 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77QspHCTrktY"
      },
      "source": [
        "### Separating data into Training and Test sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abw7-FD_riCV",
        "outputId": "e59b239f-046f-4335-8881-133b4bdf4e40"
      },
      "source": [
        "\n",
        "# define the x (features) and y (labels) variables\n",
        "y_col = 'RainTomorrow'\n",
        "x_cols = df.drop(columns=['RainTomorrow']).columns.tolist()\n",
        "\n",
        "# split the dataset into train/test datasets \n",
        "train = df.sample(frac=0.8, random_state=0)\n",
        "test = df.drop(train.index)\n",
        "\n",
        "# Standardizing data sets\n",
        "train = (train - train.min())/(train.max() - train.min())\n",
        "test = (test - test.min())/(test.max() - test.min())\n",
        "\n",
        "# Splitting training data into validation data\n",
        "valid = train.sample(frac=0.1, random_state=0)\n",
        "train = train.drop(valid.index) # Deleting rows sampled for validation data\n",
        "\n",
        "# separate the x (features) and y (labels) in the train/valid/test datasets and normalizing them\n",
        "train_features = torch.tensor(train[x_cols].values, dtype=torch.float)\n",
        "test_features = torch.tensor(test[x_cols].values, dtype=torch.float)\n",
        "valid_features = torch.tensor(valid[x_cols].values, dtype=torch.float)\n",
        "\n",
        "train_labels = torch.tensor(train[y_col].values.reshape(-1, 1), dtype=torch.float)\n",
        "test_labels = torch.tensor(test[y_col].values.reshape(-1, 1), dtype=torch.float)\n",
        "valid_labels = torch.tensor(valid[y_col].values.reshape(-1, 1), dtype=torch.float)\n",
        "\n",
        "print('train features shape:', train_features.shape)\n",
        "print('train labels shape:', train_labels.shape)\n",
        "\n",
        "print('validation features shape:', valid_features.shape)\n",
        "print('validation labels shape:', valid_labels.shape)\n",
        "\n",
        "print('test features shape:', test_features.shape)\n",
        "print('test labels shape:', test_labels.shape)\n",
        "\n",
        "print('first 5 test labels:\\n', test_labels[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train features shape: torch.Size([36027, 106])\n",
            "train labels shape: torch.Size([36027, 1])\n",
            "validation features shape: torch.Size([4003, 106])\n",
            "validation labels shape: torch.Size([4003, 1])\n",
            "test features shape: torch.Size([10008, 106])\n",
            "test labels shape: torch.Size([10008, 1])\n",
            "first 5 test labels:\n",
            " tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB8P1GFcifvd"
      },
      "source": [
        "## 3. Build the model\n",
        "Now that the data is ready, we can build a model. We will use PyTorch to define a simple logistic regression model (single-layer neural network) to predict if it will rain the next day given the weather data of the current day. Given a sample with a corresponding prediction that is above 0.5, the model will assign the \"rain\" (1) category to it, otherwise it is categorized as \"no rain\". \n",
        "\n",
        "We also define the loss function and optimization algorithm. We will use *binary cross-entropy* loss, *stochastic gradient descent*, and track the *accuracy* metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ykITrvlpZE",
        "outputId": "f1c7c8e5-5ad8-4e6c-c5b9-45b31b68c41f"
      },
      "source": [
        "# building logistic model\n",
        "class Logistic_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    @params\n",
        "        num_features(int): The number of features to construct the input layer of the NN\n",
        "    \"\"\"\n",
        "    # Defining Constructor\n",
        "    def __init__(self, num_features):\n",
        "        super(Logistic_Model, self).__init__()\n",
        "\n",
        "        # Defining Layers\n",
        "        self.fc1 = nn.Linear(num_features, 1)\n",
        "\n",
        "        # Define Sigmoid activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initializing model\n",
        "num_features = train_features.shape[1]\n",
        "model = Logistic_Model(num_features)\n",
        "\n",
        "# Moving model to use a GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Logistic_Model(\n",
              "  (fc1): Linear(in_features=106, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouH0ZFaFoXIz"
      },
      "source": [
        "# Defining Loss Function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Defining optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UraXf-5skCxm"
      },
      "source": [
        "## 4. Train the model\n",
        "No it's time to train the model. We will train it for 100 *epochs* (iterations) with a *batch size* of 1024 (the number of training examples to evaluate prior to doing gradient descent). To use batches, we will load the data into PyTorch [`Datasets`](https://pytorch.org/docs/stable/data.html#dataset-types) and [`Dataloaders`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). We used [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class that creates a PyTorch dataset using the features and labels we already have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keBa4mDOs3ov"
      },
      "source": [
        "batch_size = 1024\n",
        "\n",
        "# Defining datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "valid_dataset = torch.utils.data.TensorDataset(valid_features, valid_labels)\n",
        "\n",
        "# Loading datasets into dataloaders\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Mx0-hns9hA",
        "outputId": "941b2e83-e7dc-40ef-b91c-faadf29560de"
      },
      "source": [
        "# Function that takes output and returns predictions\n",
        "def get_predictions(output, threshold=0.5):\n",
        "    predictions = torch.zeros(output.shape)\n",
        "    for i in range(len(output)):\n",
        "        if output[i] < threshold:\n",
        "            predictions[i] = 0\n",
        "        else:\n",
        "            predictions[i] = 1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "valid_losses = []\n",
        "valid_accuracies = []\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    train_counts = 0\n",
        "    valid_counts = 0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "\n",
        "    # Setting model to train mode\n",
        "    model.train()\n",
        "\n",
        "    for train_features, train_labels in train_dataloader:\n",
        "        # Moving data to GPU if available\n",
        "        train_features, train_labels = train_features.to(device), train_labels.to(device)\n",
        "        \n",
        "        # Setting all gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate Output\n",
        "        output = model(train_features)\n",
        "            \n",
        "        # Calculate Loss\n",
        "        loss = criterion(output, train_labels)\n",
        "\n",
        "        # Calculate Gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform Gradient Descent Step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Saving loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Get Predictions\n",
        "        train_preds = get_predictions(output)\n",
        "\n",
        "        # Saving number of right predictions for accuracy\n",
        "        train_counts += train_preds.to(device).eq(train_labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    # Setting model to evaluation mode, no parameters will change\n",
        "    model.eval()\n",
        "    for valid_features, valid_labels in valid_dataloader:\n",
        "        # Moving data to GPU if available\n",
        "        valid_features, valid_labels = valid_features.to(device), valid_labels.to(device)\n",
        "        # Calculate Output\n",
        "        output = model(valid_features)\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = criterion(output, valid_labels)\n",
        "\n",
        "        # Saving loss\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "        # Get Predictions\n",
        "        valid_preds = get_predictions(output)\n",
        "\n",
        "        # Saving number of right predictions for accuracy\n",
        "        valid_counts += valid_preds.to(device).eq(valid_labels).sum().item()\n",
        "\n",
        "    # Averaging and Saving Losses\n",
        "    train_loss/=len(train_dataset)\n",
        "    valid_loss/=len(valid_dataset)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    # Getting accuracies and saving them\n",
        "    train_acc = train_counts/len(train_dataset)\n",
        "    valid_acc = valid_counts/len(valid_dataset)\n",
        "    train_accuracies.append(train_acc)\n",
        "    valid_accuracies.append(valid_acc)\n",
        "\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.2f}% \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.2f}%'.format(epoch, train_loss, train_acc*100, valid_loss, valid_acc*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.000665 \tTraining Accuracy: 63.32% \tValidation Loss: 0.000634 \tValidation Accuracy: 70.45%\n",
            "Epoch: 2 \tTraining Loss: 0.000614 \tTraining Accuracy: 72.09% \tValidation Loss: 0.000598 \tValidation Accuracy: 72.25%\n",
            "Epoch: 3 \tTraining Loss: 0.000587 \tTraining Accuracy: 72.76% \tValidation Loss: 0.000577 \tValidation Accuracy: 72.32%\n",
            "Epoch: 4 \tTraining Loss: 0.000568 \tTraining Accuracy: 72.96% \tValidation Loss: 0.000562 \tValidation Accuracy: 72.92%\n",
            "Epoch: 5 \tTraining Loss: 0.000557 \tTraining Accuracy: 73.33% \tValidation Loss: 0.000552 \tValidation Accuracy: 73.15%\n",
            "Epoch: 6 \tTraining Loss: 0.000546 \tTraining Accuracy: 73.65% \tValidation Loss: 0.000543 \tValidation Accuracy: 73.89%\n",
            "Epoch: 7 \tTraining Loss: 0.000539 \tTraining Accuracy: 74.03% \tValidation Loss: 0.000536 \tValidation Accuracy: 74.24%\n",
            "Epoch: 8 \tTraining Loss: 0.000532 \tTraining Accuracy: 74.32% \tValidation Loss: 0.000530 \tValidation Accuracy: 74.09%\n",
            "Epoch: 9 \tTraining Loss: 0.000528 \tTraining Accuracy: 74.62% \tValidation Loss: 0.000524 \tValidation Accuracy: 74.22%\n",
            "Epoch: 10 \tTraining Loss: 0.000522 \tTraining Accuracy: 74.88% \tValidation Loss: 0.000520 \tValidation Accuracy: 74.57%\n",
            "Epoch: 11 \tTraining Loss: 0.000517 \tTraining Accuracy: 75.08% \tValidation Loss: 0.000516 \tValidation Accuracy: 74.74%\n",
            "Epoch: 12 \tTraining Loss: 0.000513 \tTraining Accuracy: 75.30% \tValidation Loss: 0.000512 \tValidation Accuracy: 74.94%\n",
            "Epoch: 13 \tTraining Loss: 0.000510 \tTraining Accuracy: 75.46% \tValidation Loss: 0.000508 \tValidation Accuracy: 75.17%\n",
            "Epoch: 14 \tTraining Loss: 0.000508 \tTraining Accuracy: 75.62% \tValidation Loss: 0.000505 \tValidation Accuracy: 75.14%\n",
            "Epoch: 15 \tTraining Loss: 0.000503 \tTraining Accuracy: 75.83% \tValidation Loss: 0.000502 \tValidation Accuracy: 75.34%\n",
            "Epoch: 16 \tTraining Loss: 0.000501 \tTraining Accuracy: 75.97% \tValidation Loss: 0.000500 \tValidation Accuracy: 75.57%\n",
            "Epoch: 17 \tTraining Loss: 0.000499 \tTraining Accuracy: 76.12% \tValidation Loss: 0.000498 \tValidation Accuracy: 75.69%\n",
            "Epoch: 18 \tTraining Loss: 0.000497 \tTraining Accuracy: 76.23% \tValidation Loss: 0.000495 \tValidation Accuracy: 75.79%\n",
            "Epoch: 19 \tTraining Loss: 0.000494 \tTraining Accuracy: 76.43% \tValidation Loss: 0.000493 \tValidation Accuracy: 75.79%\n",
            "Epoch: 20 \tTraining Loss: 0.000492 \tTraining Accuracy: 76.53% \tValidation Loss: 0.000491 \tValidation Accuracy: 75.97%\n",
            "Epoch: 21 \tTraining Loss: 0.000492 \tTraining Accuracy: 76.64% \tValidation Loss: 0.000490 \tValidation Accuracy: 76.17%\n",
            "Epoch: 22 \tTraining Loss: 0.000488 \tTraining Accuracy: 76.68% \tValidation Loss: 0.000488 \tValidation Accuracy: 76.17%\n",
            "Epoch: 23 \tTraining Loss: 0.000488 \tTraining Accuracy: 76.84% \tValidation Loss: 0.000487 \tValidation Accuracy: 76.39%\n",
            "Epoch: 24 \tTraining Loss: 0.000487 \tTraining Accuracy: 76.90% \tValidation Loss: 0.000485 \tValidation Accuracy: 76.47%\n",
            "Epoch: 25 \tTraining Loss: 0.000485 \tTraining Accuracy: 76.97% \tValidation Loss: 0.000484 \tValidation Accuracy: 76.57%\n",
            "Epoch: 26 \tTraining Loss: 0.000483 \tTraining Accuracy: 77.06% \tValidation Loss: 0.000483 \tValidation Accuracy: 76.62%\n",
            "Epoch: 27 \tTraining Loss: 0.000481 \tTraining Accuracy: 77.09% \tValidation Loss: 0.000482 \tValidation Accuracy: 76.52%\n",
            "Epoch: 28 \tTraining Loss: 0.000483 \tTraining Accuracy: 77.06% \tValidation Loss: 0.000480 \tValidation Accuracy: 76.74%\n",
            "Epoch: 29 \tTraining Loss: 0.000480 \tTraining Accuracy: 77.11% \tValidation Loss: 0.000479 \tValidation Accuracy: 76.84%\n",
            "Epoch: 30 \tTraining Loss: 0.000478 \tTraining Accuracy: 77.19% \tValidation Loss: 0.000478 \tValidation Accuracy: 76.87%\n",
            "Epoch: 31 \tTraining Loss: 0.000477 \tTraining Accuracy: 77.20% \tValidation Loss: 0.000477 \tValidation Accuracy: 76.94%\n",
            "Epoch: 32 \tTraining Loss: 0.000478 \tTraining Accuracy: 77.24% \tValidation Loss: 0.000476 \tValidation Accuracy: 76.99%\n",
            "Epoch: 33 \tTraining Loss: 0.000477 \tTraining Accuracy: 77.31% \tValidation Loss: 0.000476 \tValidation Accuracy: 77.27%\n",
            "Epoch: 34 \tTraining Loss: 0.000477 \tTraining Accuracy: 77.32% \tValidation Loss: 0.000475 \tValidation Accuracy: 77.07%\n",
            "Epoch: 35 \tTraining Loss: 0.000475 \tTraining Accuracy: 77.39% \tValidation Loss: 0.000474 \tValidation Accuracy: 77.32%\n",
            "Epoch: 36 \tTraining Loss: 0.000475 \tTraining Accuracy: 77.37% \tValidation Loss: 0.000474 \tValidation Accuracy: 77.29%\n",
            "Epoch: 37 \tTraining Loss: 0.000474 \tTraining Accuracy: 77.38% \tValidation Loss: 0.000473 \tValidation Accuracy: 77.34%\n",
            "Epoch: 38 \tTraining Loss: 0.000475 \tTraining Accuracy: 77.46% \tValidation Loss: 0.000472 \tValidation Accuracy: 77.19%\n",
            "Epoch: 39 \tTraining Loss: 0.000473 \tTraining Accuracy: 77.46% \tValidation Loss: 0.000471 \tValidation Accuracy: 77.39%\n",
            "Epoch: 40 \tTraining Loss: 0.000472 \tTraining Accuracy: 77.51% \tValidation Loss: 0.000471 \tValidation Accuracy: 77.17%\n",
            "Epoch: 41 \tTraining Loss: 0.000471 \tTraining Accuracy: 77.47% \tValidation Loss: 0.000470 \tValidation Accuracy: 77.19%\n",
            "Epoch: 42 \tTraining Loss: 0.000470 \tTraining Accuracy: 77.54% \tValidation Loss: 0.000470 \tValidation Accuracy: 77.24%\n",
            "Epoch: 43 \tTraining Loss: 0.000470 \tTraining Accuracy: 77.55% \tValidation Loss: 0.000469 \tValidation Accuracy: 77.24%\n",
            "Epoch: 44 \tTraining Loss: 0.000469 \tTraining Accuracy: 77.59% \tValidation Loss: 0.000469 \tValidation Accuracy: 77.37%\n",
            "Epoch: 45 \tTraining Loss: 0.000469 \tTraining Accuracy: 77.64% \tValidation Loss: 0.000468 \tValidation Accuracy: 77.34%\n",
            "Epoch: 46 \tTraining Loss: 0.000469 \tTraining Accuracy: 77.64% \tValidation Loss: 0.000468 \tValidation Accuracy: 77.37%\n",
            "Epoch: 47 \tTraining Loss: 0.000468 \tTraining Accuracy: 77.58% \tValidation Loss: 0.000467 \tValidation Accuracy: 77.19%\n",
            "Epoch: 48 \tTraining Loss: 0.000468 \tTraining Accuracy: 77.63% \tValidation Loss: 0.000467 \tValidation Accuracy: 77.44%\n",
            "Epoch: 49 \tTraining Loss: 0.000466 \tTraining Accuracy: 77.66% \tValidation Loss: 0.000466 \tValidation Accuracy: 77.47%\n",
            "Epoch: 50 \tTraining Loss: 0.000467 \tTraining Accuracy: 77.73% \tValidation Loss: 0.000466 \tValidation Accuracy: 77.44%\n",
            "Epoch: 51 \tTraining Loss: 0.000465 \tTraining Accuracy: 77.74% \tValidation Loss: 0.000466 \tValidation Accuracy: 77.64%\n",
            "Epoch: 52 \tTraining Loss: 0.000466 \tTraining Accuracy: 77.74% \tValidation Loss: 0.000465 \tValidation Accuracy: 77.57%\n",
            "Epoch: 53 \tTraining Loss: 0.000467 \tTraining Accuracy: 77.77% \tValidation Loss: 0.000465 \tValidation Accuracy: 77.57%\n",
            "Epoch: 54 \tTraining Loss: 0.000465 \tTraining Accuracy: 77.78% \tValidation Loss: 0.000464 \tValidation Accuracy: 77.77%\n",
            "Epoch: 55 \tTraining Loss: 0.000466 \tTraining Accuracy: 77.72% \tValidation Loss: 0.000464 \tValidation Accuracy: 77.72%\n",
            "Epoch: 56 \tTraining Loss: 0.000464 \tTraining Accuracy: 77.76% \tValidation Loss: 0.000464 \tValidation Accuracy: 77.77%\n",
            "Epoch: 57 \tTraining Loss: 0.000465 \tTraining Accuracy: 77.81% \tValidation Loss: 0.000463 \tValidation Accuracy: 77.74%\n",
            "Epoch: 58 \tTraining Loss: 0.000465 \tTraining Accuracy: 77.82% \tValidation Loss: 0.000463 \tValidation Accuracy: 77.97%\n",
            "Epoch: 59 \tTraining Loss: 0.000463 \tTraining Accuracy: 77.81% \tValidation Loss: 0.000463 \tValidation Accuracy: 77.82%\n",
            "Epoch: 60 \tTraining Loss: 0.000465 \tTraining Accuracy: 77.82% \tValidation Loss: 0.000462 \tValidation Accuracy: 77.89%\n",
            "Epoch: 61 \tTraining Loss: 0.000463 \tTraining Accuracy: 77.84% \tValidation Loss: 0.000462 \tValidation Accuracy: 77.97%\n",
            "Epoch: 62 \tTraining Loss: 0.000463 \tTraining Accuracy: 77.86% \tValidation Loss: 0.000462 \tValidation Accuracy: 77.94%\n",
            "Epoch: 63 \tTraining Loss: 0.000463 \tTraining Accuracy: 77.85% \tValidation Loss: 0.000462 \tValidation Accuracy: 77.94%\n",
            "Epoch: 64 \tTraining Loss: 0.000462 \tTraining Accuracy: 77.87% \tValidation Loss: 0.000461 \tValidation Accuracy: 77.84%\n",
            "Epoch: 65 \tTraining Loss: 0.000461 \tTraining Accuracy: 77.86% \tValidation Loss: 0.000461 \tValidation Accuracy: 77.94%\n",
            "Epoch: 66 \tTraining Loss: 0.000461 \tTraining Accuracy: 77.91% \tValidation Loss: 0.000461 \tValidation Accuracy: 77.97%\n",
            "Epoch: 67 \tTraining Loss: 0.000461 \tTraining Accuracy: 77.94% \tValidation Loss: 0.000461 \tValidation Accuracy: 78.04%\n",
            "Epoch: 68 \tTraining Loss: 0.000461 \tTraining Accuracy: 77.91% \tValidation Loss: 0.000461 \tValidation Accuracy: 77.87%\n",
            "Epoch: 69 \tTraining Loss: 0.000462 \tTraining Accuracy: 77.94% \tValidation Loss: 0.000460 \tValidation Accuracy: 77.97%\n",
            "Epoch: 70 \tTraining Loss: 0.000461 \tTraining Accuracy: 77.95% \tValidation Loss: 0.000460 \tValidation Accuracy: 77.97%\n",
            "Epoch: 71 \tTraining Loss: 0.000460 \tTraining Accuracy: 77.94% \tValidation Loss: 0.000460 \tValidation Accuracy: 78.09%\n",
            "Epoch: 72 \tTraining Loss: 0.000462 \tTraining Accuracy: 78.00% \tValidation Loss: 0.000460 \tValidation Accuracy: 78.02%\n",
            "Epoch: 73 \tTraining Loss: 0.000461 \tTraining Accuracy: 77.94% \tValidation Loss: 0.000459 \tValidation Accuracy: 77.94%\n",
            "Epoch: 74 \tTraining Loss: 0.000460 \tTraining Accuracy: 77.94% \tValidation Loss: 0.000459 \tValidation Accuracy: 78.07%\n",
            "Epoch: 75 \tTraining Loss: 0.000459 \tTraining Accuracy: 78.01% \tValidation Loss: 0.000459 \tValidation Accuracy: 78.14%\n",
            "Epoch: 76 \tTraining Loss: 0.000460 \tTraining Accuracy: 77.96% \tValidation Loss: 0.000459 \tValidation Accuracy: 78.07%\n",
            "Epoch: 77 \tTraining Loss: 0.000460 \tTraining Accuracy: 78.00% \tValidation Loss: 0.000459 \tValidation Accuracy: 78.09%\n",
            "Epoch: 78 \tTraining Loss: 0.000460 \tTraining Accuracy: 77.99% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.14%\n",
            "Epoch: 79 \tTraining Loss: 0.000459 \tTraining Accuracy: 77.99% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.14%\n",
            "Epoch: 80 \tTraining Loss: 0.000460 \tTraining Accuracy: 78.02% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.14%\n",
            "Epoch: 81 \tTraining Loss: 0.000459 \tTraining Accuracy: 78.05% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.14%\n",
            "Epoch: 82 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.09% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.14%\n",
            "Epoch: 83 \tTraining Loss: 0.000459 \tTraining Accuracy: 78.04% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.22%\n",
            "Epoch: 84 \tTraining Loss: 0.000458 \tTraining Accuracy: 78.01% \tValidation Loss: 0.000458 \tValidation Accuracy: 78.17%\n",
            "Epoch: 85 \tTraining Loss: 0.000460 \tTraining Accuracy: 78.06% \tValidation Loss: 0.000457 \tValidation Accuracy: 78.12%\n",
            "Epoch: 86 \tTraining Loss: 0.000458 \tTraining Accuracy: 78.04% \tValidation Loss: 0.000457 \tValidation Accuracy: 78.24%\n",
            "Epoch: 87 \tTraining Loss: 0.000459 \tTraining Accuracy: 78.14% \tValidation Loss: 0.000457 \tValidation Accuracy: 78.24%\n",
            "Epoch: 88 \tTraining Loss: 0.000458 \tTraining Accuracy: 78.07% \tValidation Loss: 0.000457 \tValidation Accuracy: 78.17%\n",
            "Epoch: 89 \tTraining Loss: 0.000459 \tTraining Accuracy: 78.09% \tValidation Loss: 0.000457 \tValidation Accuracy: 78.14%\n",
            "Epoch: 90 \tTraining Loss: 0.000460 \tTraining Accuracy: 78.09% \tValidation Loss: 0.000457 \tValidation Accuracy: 78.24%\n",
            "Epoch: 91 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.06% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.24%\n",
            "Epoch: 92 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.07% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.09%\n",
            "Epoch: 93 \tTraining Loss: 0.000458 \tTraining Accuracy: 78.11% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.24%\n",
            "Epoch: 94 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.12% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.27%\n",
            "Epoch: 95 \tTraining Loss: 0.000458 \tTraining Accuracy: 78.09% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.19%\n",
            "Epoch: 96 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.12% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.17%\n",
            "Epoch: 97 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.15% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.22%\n",
            "Epoch: 98 \tTraining Loss: 0.000456 \tTraining Accuracy: 78.10% \tValidation Loss: 0.000456 \tValidation Accuracy: 78.24%\n",
            "Epoch: 99 \tTraining Loss: 0.000457 \tTraining Accuracy: 78.07% \tValidation Loss: 0.000455 \tValidation Accuracy: 78.22%\n",
            "Epoch: 100 \tTraining Loss: 0.000458 \tTraining Accuracy: 78.13% \tValidation Loss: 0.000455 \tValidation Accuracy: 78.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Aybqwzqa45Cx",
        "outputId": "3a46e054-29be-46af-85ef-f536e93ce4c7"
      },
      "source": [
        "plt.plot(valid_accuracies)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGnpAbVjzhQm"
      },
      "source": [
        "As the above plot suggests, our model converges to a validation accuracy close to 78%. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWuWW0tcvV7_"
      },
      "source": [
        "## 5. Evaluate the model\n",
        "Now that we trained our model, it's time to evaluate it using the test dataset, which we did not use when training the model. This gives us a sense of how well our model predicts unseen data, which is the case when we use it in the real world."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkZOiI_p6MlR",
        "outputId": "f2feb01e-a243-413b-bfec-611e62d5d8ee"
      },
      "source": [
        "test_loss = 0.0\n",
        "test_counts = 0\n",
        "\n",
        "# Setting model to evaluation mode, no parameters will change\n",
        "model.eval()\n",
        "\n",
        "for test_features, test_labels in test_dataloader:\n",
        "    # Moving data to GPU if available\n",
        "    test_features, test_labels = test_features.to(device), test_labels.to(device)\n",
        "\n",
        "    # Calculate Output\n",
        "    output = model(test_features)\n",
        "\n",
        "    # Calculate Loss\n",
        "    loss = criterion(output, test_labels)\n",
        "\n",
        "    # Saving loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # Get Predictions\n",
        "    test_preds = get_predictions(output)\n",
        "\n",
        "    # Saving number of right predictions for accuracy\n",
        "    test_counts += test_preds.to(device).eq(test_labels).sum().item()\n",
        "\n",
        "# Calculating test accuracy\n",
        "test_acc = test_counts/len(test_dataset)\n",
        "print('Test Loss: {:.6f} \\tTest Accuracy: {:.2f}%'.format(test_loss, test_acc*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.589183 \tTest Accuracy: 78.21%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92XmHVgW0xLg"
      },
      "source": [
        "Our logistic regression model fitted to the data fairly, correctly predicting the next-day's weather around 77% to 78% of the time. The distribution of rainy days and non-rainy days in our datasets originally was not balanced, but the data was balanced before training. To give our results more context, we should check the *confusion matrix* to see how the model's predictions were distributed.\n",
        "\n",
        "A confusion matrix indicates the number of correct predictions and incorrect predictions for each class. It is particularly useful whenever the data has an imbalanced representation of the classes. The diagonals of a confusion matrix indicate the correct predictions for each class, while the cross-diagonal indicates misclassified predictions. Below is an example of a binary classification confusion matrix.\n",
        "\n",
        "<figure>\n",
        "  <img src='https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png' width='35%'>\n",
        "  <figcaption>A basic confusion matrix</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_hPEGd12NbY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "b3b97cec-aea3-4c40-a06a-498028bbb948"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "test_predictions = get_predictions(model(test_features)).numpy()\n",
        "\n",
        "# Converting labels and predictions to numpy arrays\n",
        "test_predictions = get_predictions(model(test_features)).numpy()\n",
        "test_labels = test_labels.cpu().numpy()\n",
        "\n",
        "# measure the accuracy\n",
        "model_acc = metrics.accuracy_score(test_labels, test_predictions)\n",
        "print(f'logistic regression model accuracy: {round(model_acc*100, 2)}%')\n",
        "\n",
        "# plot confusion matrix\n",
        "cm = metrics.confusion_matrix(test_labels, test_predictions)\n",
        "print('confusion matrix:\\n', cm) \n",
        "\n",
        "plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.xticks([0, 1], [0, 1])\n",
        "plt.yticks([0, 1], [0,1])\n",
        "plt.title('Confusion matrix ')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression model accuracy: 78.79%\n",
            "confusion matrix:\n",
            " [[314  70]\n",
            " [ 98 310]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujEVg8_C4Y6J"
      },
      "source": [
        "From the confusion matrix, we can deduce that the model performed relatively well. The number of misclassified samples of both rainy and non-rainy days were close.  Nonetheless, it's important to be aware of misleading model results, therefore the confusion matrix is very important tool to use.\n",
        "\n",
        "Let's take a look at a more comprehensive set of evaluation metrics: accuracy, precision, and recall. Precision indicates the model's ability to return only relevant instances. While recall indicates the model's ability to identify all relevant instances; and depending on our data we may want a higher precision score or vice versa. If your curious, here is an in-depth discussion about these metrics: [Beyond Accuracy: Precision and Recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6JPh9RVB_oX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426838e9-8768-4bea-8e51-9b0777a25162"
      },
      "source": [
        "print(\"Accuracy: {}%\".format(round(model_acc*100, 2)))\n",
        "print(\"Precision:\", metrics.precision_score(test_labels, test_predictions, zero_division=True))\n",
        "print(\"Recall:\" ,metrics.recall_score(test_labels, test_predictions, zero_division=True))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 78.79%\n",
            "Precision: 0.8157894736842105\n",
            "Recall: 0.7598039215686274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMvSCZRn6ATD"
      },
      "source": [
        "What's the take away from all this...\n",
        "\n",
        "**Always, always contextualize the model's results.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDoNhPh367sZ"
      },
      "source": [
        "# Summary\n",
        "\n",
        "- We use *one-hot encoding* to represent categorical data.\n",
        "- Logistic regression is popular and foundational algorithm for classification in machine learning and deep learning (neural networks). \n",
        "- The *sigmoid* logit function maps the input features to a probability distribution.\n",
        "- Linear and logistic regression are very similar, they differ in two ways. First, the labels are continous numerical values in linear regression, while they are discrete numerical values (0 and 1) each representing a particular category. Second, logistic regression uses the sigmoid function to transform the input features into a probability space and the model learns the optimal parameters to maximize the probability of confidently predicting the correct class.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mITAGLVCRsa"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}
