{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression What Makes Us Happy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7acJX4u7UmeJu70eK9JQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krmiddlebrook/intro_to_graphing_in_python/blob/master/notebooks/machine_learning/lesson%201%20-%20linear%20regression/examples/Linear_Regression_What_Makes_Us_Happy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5H75AdnZ8KJ",
        "colab_type": "text"
      },
      "source": [
        "# Multiple Linear Regression: What Makes Us Happy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvzapAQFaK3t",
        "colab_type": "text"
      },
      "source": [
        "In our previous [example](https://colab.research.google.com/github/krmiddlebrook/intro_to_graphing_in_python/blob/master/notebooks/machine_learning/lesson%201%20-%20linear%20regression/examples/simple_linear_regression_what_makes_us_happy.ipynb#scrollTo=6QeOZ9rN8b1E), we explored the topic of single-variable linear regression, which attempts to model the relationship between two variables by fitting a linear equation to the observed data. In this simple regression analysis, we have one explanatory variable (i.e., one x variable) and one dependent variable (i.e., y variable). However, what happens if we believe there is more than one explanatory variable that impacts the dependent variable? How would we model this?\n",
        "\n",
        "This is where multiple linear regression can be helpful. In this type of model, we attempt to model the relationship between multiple explanatory variables (i.e., multiple x variables) to a single dependent variable (i.e., y variable). While adding more variables allows us to model more complex \"real-world\" relationships there are also additional steps we must take to make sure our model is sound and robust.\n",
        "\n",
        "In this example, we will build a multiple regression model using the [World Happiness Report dataset](https://www.kaggle.com/unsdsn/world-happiness) to better understand what contributes to world happiness. Just like simple linear regression, we will perform the following steps:\n",
        "\n",
        "1. Explore the dataset\n",
        "2. Visualize the variables and relationships between them (i.e., correlation) to find interesting variables to explore\n",
        "3. Build a model\n",
        "4. Fit the data to the model\n",
        "5. Measure the quality of our model (i.e., how well it fits the data).\n",
        "6. Draw conclusions\n",
        "\n",
        "Since we already covered steps 1 and 2 in the [single-variable linear regression example](https://colab.research.google.com/github/krmiddlebrook/intro_to_graphing_in_python/blob/master/notebooks/machine_learning/lesson%201%20-%20linear%20regression/examples/simple_linear_regression_what_makes_us_happy.ipynb#scrollTo=6QeOZ9rN8b1E),\n",
        "we'll start our analysis by introducing the concepts behind multi-variable linear regression, followed by step 3 and 4: building a multi-variable linear regression model and fitting it to the data. Then we'll perform step 5: measuring how well our model fits the data. Finally, we'll draw conclusions about our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1cMSwsmuz1",
        "colab_type": "text"
      },
      "source": [
        "## What is Multiple Linear Regression (i.e., multi-variable linear regression or linear regression)\n",
        "In this section, we will start with a quick intuitive walk-through behind ``Linear Regression ``, then we'll apply these concepts to the World Happiness Report to see what may contribute to our happiness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSzaKF_3eweG",
        "colab_type": "text"
      },
      "source": [
        "Recall that single-variable linear regression aims to fit a line to the data (i.e., an x and y variable) using the following formula: \n",
        "$$\n",
        "y = ax + b\n",
        "$$\n",
        "where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n",
        "![single-variable linear regression](https://i0.wp.com/www.theanalysisfactor.com/wp-content/uploads/2019/06/linear-regression-gpa.png?w=470&ssl=1)\n",
        "\n",
        "In a similar fashion, multiple linear regression aims to fit a line to the data when there are multiple x variables rather than only one. This allows us to model more complex \"real-world\" relationships between the data. In multiple linear regression, we fit a line to the data using the following formula:\n",
        "$$\n",
        "y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n \n",
        "$$\n",
        "where there are multiple $x$ values (i.e., $x_1$ through $x_n$), $a_0$ is the *intercept*, and $a_1$ through $a_n$ are the coefficients that correspond to the each $x$ variable (e.g., $a_1$ corresponds to $x_1$ and so on). The coefficients are similar to the *slope* we saw in single-variable linear regression. They can be interpreted as the expected change in $y$ corresponding to a unit increase in an $x_i$ (for example $x_2$), when all other x variables stay the same.\n",
        "\n",
        "In general, multi-linear regression has the following benefits:\n",
        "1. Provides better predictive capability than single-variable linear regression. \n",
        "2. Gives a crude estimate of the relative importance of each x variable in the relationship with the y variable.\n",
        "3. More interpretable than complex non-linear models.\n",
        "4. Can be fit quickly.\n",
        "\n",
        "However, multi-linear regression has the following weaknesses:\n",
        "1. Can't easily be visualized like single-variable linear regression\n",
        "2. Assumes there is a linear relationship between explanatory variabels (i.e., the x variables) and the dependent variable (i.e., the y variable).\n",
        "For now, we won't worry too about these weaknesses, instead we'll address these in a future example.\n",
        "\n",
        "While multiple linear regression is not perfect, it is one of the foundations of machine learning. By learning the fundementals of this algorithm, you'll be better prepared to learn and implement more complex machine learning models in the future. \n",
        "\n",
        "In the rest of this example we will:\n",
        "1. load the dataset\n",
        "2. prepare the data for the model\n",
        "3. build a linear regression model\n",
        "4. fit the model to the data\n",
        "5. Observe the model coefficients (i.e., $a_1$ through $a_n$ values)\n",
        "6. Measure the quality of our model using R-squared\n",
        "7. Make some predictions and measure the quality of the model using other metrics \n",
        "8. Draw conclusions\n",
        "\n",
        "Again, our goal is to predict what the \"Happiness Score\" will be in, given a few explanatory variables that we will define below. Afterwards, you will have the tools to apply multiple linear regression on other datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lehhNxE1fLxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the python libraries and modules we need\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHqEvmn1ZYa2",
        "colab_type": "code",
        "outputId": "e47c8a9b-7c9f-4863-91e9-0653ce775980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# 1. load the dataset into a dataframe\n",
        "data_url = 'https://raw.githubusercontent.com/krmiddlebrook/intro_to_graphing_in_python/master/datasets/world-happiness/2019.csv'\n",
        "happy2019 = pd.read_csv(data_url)\n",
        "happy2019.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Overall rank</th>\n",
              "      <th>Country or region</th>\n",
              "      <th>Score</th>\n",
              "      <th>GDP per capita</th>\n",
              "      <th>Social support</th>\n",
              "      <th>Healthy life expectancy</th>\n",
              "      <th>Freedom to make life choices</th>\n",
              "      <th>Generosity</th>\n",
              "      <th>Perceptions of corruption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Finland</td>\n",
              "      <td>7.769</td>\n",
              "      <td>1.340</td>\n",
              "      <td>1.587</td>\n",
              "      <td>0.986</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>7.600</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.573</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Norway</td>\n",
              "      <td>7.554</td>\n",
              "      <td>1.488</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.028</td>\n",
              "      <td>0.603</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Iceland</td>\n",
              "      <td>7.494</td>\n",
              "      <td>1.380</td>\n",
              "      <td>1.624</td>\n",
              "      <td>1.026</td>\n",
              "      <td>0.591</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Netherlands</td>\n",
              "      <td>7.488</td>\n",
              "      <td>1.396</td>\n",
              "      <td>1.522</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.557</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Overall rank Country or region  ...  Generosity  Perceptions of corruption\n",
              "0             1           Finland  ...       0.153                      0.393\n",
              "1             2           Denmark  ...       0.252                      0.410\n",
              "2             3            Norway  ...       0.271                      0.341\n",
              "3             4           Iceland  ...       0.354                      0.118\n",
              "4             5       Netherlands  ...       0.322                      0.298\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgFdnBbl6zr",
        "colab_type": "code",
        "outputId": "6e16adde-c0a3-4c26-b659-2d41cd81b7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 2. prepare our data for the model\n",
        "x_cols = ['GDP per capita', 'Social support']\n",
        "y_col = ['Score']\n",
        "X = happy2019.loc[:, x_cols].values\n",
        "y = happy2019.loc[:, y_col].values\n",
        "print('X shape:', X.shape, 'y shape:', y.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (156, 2) y shape: (156, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc_gJ4RllO5q",
        "colab_type": "code",
        "outputId": "1d5197ee-a0f1-4fee-b5fe-9b0e9aedba78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 3. initialize (i.e., build) the model\n",
        "model = LinearRegression(fit_intercept=True)\n",
        "\n",
        "# 4. fit the model to the data\n",
        "model.fit(X, y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model intercept (i.e., a_0):  2.3297706377318717\n",
            "model coefficients (i.e., a_1,...,a_n):  [1.34647807 1.53751048]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuUBCG_tR7j",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Observe the model coefficients\n",
        "In the case of multivariable linear regression, the regression model has to find the most optimal coefficients (i.e., the $a_i$ values, including the intercept term $a_0$) for all the x variables. To see what coefficients our regression model has chosen, we can make a dataframe or simply print the values. While printing the values is easier, creating a dataframe keeps our code organized and makes identify the value that corresponds to each coefficient easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqLOuoimtIGh",
        "colab_type": "code",
        "outputId": "48a49684-7229-4429-a956-ea59570910f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "print('model intercept (i.e., a_0): ', model.intercept_[0])\n",
        "print('model coefficients (i.e., a_1,...,a_n): ', model.coef_[0])\n",
        "\n",
        "coef_values = [model.intercept_[0]] + model.coef_[0].tolist() \n",
        "coef_labels = ['Intercept'] + x_cols\n",
        "coeff_df = pd.DataFrame(coef_values, index=coef_labels, columns=['Coefficient']) \n",
        "coeff_df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Coefficient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Intercept</th>\n",
              "      <td>2.329771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GDP per capita</th>\n",
              "      <td>1.346478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Social support</th>\n",
              "      <td>1.537510</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Coefficient\n",
              "Intercept          2.329771\n",
              "GDP per capita     1.346478\n",
              "Social support     1.537510"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkWcbaEruOxw",
        "colab_type": "text"
      },
      "source": [
        "This means that for a unit increase in \"GDP per capita\", there is a 1.3465 unit increase in the \"Happiness Score\", when all other x variables stay the same. Similarly, a unit increase in “Social support“ results in an increase of 1.5375 units in the \"Happiness Score\", when all other x variables stay the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqp7iWYhjcoj",
        "colab_type": "text"
      },
      "source": [
        "### Step 6: Measuring the quality of the model using R-squared\n",
        "\n",
        "Now that we've build and fit the model, we want to know how well it fits the data. \n",
        "\n",
        "To this end, we will use the R² score of the model (pronounced R-squared), which indicates goodness of fit or how well our data fits the model. The higher the R-squared metric, the better the data fit our model. However, one limitation is that R-squared increases as the number of features increase in our model, so if I keep adding variables even if they're poor choices R-squared will still go up! A more popular metric is the adjusted R-squared which penalizes more complex models (i.e., models with more exploratory variables). However, to keep things simple, we will only calculate the normal R-squared value. You can learn more about R² [here](https://en.wikipedia.org/wiki/Coefficient_of_determination)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfNW-b6TjB9T",
        "colab_type": "code",
        "outputId": "d74be5b7-189b-464e-c8e1-b33fa93a8f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let's see the R-squared value\n",
        "model.score(X,y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7037077314606889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-ivt1tGx-L",
        "colab_type": "text"
      },
      "source": [
        "Since the R-squared value is pretty close to 1, we will assume our model fits the data fairly well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz4FR8qKzk-u",
        "colab_type": "text"
      },
      "source": [
        "### Step 7: Make some predictions and measure the quality of the model using other evaluation metrics\n",
        "\n",
        "There are other metrics we can use to measure the quality of our model. Some of the most common evaluation metrics for regression models include: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). A brief explanation of each metric is described below:\n",
        "\n",
        "- Mean Absolute Error (MAE): Is the mean of the absolute value of the errors. This gives an idea of magnitude but no sense of direction (too high or too low).\n",
        "- Mean Squared Error (MSE): Is the mean of the squared errors. MSE is more popular than MAE because MSE \"punishes\" more significant errors.\n",
        "- [Root Mean Squared Error (RMSE)](https://www.youtube.com/watch?v=3NXg44WBZ14): Is the square root of the mean of the squared errors. RMSE is even more favored because it allows us to interpret the output in y-units.\n",
        "\n",
        "In general, a value closer to 0 is what you want for these metrics. We can use Sklearns metrics module to calculate these for us. To apply these metrics, we will first make some predictions using our model, plot a few predictions to get a sense of the data, and then measure the quality of these predictions using the above evaluation metrics.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYyZ1ExFaELe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f02fd7fc-bc7d-4ba4-91ba-dcdb116a9e29"
      },
      "source": [
        "# First make predictions using the model\n",
        "preds = model.predict(X)\n",
        "preds = pd.DataFrame(preds, columns=['predicted'])\n",
        "actual = pd.DataFrame(y, columns=['actual'])\n",
        "\n",
        "actual_vs_pred = pd.concat((preds, actual), axis=1)\n",
        "actual_vs_pred.head(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predicted</th>\n",
              "      <th>actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.574080</td>\n",
              "      <td>7.769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.610454</td>\n",
              "      <td>7.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.765672</td>\n",
              "      <td>7.554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.684827</td>\n",
              "      <td>7.494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.549545</td>\n",
              "      <td>7.488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   predicted  actual\n",
              "0   6.574080   7.769\n",
              "1   6.610454   7.600\n",
              "2   6.765672   7.554\n",
              "3   6.684827   7.494\n",
              "4   6.549545   7.488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoh9QIqJyM0w",
        "colab_type": "code",
        "outputId": "736ddd56-6126-4d34-d82e-d6b78ccfcfee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "# let's plot the difference to get an idea of how close the predictions are\n",
        "actual_vs_pred.loc[:25, :].plot(kind='bar',figsize=(10,8))\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tzbOmeazDTv",
        "colab_type": "text"
      },
      "source": [
        "As we can observe, it looks like our model is making pretty decent predictions for the \"Happiness Score\". Now let's calculate the other evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ_8vOeszCVR",
        "colab_type": "code",
        "outputId": "822a3d7c-02eb-4f74-e63e-858e9a1a9d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# calculate the other evaluation metrics  \n",
        "from sklearn import metrics\n",
        "\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(actual.values, preds.values))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(actual.values, preds.values))  \n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(actual.values, preds.values)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 0.48963214998748994\n",
            "Mean Squared Error: 0.3647634280796388\n",
            "Root Mean Squared Error: 0.6039564786304049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3knqQw5M1Kp",
        "colab_type": "text"
      },
      "source": [
        "We can see that most of the evaluation metrics are close to 0. For example, the root mean squared error (RMSE) indicates that the square root of the average squared difference between the true \"Happiness score\" and the predicted \"Happiness Score\" is about 0.6 units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q559Y23hp2A4",
        "colab_type": "text"
      },
      "source": [
        "### Now you try extending the multiple linear regression model to all the numerical columns in the happiness dataset rather than just the first two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNkK6QijM0wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}