{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the last lesson, we  talked about value-based methods (Monte Carlo, Temporal Difference, Q-Learning). In this lesson, we will talk about policy-based methods. More specifically, POLICY GRADIENT!!!\n",
        "\n",
        "---\n",
        "\n",
        "Before we talk about policy gradient. Let's review the difference between value-based and policy-based methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Value-Based | Policy-Based\n",
        ":--: | :--:\n",
        "Optimizing a **value function** (which will lead to an optimal policy) | Optimizing a **policy** directly\n",
        "**Deterministic** (the policy outputs are fixed) | **Deterministic** or **Stochastic**\n",
        "Relatively **short** training cost | Relatively **long** training cost\n",
        "Generally for **discrete** action space | Suitable for both **discrete** and **continuous** action space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lesson, we will specifically discusses one of the policy gradient algorithm called the [REINFORCE](https://www.analyticsvidhya.com/blog/2020/11/reinforce-algorithm-taking-baby-steps-in-reinforcement-learning/) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U_M4_9pe9hC"
      },
      "source": [
        "We will begin by importing necessary packages and defining the policy network that the agent will use to decide its actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a-iM37gYe4kn"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Policy Network\n",
        "class SoftmaxPolicy(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super(SoftmaxPolicy, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_inputs, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, n_outputs)\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define Policy Gradient Agent\n",
        "class SoftmaxAgent:\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        self.policy_network = SoftmaxPolicy(n_inputs, n_outputs)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.01)\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        action_scores = self.policy_network(state)\n",
        "        action_probs = torch.softmax(action_scores, dim=1)\n",
        "        action = np.random.choice(len(action_probs[0]), p=action_probs.detach().numpy()[0])\n",
        "        log_prob = torch.log(action_probs[0, action])\n",
        "        return action, log_prob\n",
        "\n",
        "    def update_policy(self, rewards, log_probs):\n",
        "        discounted_rewards = []\n",
        "        for t in range(len(rewards)):\n",
        "            Gt = 0 \n",
        "            pw = 0\n",
        "            for r in rewards[t:]:\n",
        "                Gt = Gt + self.gamma**pw * r\n",
        "                pw = pw + 1\n",
        "            discounted_rewards.append(Gt)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
        "\n",
        "        policy_gradient = []\n",
        "        for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
        "            policy_gradient.append(-log_prob * Gt)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "        policy_gradient.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perhaps you understand most of the code above so far (assume you know how a basic neural networks works) except for **get_action** and **update_policy**\n",
        "\n",
        "___\n",
        "\n",
        "**get_action**\n",
        "1. We pass in the current state to the policy network\n",
        "2. From the network output, we use softmax (oh softmax!) to get probability distribution of actions (how likely we will choose each action)\n",
        "3. We choose an action according to the probability and return both the chosen *action* and the log probability of choosing that  *action* (for why we need the log probability is because of [Policy Gradient Theorem](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d), the math theory behind this algorithm)\n",
        "\n",
        "**update_policy**\n",
        "1. Initialize an empty list for discounted rewards\n",
        "2. Using the *rewards* from playing the game to update the policy according to the following equation from HuggingFace\n",
        "\n",
        "\n",
        "**One more thing**\n",
        "- If you are wondering about why we want to have *normalized discounted rewards*, check [this](https://datascience.stackexchange.com/questions/20098/why-do-we-normalize-the-discounted-rewards-when-doing-policy-gradient-reinforcem).\n",
        "- It is okay if you don't know what this algorithm is doing. To put it simply, this algorithm is an intelligent deep learning approach to learn the optimal policy via gradient ascent (maximize rewards). \n",
        "---\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 4.1**: Is the action space here **discrete** or **continuous** ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 4.2**: Is the observation space **discrete** or **continuous** ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will train the agent to play [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the Agent\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "agent = SoftmaxAgent(env.observation_space.shape[0], env.action_space.n)\n",
        "n_episodes = 80\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()[0]\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, log_prob = agent.get_action(state)\n",
        "        new_state, reward, done, _, _ = env.step(action.numpy())\n",
        "     \n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "        state = new_state\n",
        "        if done:\n",
        "            agent.update_policy(rewards, log_probs)\n",
        "            episode_reward = sum(rewards)\n",
        "            print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how well our agent does on the game!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# reset the environment\n",
        "obs = env.reset()\n",
        "\n",
        "img = plt.imshow(env.render()) # only call this once\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  img.set_data(env.render()) # just update the data\n",
        "  display.display(plt.gcf())\n",
        "  display.clear_output(wait=True)\n",
        "\n",
        "  action, _ = agent.get_action(state) # predict the action and state using the model\n",
        "  new_state, reward, done, _, _ = env.step(action.numpy())\n",
        "  if done :\n",
        "    break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you saw from the table above, Policy Gradient also work for scenario with continuous observation space!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GaussianPolicy(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super(GaussianPolicy, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_inputs, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, n_outputs)\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.ReLU(x)\n",
        "        \n",
        "        mean = x.mean()\n",
        "        std_dev = torch.exp(x) # std deviation must be positive, hence we take exp.\n",
        "        return mean, std_dev\n",
        "\n",
        "class GaussianAgent:\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        self.policy_network = GaussianPolicy(n_inputs, n_outputs)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.01)\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        mean, std_dev = self.policy_network(state)\n",
        "        normal_distribution = torch.distributions.Normal(mean, std_dev)\n",
        "        action = normal_distribution.sample()\n",
        "        log_prob = normal_distribution.log_prob(action)\n",
        "        return action, log_prob\n",
        "\n",
        "    def update_policy(self, rewards, log_probs):\n",
        "        discounted_rewards = []\n",
        "        for t in range(len(rewards)):\n",
        "            Gt = 0\n",
        "            pw = 0\n",
        "            for r in rewards[t:]:\n",
        "                Gt = Gt + self.gamma**pw * r\n",
        "                pw = pw + 1\n",
        "            discounted_rewards.append(Gt)\n",
        "\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
        "\n",
        "        policy_gradient = []\n",
        "        for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
        "            policy_gradient.append(-log_prob * Gt)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "        policy_gradient.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different from a softmax policy that you see above, which output a discrete number of actions, we use the gaussian policy here to construct a [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) that is suitable for continuous action space. Think of this as creating a function, which accepts input and return output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "agent = GaussianAgent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "n_episodes = 10\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()[0]\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "    action, log_prob = agent.get_action(state)\n",
        "\n",
        "    while not done:\n",
        "        action, log_prob = agent.get_action(state)\n",
        "        new_state, reward, done, _, _ = env.step(action.numpy())\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "        state = new_state.reshape(-1)\n",
        "\n",
        "        if done:\n",
        "            agent.update_policy(rewards, log_probs)\n",
        "            episode_reward = sum(rewards)\n",
        "            print(\"Episode \" + str(episode) + \": \" + str(episode_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how far you have been following along the lesson!\n",
        "\n",
        "**Question 4.3**:  Is the action space here **discrete** or **continuous** ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How about this game, how well does our agent perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# reset the environment\n",
        "obs = env.reset()\n",
        "\n",
        "img = plt.imshow(env.render()) # only call this once\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  img.set_data(env.render()) # just update the data\n",
        "  display.display(plt.gcf())\n",
        "  display.clear_output(wait=True)\n",
        "\n",
        "  action, _ = agent.get_action(state) # predict the action and state using the model\n",
        "  new_state, reward, done, _, _ = env.step(action.numpy())\n",
        "  if done :\n",
        "    break\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
