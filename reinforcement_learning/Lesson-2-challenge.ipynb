{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BreakoutMentors/Data-Science-and-Machine-Learning/blob/main/reinforcement_learning/Lesson-2-challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zalJXShDrpdV"
      },
      "source": [
        "# The Aventures of Meow the Cat\n",
        "\n",
        "---\n",
        "\n",
        "Aha! You have now learn what a Markov Decision Process is and it is your time to take a deeper look at it! If you take a look at the transition matrix below, you wll find something similar to the lesson. Enjoy!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54JyWUXr0YW"
      },
      "source": [
        "## Part 1: Defining the MDP\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4UQ7fNqxHC9"
      },
      "outputs": [],
      "source": [
        "# Transition Matrix\n",
        "P = {\n",
        "    \"Sleep\": {\"Sleep\": 0.2 , \"Play\": 0.8, \"Eat\": 0.0},\n",
        "    \"Play\" : {\"Sleep\": 0.0 , \"Play\": 0.2, \"Eat\": 0.8},\n",
        "    \"Eat\"  : {\"Sleep\": 1.0 , \"Play\": 0.0, \"Eat\": 0.0}\n",
        "}\n",
        "\n",
        "# Reward Matrix\n",
        "R = {\n",
        "    \"Sleep\": {\"Sleep\": 1 , \"Play\": 5, \"Eat\": 0},\n",
        "    \"Play\" : {\"Sleep\": 0 , \"Play\": 3, \"Eat\": 10},\n",
        "    \"Eat\"  : {\"Sleep\": 5 , \"Play\": 0, \"Eat\": 0}\n",
        "}\n",
        "\n",
        "# Initial Policy\n",
        "policy = {\n",
        "    \"Sleep\": {\"Sleep\": 0.5 , \"Play\": 0.5, \"Eat\": 0.0},\n",
        "    \"Play\" : {\"Sleep\": 0.0 , \"Play\": 0.5, \"Eat\": 0.5},\n",
        "    \"Eat\"  : {\"Sleep\": 1.0 , \"Play\": 0.0, \"Eat\": 0.0}\n",
        "}\n",
        "\n",
        "P, R, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzdn4jGjxTTh"
      },
      "source": [
        "**Question 2.5**: If I am currently sleeping, what will I more likely to do next: sleeping, playing or eating?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZYCOXSqxlFI"
      },
      "source": [
        "**Question 2.6**: In order to maximize my reward, what actions should I choose if I am in \"Sleep\", \"Play\" and \"Eat\" state, respectively?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZTdPfn6r5L9"
      },
      "source": [
        "## Part 2: Evaluating the Policy\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDp5hykM0-qB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_expected_reward(state, P, R, policy):\n",
        "    expected_reward = 0.0\n",
        "    for next_state in policy[state]:\n",
        "        action_probability = policy[state][next_state]\n",
        "        reward = R[state][next_state]\n",
        "        expected_reward += action_probability * reward\n",
        "    return expected_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BrX7k91EIa"
      },
      "source": [
        "**Question 2.7**: Using the function above, calculate the expected reward for all the states!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwFFu7IX7dNM"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7w3EZKur8fN"
      },
      "source": [
        "## Part 3: Policy Iteration\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzAXRo_u7nL1"
      },
      "source": [
        "**Question 2.8**: Given the above policy, calculate reward for starting at each state and the total expected reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3uc5AB-8IEt"
      },
      "outputs": [],
      "source": [
        "# New policy\n",
        "new_policy = {\n",
        "    \"Sleep\": {\"Sleep\": 0.7 , \"Play\": 0.3, \"Eat\": 0.0},\n",
        "    \"Play\" : {\"Sleep\": 0.0 , \"Play\": 0.7, \"Eat\": 0.3},\n",
        "    \"Eat\"  : {\"Sleep\": 1.0 , \"Play\": 0.0, \"Eat\": 0.0}\n",
        "}\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPuIrXhsKPt"
      },
      "source": [
        "## Part 4: Design Your OWN MDP !!!\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V05oZtdK8Q-1"
      },
      "source": [
        "**Question 2.9**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPSkmaPtsNKE"
      },
      "outputs": [],
      "source": [
        "# Transition Matrix\n",
        "\n",
        "\n",
        "# Reward Matrix\n",
        "\n",
        "\n",
        "# Policy\n",
        "\n",
        "\n",
        "\n",
        "# Policy Evaluation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Policy Iteration"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}