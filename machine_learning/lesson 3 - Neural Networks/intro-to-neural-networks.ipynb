{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro-to-Neural-Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5J3T04mlsuzsgIEPPTKbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BreakoutMentors/Data-Science-and-Machine-Learning/blob/master/machine_learning/lesson%203%20-%20Neural%20Networks/intro-to-neural-networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0mauKRSBpNE",
        "colab_type": "text"
      },
      "source": [
        "# Intro to Neural Networks\n",
        "<figure><img src='https://mk0analyticsindf35n9.kinstacdn.com/wp-content/uploads/2018/12/nural-network-banner.gif' width='70%'></img><figcaption>A Feed Forward Neural Network</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "In the previous lesson, we introduced the softmax regression method to solve multi-class classification tasks ([softmax regression](https://github.com/BreakoutMentors/Data-Science-and-Machine-Learning/blob/master/machine_learning/lesson%202%20-%20logistic%20regression/softmax-regression.ipynb)), implementing a classifer to recognize 10 handwritten digits from the MNIST digits dataset. \n",
        "\n",
        "We've come a long way and covered many concepts throughout this series, with each lesson building on the previous material. We've learned how to clean data, create linear models (via linear regression), coerce model outputs into a valid probability distribution (via logistic and softmax regression), train models using Sklearn and Tensorflow, apply the appropriate loss function, and to minimize it with respect to our model's parameters (via optimization algorithms). Now that we have a healthy understanding of these concepts in the context of simple linear models, we are ready to explore neural networks--one of the most exciting and successful methods in modern machine learning! \n",
        "\n",
        "In this lesson, we describe deep linear neural networks at a high level, focusing on their structure, and demonstrate how to build one using Tensorflow. \n",
        "\n",
        "To make this lesson more approachable, we don't cover every detail about neural networks here, but we aim to provide enough information for you to create your own  neural networks and to inspire you to explore deep learning in more detail.  \n",
        " \n",
        "Lesson roadmap: \n",
        "- High level introduction to *neural networks*.\n",
        "- Building neural networks in Python - recreating the feed forward neural network model in 3Blue1Brown's excellent video [But what is a Neural Network? | Deep learning, chapter 1](https://www.youtube.com/watch?v=aircAruvnKk&t=436s) and training it to classify handwritten digits. As you will see, this simple feed forward neural network achieves impressive results. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_vNvRWXQPJ-",
        "colab_type": "text"
      },
      "source": [
        "## Neural Networks\n",
        "Although neural networks only recently became popular, they've been around for quite some time. In fact, they first appeared in machine learning research way back in the late 1950s! But they didn't become popular until after 2012, when researchers built a neural network to classify different kinds of labeled images, achieving groundbreaking results (see [ImageNet Classification with Deep Convolutional\n",
        "Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)). Since then, neural networks have become widely used in machine learning. Neural networks are successful, in part, because they can effectively learn representations of complex data (e.g., images, text, sound, tabular, etc.), especially given enough data and computing power.    \n",
        "\n",
        "At a high level, there are three fundemental types of neural networks: 1) encoders, 2) decoders, or 3) a combination of both. We will focus on *encoders*. \n",
        "\n",
        "Encoder networks take in some input data (i.e., images, texts, sounds, etc.) and output *predictions*, just like the linear models we've been working with. The simplest type of neural networks are called feed forward neural networks (FFNNs), and they consist of many *layers* of *neurons* each *fully-connected* to those in the layer below (from which they receive input) and those above (which they, in turn, influence). \n",
        "\n",
        "FFNNs may sound complex right now, but hang in there. In many ways FFNNs are the superpowered version of the linear models we already know about. Like the linear models we've discussed (linear/logistic/softmax regression), neural networks can be configured to solve different kinds of tasks: either *regression* or *classification*. \n",
        "\n",
        "\n",
        "Here are some quick facts about neural networks:\n",
        "- They are effective models for learning to represent complex data (like images, text, sound, tabular, etc.).\n",
        "- Encoder-based networks, which take input data and output predictions, are probably the most common neural networks - they are useful for classification and regression tasks\n",
        "- Feed forward neural networks (FFNNs) are the simplest type of neural network. They consist of many *layers* of *neurons* each *fully-connected* to those in the layer below (from which they receive input) and those above (which they, in turn, influence). \n",
        "- FFNNs are like linear models on steriods. They have many more parameters than simple linear models, which enables them to learn more complex relationships from the input data. \n",
        "- Even though FFNNs are the simplest kind of neural network, they can be very effective. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig4f4QOYdvNM",
        "colab_type": "text"
      },
      "source": [
        "**Challenge:** What are two tasks that you think encoder networks might be at good at solving?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd3jWYZoLbjV",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward Neural Networks\n",
        "<figure><img src='https://thumbs.gfycat.com/WeepyConcreteGemsbok-size_restricted.gif' width='100%'></img><figcaption>A Feed Forward Neural Network | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "Did you watch the [3Blue1Brown video on neural networks](https://www.youtube.com/watch?v=aircAruvnKk&t=436s)? If you haven't yet, I highly recommend checking it out (feel free to rewatch it too, it's a great overview of neural networks). I'll frequently be referencing important concepts that the video talks about. \n",
        "\n",
        "In the following sections, we will summarize the key concepts behind neural networks. First, we describe the motivation and inspiration behind neural networks. Then, we dive into the structure of neural networks, outlining a few critical pieces that make them work. \n",
        "\n",
        "*Note, we describe these concepts from the perspective of a feed forward neural network. That said, the fundemental ideas discussed generalize to almost every type of neural network.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMetO8ANP0Ft",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Networks: Neural Network $=$ Brain?\n",
        "<figure><img src='https://github.com/BreakoutMentors/Data-Science-and-Machine-Learning/blob/master/images/neural-network-brain-pizza-yoda-analogy.png?raw=true' width='75%'></img><figcaption>\"Pizza, I like\" - Yoda</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "No, neural networks $\\neq$ brains.\n",
        "While neural networks don't actually operate like brains, they were inspired by them. \n",
        "\n",
        "Let's consider an extremely oversimplified version of the brain. The brain is an organ that uses neurons to process information and make decisions. The neurons are what the brain uses to process data (i.e., information about the world). When some piece of data is sent to a neuron it activates (or dpesn't). The magnitude/strength (i.e., positive or negative) of the activation triggers other groups of neurons to activate (or not). Eventually, this process outputs a decision--based on a combination of the prior triggers and activations--as a response to the input data. As an example, let's say there is a pizza in the kitchen and my nose picks up the scent. The smell of freshly baked dough and melted cheese activates my \"I'm hungry neurons\". Eventually, I can't ignore these neurons any longer, so I run to the kitchen and eat some pizza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjoDiltUbVTg",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Networks: Neurons\n",
        "<figure><img src='https://github.com/BreakoutMentors/Data-Science-and-Machine-Learning/blob/master/images/neuron-3blue1brown.png?raw=true' width='75%'></img><figcaption>Neural Networks: Neuron | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "**Neurons** are at the core of neural networks (after all, they are practically in the name). At a high level, a neuron holds a corresponding value (i.e., number) called an **activation**. The activation can be represented by a tiny value, a large value, or a value somewhere in between. A neuron is \"lit up\" (i.e., activated) when its corresponding activation is large, and it is \"dim\" (i.e., not very activated) when its activation is small. Connecting this to the pizza example, my \"I'm hungry neurons\" lit up after I smelled the pizza in the kitchen.    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCxJiSdYOGjl",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Networks: Layers\n",
        "<figure><img src='https://miro.medium.com/max/1280/1*_nTmA2RowzQBCqI9BVtmEQ.gif' width='75%'></img><figcaption>The Neural Network's Secret Sauce: Stacking Layers | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "The secret sauce driving neural networks is the technique of *stacking layers*. At a high level, this method enables the neural network to learn an effective representation of the data. The layers that are in between the input layer and the output layer are called *hidden layers*.\n",
        "\n",
        "A **layer** is composed of a set of **neurons**. We can manually configure the number of neurons we want to have in each layer, except for in the first and last ones. When we add more neurons and layers to the model, we add more parameters (weights and biases) to it. As a result, larger models (models with many parameters) can be computationally expensive, but very effective. This creates a trade-off between computation effeciency and model representation ability (making smaller models as effective as bigger ones is an active area of research). \n",
        "\n",
        "For classification tasks, the number of neurons in the last layer is determined by the number of categories/classes in the dataset. While in regression tasks, there is generally only one neurons in the final layer, since we are predicting a continuous value (e.g., the happiness score for a particular country). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlPlPHMIiQMb",
        "colab_type": "text"
      },
      "source": [
        "**Challenge:** In the above figure (from previous cell), how many layers are in the neural network? How many are hidden layers? How many neurons are in the first layer? How many are in the last layer? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTaJK53oefzY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoGtSuj-h7D9",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Networks: Weights & Activation Functions \n",
        "<figure><img src='https://thumbs.gfycat.com/BabyishGeneralFruitfly-size_restricted.gif' width='65%'></img><figcaption>Calculating a Neuron's Activation: Connections and Weights (1) | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "<figure><img src='https://thumbs.gfycat.com/GlitteringCavernousGoosefish-small.gif' width='65%'></img><figcaption>Calculating a Neuron's Activation: Connections and Weights (2)| <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "Neural networks pass information through the network using connections between pairs of neurons in adjacent layers. Each connection has a corresponding **weight** parameter that is learned during the model training phase. As shown in the figure above, the activation of a neuron in a subsequent layer is determined by the *weighted sum* of the weights and activations of the neurons in the previous layer (i.e., connections). A **bias** term is added at the end of the weighted sum to control how large/small a neuron's weighted sum must be to activate. Before the neuron receives a final activation value, the weighted sum is *squeezed* by an **activation function**. \n",
        "\n",
        "Activation functions and parameters (weights and biases)may sound intimidating. Fortunately, you already know a lot about these concepts: 1) the *sigmoid* and *softmax* logit functions are examples of activation functions, 2) linear models (linear/logistic/softmax) use the same *weighted sum* method to activate neurons in subsequent layers, the difference is these networks only have one layer after the input. \n",
        "\n",
        "As you may remember from the logistic and softmax lessons, these logit functions convert the inputs to a valid probability space. An activation function, more generally, can be defined as any function that transforms the neuron output. It is common to choose an activation function that normalizes the input between 0 and 1 or -1 and 1. \n",
        "\n",
        "\n",
        "Activation functions play a critical role in building effective deep neural networks. They can help the network converge quickly (find the right parameters) and improve the model's overall performance. \n",
        "\n",
        "In the diagrams above, the second layer has one neuron. This neuron is connected to every other neuron in the previous layer. Consequently, it has 784 connections plus one bias term. That's a lot of number crunching! For this reason, we generally select activation functions that can be computed effeciently (quickly). \n",
        "\n",
        "<figure><img src='https://github.com/BreakoutMentors/Data-Science-and-Machine-Learning/blob/master/images/sigmoid-activation-3Blue1Brown.png?raw=true' width='65%'></img><figcaption>Calculating a Neuron's Activation: Sigmoid Activation Function (2)| <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "So far we've only discussed connections and activations in the context of one neuron in a subsequent layer. But, most layers have many neurons. The good news is, we calculate neuron activations in the same way as before. The bad news is, we have to repeat the calculation process many times over. For example, in the diagrams below we see that all 16 neurons in the 2nd layer are connected to every other neuron in the 1st layer (i.e., 784 neurons). Thus, we need to  perform $784\\times16$ weights $ + 16$ biases calculations to get the activations for the 16 neurons in the 2nd layer. Doing this by hand would be way too difficult, but luckily, we can make computers do most of the heavy lifting. \n",
        "\n",
        "<figure><img src='https://github.com/BreakoutMentors/Data-Science-and-Machine-Learning/blob/master/images/2-layer-weights-biases-connections-3Blue1Brown.png?raw=true' width='65%'></img><figcaption>Calculating a Neuron's Activation: Sigmoid Activation Function (2) | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRgYYTTyiI1_",
        "colab_type": "text"
      },
      "source": [
        "**Challenge:** In the below neural network diagram, how many weights and biases are there between the 2nd layer and the 3rd layer? How many total weights and biases are there in the entire network? Hint: all neurons are connected to every other neuron in the previous layer. \n",
        "\n",
        "<figure><img src='https://thumbs.gfycat.com/DeadlyDeafeningAtlanticblackgoby-poster.jpg' width='65%'></img><figcaption>A Neural Network: Total Weights & Biases | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt1EV_1rXaj8",
        "colab_type": "text"
      },
      "source": [
        "## Building a Neural Network: Summary\n",
        "Now that we know a little about neural networks, it's time to make our own! In this section, we demonstrate how to build a neural network in Python using Tensorflow. Specifically, we implement the neural network from 3Blue1Brown's video [But what is a Neural Network? Deep Learning Part 1](https://www.youtube.com/watch?v=aircAruvnKk&t=436s) to classify 10 types of handwritten digits from the MNIST dataset. Before we start, let's summarize what we know so far about neural networks:\n",
        "- *Stacking layers* is their secret sauce - enabling the model to learn an effective representations of the data (most of the time).\n",
        "- Layers are comprised of *neurons*. We configure the number of neurons in *hidden layers*. \n",
        "- Neurons hold a corresponding *activation* - large activations \"light up\" neurons.   \n",
        "- The activations of neurons are determined by the weighted sum of their *connections* with the previous layer's neurons - quantified by *weights* and a *bias* term. The resulting output is then squeezed by an *activation function* such as the *sigmoid* function.\n",
        "- For classification tasks, the number of neurons in the last layer corresponds to the number of classes/categories in the dataset.    \n",
        "\n",
        "Now, it's time to make our first neural network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJK_aPDqWWV4",
        "colab_type": "text"
      },
      "source": [
        "### Classification of Handwritten Digits with a Feed Forward Neural Network\n",
        "<figure><img src='https://thumbs.gfycat.com/ViciousUnnaturalAmethystsunbird-max-1mb.gif' width='75%'></img><figcaption>A Neural Network: Total Weights & Biases | <em>Source: <a href='https://www.youtube.com/watch?v=aircAruvnKk&t=436s'>3Blue1Brown - But what is a Neural Network? Deep Learning Part 1</a></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "In this section, we will recreate the feed forward neural network (FFNN) from 3Blue1Brown's video [But what is a Neural Network? Deep Learning Part 1](https://www.youtube.com/watch?v=aircAruvnKk&t=436s) and use it to classify handwritten digits from the MNIST dataset. This process involves several steps: 1) [loading the dataset](#-Step-1:-Loading-the-Dataset), 2) [building the model](#-Step-2:-Building-the-Model), 3) [training the model](#-Step-3:-Training-the-Model), 4) [testing the model](#-Step-4:-Testing-the-Model).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHnGsX_4X_HN",
        "colab_type": "text"
      },
      "source": [
        "### Prerequisites: Google Colab + building neural networks in python \n",
        "We recommend that you run this this notebook in the cloud on Google Colab, if you're not already doing so. It's the simplest way to get started. Google Colab gives you free access to specialized compute resources called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_processing_unit). In modern machine learning these resources are frequently used because they significantly speed up model training compared to using [CPUs](https://en.wikipedia.org/wiki/Central_processing_unit) (your computer is probably using CPUs). At a high level, GPUs and TPUs are special types of computer chips that excel at performing computations on large matrices. They perform mathematical matrix operations like multiplication, addition, subtraction, etc. at a much higher rate (i.e., speed) than CPUs.    \n",
        "\n",
        "Native Python code won't run on GPUs and TPUs because they use specialized operating system *kernels*. We could convert our code to a language that these kernals can understand, but that would be a very tedious and frustrating process. Fortunately, there are several open-source Python libraries exist that do the heavy lifting for us. In particular, the two most popular open-source libraries are [PyTorch](https://pytorch.org/) and [Tensorflow](https://www.tensorflow.org/). These libraries enable us to build custom neural networks in Python that can run on GPUs and TPUs! \n",
        "\n",
        "In this lesson we will use Tensorflow because it is a bit easier to use (while you are learning about neural networks) and it comes preinstalled in Google Colab. It is also possible to [install TensorFlow locally](https://www.tensorflow.org/install/). But, the simple solution is normally best (i.e., use Google Colab).\n",
        " \n",
        "[tf.keras](https://www.tensorflow.org/guide/keras) is the simplest way to build and train neural network models in TensorFlow, so we will use it throughout this lessons.\n",
        "\n",
        "Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because 1) it comes with TensorFlow so you don't need to install anything extra and 2) it comes with powerful TensorFlow-specific features.\n",
        "\n",
        "Lastly, to accelerate model training time, you may want to run this notebook on a GPU in Google Colab. To do this, click on the \"Runtime\" tab in the top left corner of the notebook, click \"Change runtime type\", and select the \"GPU\" option under \"Hardware accelerator\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ZEitUdbQJo",
        "colab_type": "code",
        "outputId": "a5ae5058-3c70-4c58-bb43-b864d491afbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import IPython\n",
        "from six.moves import urllib\n",
        "\n",
        "print('Tensorflow version:', tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu7E8vdNnetM",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Loading the Dataset\n",
        "The MNIST dataset contains 70k grayscale images of handwritten digits at a resolution of $28 \\times 28$ pixels. Our goal is to build a classification model to take one of these images as input and predict the most likely digit contained in the image (along with a relative confidence about the prediction):\n",
        "\n",
        "<figure><img src=\"https://i.imgur.com/ITrm9x4.png\" width=\"65%\"><figcaption><em>Source: <a href=\"https://deeplearning.mit.edu/\">MIT Deep Learning</a></em></figcaption></figure>\n",
        "\n",
        "Loading the dataset will return four NumPy arrays:\n",
        "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
        "* The `test_images` and `test_labels` arrays are the *test set*--the data the model is tested on.\n",
        "\n",
        "The images are $28\\times28$ NumPy arrays (i.e., the x variables), with pixel values ranging between 0 and 255. The *labels* (i.e., y variable) are an array of integers, ranging from 0 to 9. We will use *one-hot encoding* (the technique we learned about in the logistic regression lesson) to convert these labels to vectors (i.e., arrays with mostly 0s and a 1 at the index that corresponds to the data sample's digit category). We also need to *normalize* the input images by subtracting the mean and standard deviation of the pixels. Normalizing the data encourages our model to learn more generalizable features and helps it perform better on outside data. The final data processing step is \"flattening\" the $28\\times28$ image pixel matrices into $784 \\times 1$ arrays. We reshape the image matrices into arrays because our model expects the input to be a tensor with $784$ features. \n",
        "\n",
        "Now, let's load the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoRQV2SJOUux",
        "colab_type": "code",
        "outputId": "c14928a9-9ae2-47fe-9e30-5b16c2a18a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (-1, 28*28) # this will be used to reshape the 28x28 image pixel matrices into 784 pixel vectors \n",
        "\n",
        "# the data, split between train and test sets\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize train/test images \n",
        "train_images = train_images.astype(\"float32\")\n",
        "test_images = test_images.astype(\"float32\")\n",
        "\n",
        "mean = train_images.mean()\n",
        "std = train_images.std()\n",
        "\n",
        "train_images -= mean\n",
        "train_images /= std\n",
        "\n",
        "test_images -= mean\n",
        "test_images /= std\n",
        "\n",
        "print(f'normalized images mean and std pixel values: {round(train_images.mean(), 4)}, {round(train_images.std(), 4)}')\n",
        "\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape(input_shape)\n",
        "test_images = test_images.reshape(input_shape)\n",
        "\n",
        "print(\"train_images shape:\", train_images.shape)\n",
        "print(train_images.shape[0], \"train samples\")\n",
        "print(test_images.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices (i.e., \"one-hot encode\" the y labels)\n",
        "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalized images mean and std pixel values: -0.0, 1.0\n",
            "train_images shape: (60000, 784)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnGClpLKpbsP",
        "colab_type": "text"
      },
      "source": [
        "Let's display the first 5 images from the *training set* and display the class name below each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe6oeXCBpY9U",
        "colab_type": "code",
        "outputId": "d15cabc1-adab-42d9-9cdd-029fecd20ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "plt.figure(figsize=(10,2))\n",
        "for i in range(5):\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(np.argmax(train_labels[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x144 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H65HEL3dnKJ",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2: Building the Model\n",
        "Remember that the secret sauce of neural networks is *stacking layers*? In code, we take advantage of this secret sauce by constructing several layers and combining them to create a neural network model. Building the model is a two step process that involves 1) stacking layers together using `keras.Sequential`, 2) configuring the loss function, optimizer, and metrics to monitor the model using the keras `compile` method. Loss functions, optimizers, and metrics aren't formally discussed in this lesson. Don't worry too much about them for now. They will be described in detail in a future lesson. The goal of this lesson is to introduce the underlying structure of neural networks, and demonstrate how to build/train/test one in Python. Nonetheless, a quick summary about loss functions, optimizers, and metrics can't hurt:\n",
        "\n",
        "* **Loss function** - measures how accurate the model is during training, we want to minimize the value this function returns using an optimization method.\n",
        "* **Optimizer** - defines the optimization method to use to update the model's weights based on the data it sees and its loss function.\n",
        "* **Metrics** - monitors the model using a set of user-defined metrics; metrics are calculated at the end of every train and test cycle.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNAeKxvdBzPS",
        "colab_type": "text"
      },
      "source": [
        "**Building the Model - Step 1: Stacking Layers with `keras.Sequential`**\n",
        "\n",
        "The [3Blue1Brown video](https://www.youtube.com/watch?v=aircAruvnKk&t=436s) used a feed forward neural network with 2 hidden layers to classify handwritten digits. To recreate this neural network, first we need to build a model that 1) takes 784 image pixel feature vectors as input, 2) has 2 hidden layers with 16 neurons and the sigmoid activation function, and 3) includes a final layer with 10 neurons (i.e., there are 10 digit classes so we need 10 neurons) and the *softmax* activation function. The softmax activation function normalizes the activations for the output neurons such that:\n",
        "- every activation is between 0 and 1\n",
        "- the sum of all activations is 1\n",
        "\n",
        "Notice that the softmax activation is similar to the sigmoid activation--neuron activations are squeezed between 0 and 1. Softmax differs from sigmoid by constraining the sum of all activations to 1. For multi-class classification problems, where multiple categories/classes are present in the y variable, it is common to use the softmax activation (or a varient) in the final layer. This is because the softmax activation enables us to treat the final neuron activations as confidence values (i.e., probabilities). The neuron with the largest activation is selected as the category/class prediction.       \n",
        "\n",
        "Let's see what this looks like in Python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX0Zmi36_bSI",
        "colab_type": "code",
        "outputId": "418fec94-815b-4e24-bc60-b86dd01513aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# step 1: stack model layers using keras.Sequential\n",
        "model = keras.Sequential([\n",
        "    Input(shape=input_shape[1]), # each input is a 784 feature pixel vector\n",
        "    Dense(16, activation=\"sigmoid\"), # hidden layer 1\n",
        "    Dense(16, activation='sigmoid'), # hidden layer 2\n",
        "    Dense(num_classes, activation=\"softmax\"), # final output layer \n",
        "])\n",
        "    \n",
        "# print the model summary to see its structure and the number of parameters (i.e., weights and biases)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 16)                12560     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 13,002\n",
            "Trainable params: 13,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SCDKeEKPCgf",
        "colab_type": "text"
      },
      "source": [
        "**Building the Model - Step 2: Configuring the Loss Function, Optimizer, & Metrics with the Keras `compile` Method**\n",
        "\n",
        "The model structure is defined in step 1, so most of the building process is finished. But, we still need to configure the model's loss function, optimizer, and metrics using the keras `compile` method. We will use binary cross entropy (BCE) for the loss function, the Adam optimization method, and monitor accuracy, precision, and recall of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFeAtXW9Uth8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# step 2: configure the loss function, optimizer, and model metrics  \n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", # BCE loss \n",
        "              optimizer=\"adam\", # Adam optimization\n",
        "              metrics=[\"accuracy\", keras.metrics.Precision(), keras.metrics.Recall()] # monitor metrics\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVrlp2oKVRvT",
        "colab_type": "text"
      },
      "source": [
        "Now, we can train the model! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiCTup_LfIrt",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3: Training the Model \n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model—in this example, the `train_images` and `train_labels` arrays.\n",
        "2. The model learns to associate images and labels.\n",
        "3. We ask the model to make predictions on a test set—in this example, the `test_images` array. We verify that the predictions match the labels from the `test_labels` array. \n",
        "\n",
        "We call the `model.fit` method to train the model—the model is \"fit\" to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXy44H7VfHmS",
        "colab_type": "code",
        "outputId": "957c1dfb-1c02-46f3-b42d-8ac54422d830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "# fit the model to the data; train for 15 epochs, use batch size 128, and 10% of the training data \n",
        "# for validation to model performance during training, and configure the early stopping callback\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "history = model.fit(train_images, \n",
        "                    train_labels, \n",
        "                    batch_size=batch_size, \n",
        "                    epochs=epochs, \n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 1.7606 - accuracy: 0.5854 - precision: 0.9969 - recall: 0.0119 - val_loss: 1.1825 - val_accuracy: 0.8077 - val_precision: 0.9988 - val_recall: 0.1363\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.9193 - accuracy: 0.8195 - precision: 0.9678 - recall: 0.4169 - val_loss: 0.6631 - val_accuracy: 0.8790 - val_precision: 0.9645 - val_recall: 0.7028\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.5980 - accuracy: 0.8674 - precision: 0.9407 - recall: 0.7559 - val_loss: 0.4662 - val_accuracy: 0.9005 - val_precision: 0.9490 - val_recall: 0.8440\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.4602 - accuracy: 0.8879 - precision: 0.9350 - recall: 0.8354 - val_loss: 0.3771 - val_accuracy: 0.9098 - val_precision: 0.9472 - val_recall: 0.8752\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.3871 - accuracy: 0.9007 - precision: 0.9368 - recall: 0.8653 - val_loss: 0.3258 - val_accuracy: 0.9140 - val_precision: 0.9450 - val_recall: 0.8908\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3426 - accuracy: 0.9086 - precision: 0.9390 - recall: 0.8814 - val_loss: 0.2929 - val_accuracy: 0.9203 - val_precision: 0.9443 - val_recall: 0.8987\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3116 - accuracy: 0.9154 - precision: 0.9414 - recall: 0.8924 - val_loss: 0.2689 - val_accuracy: 0.9262 - val_precision: 0.9463 - val_recall: 0.9067\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.2887 - accuracy: 0.9209 - precision: 0.9445 - recall: 0.9010 - val_loss: 0.2519 - val_accuracy: 0.9300 - val_precision: 0.9495 - val_recall: 0.9143\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.2701 - accuracy: 0.9254 - precision: 0.9461 - recall: 0.9069 - val_loss: 0.2387 - val_accuracy: 0.9363 - val_precision: 0.9532 - val_recall: 0.9195\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.2550 - accuracy: 0.9292 - precision: 0.9487 - recall: 0.9129 - val_loss: 0.2277 - val_accuracy: 0.9377 - val_precision: 0.9534 - val_recall: 0.9245\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2421 - accuracy: 0.9325 - precision: 0.9505 - recall: 0.9173 - val_loss: 0.2202 - val_accuracy: 0.9388 - val_precision: 0.9519 - val_recall: 0.9262\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2313 - accuracy: 0.9354 - precision: 0.9520 - recall: 0.9210 - val_loss: 0.2117 - val_accuracy: 0.9407 - val_precision: 0.9535 - val_recall: 0.9295\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2216 - accuracy: 0.9378 - precision: 0.9540 - recall: 0.9247 - val_loss: 0.2058 - val_accuracy: 0.9432 - val_precision: 0.9561 - val_recall: 0.9322\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2135 - accuracy: 0.9398 - precision: 0.9552 - recall: 0.9277 - val_loss: 0.2016 - val_accuracy: 0.9448 - val_precision: 0.9571 - val_recall: 0.9343\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2065 - accuracy: 0.9421 - precision: 0.9566 - recall: 0.9305 - val_loss: 0.1972 - val_accuracy: 0.9453 - val_precision: 0.9562 - val_recall: 0.9345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAu0kWT4sDFj",
        "colab_type": "text"
      },
      "source": [
        "**Challenge:** As the model is trained, the loss and metrics are displayed. What is the final precision score on the training data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WIkY_j1XtO8",
        "colab_type": "text"
      },
      "source": [
        "Now that we finished training, let's view the results. We'll use the Pandas library to store the training history in a dataframe.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBElbDoDh9-F",
        "colab_type": "code",
        "outputId": "1d90c412-ad6b-405a-80af-573b3d3cbeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "# store the training history in a dataframe\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "# see what the hist dataframe looks like\n",
        "hist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.760630</td>\n",
              "      <td>0.585370</td>\n",
              "      <td>0.996899</td>\n",
              "      <td>0.011907</td>\n",
              "      <td>1.182502</td>\n",
              "      <td>0.807667</td>\n",
              "      <td>0.998779</td>\n",
              "      <td>0.136333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.919317</td>\n",
              "      <td>0.819500</td>\n",
              "      <td>0.967759</td>\n",
              "      <td>0.416889</td>\n",
              "      <td>0.663095</td>\n",
              "      <td>0.879000</td>\n",
              "      <td>0.964547</td>\n",
              "      <td>0.702833</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.598027</td>\n",
              "      <td>0.867352</td>\n",
              "      <td>0.940728</td>\n",
              "      <td>0.755944</td>\n",
              "      <td>0.466162</td>\n",
              "      <td>0.900500</td>\n",
              "      <td>0.949026</td>\n",
              "      <td>0.844000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.460170</td>\n",
              "      <td>0.887944</td>\n",
              "      <td>0.934964</td>\n",
              "      <td>0.835407</td>\n",
              "      <td>0.377084</td>\n",
              "      <td>0.909833</td>\n",
              "      <td>0.947150</td>\n",
              "      <td>0.875167</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.387141</td>\n",
              "      <td>0.900741</td>\n",
              "      <td>0.936788</td>\n",
              "      <td>0.865315</td>\n",
              "      <td>0.325761</td>\n",
              "      <td>0.914000</td>\n",
              "      <td>0.945014</td>\n",
              "      <td>0.890833</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.342563</td>\n",
              "      <td>0.908593</td>\n",
              "      <td>0.938962</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>0.292924</td>\n",
              "      <td>0.920333</td>\n",
              "      <td>0.944308</td>\n",
              "      <td>0.898667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.311641</td>\n",
              "      <td>0.915444</td>\n",
              "      <td>0.941415</td>\n",
              "      <td>0.892444</td>\n",
              "      <td>0.268913</td>\n",
              "      <td>0.926167</td>\n",
              "      <td>0.946252</td>\n",
              "      <td>0.906667</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.288664</td>\n",
              "      <td>0.920944</td>\n",
              "      <td>0.944536</td>\n",
              "      <td>0.901000</td>\n",
              "      <td>0.251913</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.949463</td>\n",
              "      <td>0.914333</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.270072</td>\n",
              "      <td>0.925444</td>\n",
              "      <td>0.946061</td>\n",
              "      <td>0.906852</td>\n",
              "      <td>0.238743</td>\n",
              "      <td>0.936333</td>\n",
              "      <td>0.953179</td>\n",
              "      <td>0.919500</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.255047</td>\n",
              "      <td>0.929204</td>\n",
              "      <td>0.948673</td>\n",
              "      <td>0.912852</td>\n",
              "      <td>0.227671</td>\n",
              "      <td>0.937667</td>\n",
              "      <td>0.953420</td>\n",
              "      <td>0.924500</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.242141</td>\n",
              "      <td>0.932537</td>\n",
              "      <td>0.950457</td>\n",
              "      <td>0.917296</td>\n",
              "      <td>0.220159</td>\n",
              "      <td>0.938833</td>\n",
              "      <td>0.951867</td>\n",
              "      <td>0.926167</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.231349</td>\n",
              "      <td>0.935407</td>\n",
              "      <td>0.952009</td>\n",
              "      <td>0.920963</td>\n",
              "      <td>0.211726</td>\n",
              "      <td>0.940667</td>\n",
              "      <td>0.953496</td>\n",
              "      <td>0.929500</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.221646</td>\n",
              "      <td>0.937759</td>\n",
              "      <td>0.954011</td>\n",
              "      <td>0.924667</td>\n",
              "      <td>0.205798</td>\n",
              "      <td>0.943167</td>\n",
              "      <td>0.956068</td>\n",
              "      <td>0.932167</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.213531</td>\n",
              "      <td>0.939759</td>\n",
              "      <td>0.955192</td>\n",
              "      <td>0.927704</td>\n",
              "      <td>0.201576</td>\n",
              "      <td>0.944833</td>\n",
              "      <td>0.957145</td>\n",
              "      <td>0.934333</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.206547</td>\n",
              "      <td>0.942093</td>\n",
              "      <td>0.956576</td>\n",
              "      <td>0.930500</td>\n",
              "      <td>0.197209</td>\n",
              "      <td>0.945333</td>\n",
              "      <td>0.956173</td>\n",
              "      <td>0.934500</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss  accuracy  precision  ...  val_precision  val_recall  epoch\n",
              "0   1.760630  0.585370   0.996899  ...       0.998779    0.136333      0\n",
              "1   0.919317  0.819500   0.967759  ...       0.964547    0.702833      1\n",
              "2   0.598027  0.867352   0.940728  ...       0.949026    0.844000      2\n",
              "3   0.460170  0.887944   0.934964  ...       0.947150    0.875167      3\n",
              "4   0.387141  0.900741   0.936788  ...       0.945014    0.890833      4\n",
              "5   0.342563  0.908593   0.938962  ...       0.944308    0.898667      5\n",
              "6   0.311641  0.915444   0.941415  ...       0.946252    0.906667      6\n",
              "7   0.288664  0.920944   0.944536  ...       0.949463    0.914333      7\n",
              "8   0.270072  0.925444   0.946061  ...       0.953179    0.919500      8\n",
              "9   0.255047  0.929204   0.948673  ...       0.953420    0.924500      9\n",
              "10  0.242141  0.932537   0.950457  ...       0.951867    0.926167     10\n",
              "11  0.231349  0.935407   0.952009  ...       0.953496    0.929500     11\n",
              "12  0.221646  0.937759   0.954011  ...       0.956068    0.932167     12\n",
              "13  0.213531  0.939759   0.955192  ...       0.957145    0.934333     13\n",
              "14  0.206547  0.942093   0.956576  ...       0.956173    0.934500     14\n",
              "\n",
              "[15 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsVOPpkYjp74",
        "colab_type": "text"
      },
      "source": [
        "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training converges (i.e., reaches an optimal loss value) without noticeably overfitting the data as the plot shows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMR71TCgirCr",
        "colab_type": "code",
        "outputId": "c7f0f33f-38ff-48ad-96cd-95b98f410119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "def plot_loss():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,max(hist['loss'].max()+0.2, hist['val_loss'].max()+0.2)])\n",
        "\n",
        "plot_loss()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_91UyCZC3I",
        "colab_type": "text"
      },
      "source": [
        "Now, let's plot accuracy metric on the training and validation set. Similar to the loss metric, we expect the validation accuracy to be a bit lower than the training accuracy. If the validation accuracy is noticeably different than the training one, we might want to do some more analysis. When the validation accuracy is much lower than the training accuracy, the model could be overfitting. When the it is much higher than the training accuracy, the model could be underfitting (this happens less often). However, the plot suggests the model is not overfitting/underfitting the data.         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ltdcaeAa6YM",
        "colab_type": "code",
        "outputId": "60450347-82df-4685-9842-eefa22261c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "def plot_loss():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,1])\n",
        "\n",
        "plot_loss()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQYPGdGhkFMt",
        "colab_type": "text"
      },
      "source": [
        "#### Step 4: Testing the Model\n",
        "Our results on the training and validation data look promising, but we want to know whether our model performs well on unknown data. For this, we compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lznc7jmUwFbD",
        "colab_type": "code",
        "outputId": "3484d372-f047-4f18-d3cc-30d50f738c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(test_images.shape)\n",
        "test_loss, test_acc, test_prec, test_rec = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2238 - accuracy: 0.9332 - precision: 0.9470 - recall: 0.9217\n",
            "Test accuracy: 0.9332000017166138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUeSBG87wLVo",
        "colab_type": "text"
      },
      "source": [
        "Often times, the accuracy on the test dataset is a little less than the accuracy on the training dataset. Small differences are ok, but we don't want the test results to differ significantly from the training results--this suggests the model is overfitting/underfitting. \n",
        "\n",
        "**Challenge:** Do you think the difference between the training accuracy and the testing accuracy is significant? Is the model overfitting? Is it underfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nssakhiv2OfI",
        "colab_type": "text"
      },
      "source": [
        "## Recap\n",
        "You made it! We covered a lot of material in this lesson. Don't worry if it doesn't all make sense yet. The concepts will become more intuitive as you practice building, training, and testing your own neural network models. \n",
        "\n",
        "Let's summarize what we learned about neural networks:\n",
        "- Neural Networks are popular and successful machine learning models that can learn effective representations from data (i.e., images, text, sound). They can and *classification* tasks (see [Part 2](#-Part-2:-Classification-of-MNIST-Digits-with-Convolutional-Neural-Networks)), and also to generate images, text, videos, and sound.  \n",
        "- Special libraries like Tensorflow and Pytorch enable us to build neural networks in Python and train them on accelerated hardware like GPUs and TPUs. \n",
        "- Several steps are involved in making an effective neural network: \n",
        "  1. Loading the dataset\n",
        "  2. Building the model--stacking several layers and configuring the loss function, optimizer, and metrics.\n",
        "  3. Training the model--fitting the model on the training data.\n",
        "  4. Evaluating/Testing the model--evaluating the model on the testing data. \n",
        "- Once a model is trained, it can be used to make predictions on outside data (see [Part 2, Step 5](#-Step-5:-Make-predictions-on-outside-data)).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4s3YeuF1NoV",
        "colab_type": "text"
      },
      "source": [
        "#### Acknowlegements\n",
        "- [MIT Deep Learning Basics](https://www.youtube.com/watch?v=O5xeyoRL95U&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)\n",
        "- [Dive into Deep Learning](https://d2l.ai/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXMwHjBZi9gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}