{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Aventures of Meow the Cat\n",
        "\n",
        "---\n",
        "\n",
        "Aha! You have now learn what a Markov Decision Process is and it is your time to take a deeper look at it! If you take a look at the transition matrix below, you wll find something similar to the lesson. Enjoy!!"
      ],
      "metadata": {
        "id": "zalJXShDrpdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Defining the MDP"
      ],
      "metadata": {
        "id": "c54JyWUXr0YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transition Matrix\n",
        "P = {\n",
        "    \"Sleep\": {\"Sleep\": 0.2 , \"Play\": 0.8, \"Eat\": 0.0},\n",
        "    \"Play\" : {\"Sleep\": 0.0 , \"Play\": 0.2, \"Eat\": 0.8},\n",
        "    \"Eat\"  : {\"Sleep\": 1.0 , \"Play\": 0.0, \"Eat\": 0.0}\n",
        "}\n",
        "\n",
        "# Reward Matrix\n",
        "R = {\n",
        "    \"Sleep\": {\"Sleep\": 1 , \"Play\": 5, \"Eat\": 0},\n",
        "    \"Play\" : {\"Sleep\": 0 , \"Play\": 3, \"Eat\": 10},\n",
        "    \"Eat\"  : {\"Sleep\": 5 , \"Play\": 0, \"Eat\": 0}\n",
        "}\n",
        "\n",
        "# Initial Policy\n",
        "policy = {\n",
        "    \"Sleep\": {\"Sleep\": 0.5 , \"Play\": 0.5, \"Eat\": 0.0},\n",
        "    \"Play\" : {\"Sleep\": 0.0 , \"Play\": 0.5, \"Eat\": 0.5},\n",
        "    \"Eat\"  : {\"Sleep\": 1.0 , \"Play\": 0.0, \"Eat\": 0.0}\n",
        "}\n",
        "\n",
        "P, R, policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4UQ7fNqxHC9",
        "outputId": "754b7eb0-28fc-4672-d0c4-df059bc9f1e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Sleep': {'Sleep': 0.2, 'Play': 0.8, 'Eat': 0.0},\n",
              "  'Play': {'Sleep': 0.0, 'Play': 0.2, 'Eat': 0.8},\n",
              "  'Eat': {'Sleep': 1.0, 'Play': 0.0, 'Eat': 0.0}},\n",
              " {'Sleep': {'Sleep': 1, 'Play': 5, 'Eat': 0},\n",
              "  'Play': {'Sleep': 0, 'Play': 3, 'Eat': 10},\n",
              "  'Eat': {'Sleep': 5, 'Play': 0, 'Eat': 0}},\n",
              " {'Sleep': {'Sleep': 0.5, 'Play': 0.5, 'Eat': 0.0},\n",
              "  'Play': {'Sleep': 0.0, 'Play': 0.5, 'Eat': 0.5},\n",
              "  'Eat': {'Sleep': 1.0, 'Play': 0.0, 'Eat': 0.0}})"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question**: If I am currently sleeping, what will I more likely to do next: sleeping, playing or eating?"
      ],
      "metadata": {
        "id": "qzdn4jGjxTTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Solution**: Playing, see the value on \"Sleep\" row and ‚ÄùPlay\" column for the transition matrix"
      ],
      "metadata": {
        "id": "ekRT_lbo4cw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question**: In order to maximize my reward, what actions should I choose if I am in \"Sleep\", \"Play\" and \"Eat\" state, respectively?"
      ],
      "metadata": {
        "id": "3ZYCOXSqxlFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Solution**:\n",
        "\n",
        "- State: Sleep\n",
        "  - Action: Play\n",
        "- State: Play  \n",
        "  - Action: Eat\n",
        "- State: Eat\n",
        "  - Action: Sleep"
      ],
      "metadata": {
        "id": "BVsmyz0X49WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Evaluating the Policy"
      ],
      "metadata": {
        "id": "hZTdPfn6r5L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_expected_reward(state, P, R, policy):\n",
        "    expected_reward = 0.0\n",
        "    for next_state in policy[state]:\n",
        "        action_probability = policy[state][next_state]\n",
        "        reward = R[state][next_state]\n",
        "        expected_reward += action_probability * reward\n",
        "    return expected_reward"
      ],
      "metadata": {
        "id": "nDp5hykM0-qB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question**: Using the function above, calculate the expected reward for all the states!"
      ],
      "metadata": {
        "id": "H1BrX7k91EIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "NwFFu7IX7dNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Solution**"
      ],
      "metadata": {
        "id": "-nwDkr707YF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the expected reward for each state\n",
        "states = [\"Sleep\", \"Play\", \"Eat\"]\n",
        "for state in states:\n",
        "    print(f\"Expected reward for {state}: {calculate_expected_reward(state, P, R, policy)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3BXmg_77bQF",
        "outputId": "27083ec2-d09a-4eb2-d70f-b839fd22d8c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected reward for Sleep: 3.0\n",
            "Expected reward for Play: 6.5\n",
            "Expected reward for Eat: 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Policy Iteration"
      ],
      "metadata": {
        "id": "t7w3EZKur8fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question**: Given the above policy, calculate reward for starting at each state and the total expected reward."
      ],
      "metadata": {
        "id": "mzAXRo_u7nL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New policy\n",
        "new_policy = {\n",
        "    \"Sleep\": {\"Sleep\": 0.7 , \"Play\": 0.3, \"Eat\": 0.0},\n",
        "    \"Play\" : {\"Sleep\": 0.0 , \"Play\": 0.7, \"Eat\": 0.3},\n",
        "    \"Eat\"  : {\"Sleep\": 1.0 , \"Play\": 0.0, \"Eat\": 0.0}\n",
        "}\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "J3uc5AB-8IEt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Solution**"
      ],
      "metadata": {
        "id": "ATveh_5z7-sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_expected_reward = 0\n",
        "expected_reward = 0\n",
        "# Calculate the expected reward for each state\n",
        "for state in states:\n",
        "    expected_reward = calculate_expected_reward(state, P, R, new_policy)\n",
        "    total_expected_reward += expected_reward\n",
        "    print(f\"Expected reward for {state}: {expected_reward}\")\n",
        "\n",
        "print(f\"Total Expected reward is: {total_expected_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJe0y8DD76Be",
        "outputId": "c8f7bbbd-f3c4-48fc-9342-afd1a5ced533"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected reward for Sleep: 2.2\n",
            "Expected reward for Play: 5.1\n",
            "Expected reward for Eat: 5.0\n",
            "Total Expected reward is: 12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Design Your OWN MDP !!!"
      ],
      "metadata": {
        "id": "5aPuIrXhsKPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question**"
      ],
      "metadata": {
        "id": "V05oZtdK8Q-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transition Matrix\n",
        "\n",
        "\n",
        "# Reward Matrix\n",
        "\n",
        "\n",
        "# Policy\n",
        "\n",
        "\n",
        "\n",
        "# Policy Evaluation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Policy Iteration"
      ],
      "metadata": {
        "id": "KPSkmaPtsNKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}